{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Email\n",
    "cat <<EOF > q2\n",
    "#!/bin/bash\n",
    "echo \\$PWD\n",
    "echo $PWD\n",
    "EOF\n",
    "#mail -s \"test email\" -a test.txt -r Sophia.yue@aexp.com Sophia.yue@aexp.com  < test.txt  # working\n",
    "test_file=\"/axp/buanalytics/csramp/dev/cor/data/pps_defect_spain_10152018.xlsx\"\n",
    "#mail -s \"test email\" -a test.txt -r Sophia.yue@aexp.com Sophia.yue@aexp.com  sophia.yue@aexp.com < test.txt  # send me twice ; the first sophia is sender the second and the third sophia are the receiptent\n",
    "#mail -s \"test email\" -a test.txt -r Sophia.yue@aexp.com Sophia.yue@aexp.com   < ${test_file} # attach test.txt\n",
    "mail -s \"test email\" -a ${test_file} -r Sophia.yue@aexp.com sophia.yue@aexp.com < test.txt\n",
    "#cat q2 | mail -s \"test email\" -a test.txt -r Sophia.yue@aexp.com Sophia.yue@aexp.com  < test.txt\n",
    "\n",
    "#cat q2 | mailx -s \"test email\" -a test.txt -r Sophia.yue@aexp.com Sophia.yue@aexp.com  < test.txt\n",
    "#cat q2 | mailx -s \"test email\" -a test.txt -r Sophia.yue@aexp.com Sophia.yue@aexp.com  < body.txt\n",
    "#cat q2 | mailx -s \"test email\" -a test.txt -r Sophia.yue@aexp.com Sophia.yue@aexp.com  < test.txt  # not take q2 as body; take test.txt as body\n",
    "\n",
    "#mail -s \"test email\" -a  test.txt -r Sophia.yue@aexp.com Sophia.yue@aexp.com  < test.txt\n",
    "echo \"send email\"\n",
    "\n",
    "/axp/platform/cloak/bin/spark/cloak-sparksubmit\n",
    " \n",
    "#!/bin/bash\n",
    "\n",
    "export SPARK_HOME=/opt/mapr/spark/spark-1.6.1/\n",
    "\n",
    "/opt/mapr/spark/spark-1.6.1/bin/spark-submit --master yarn-client --driver-memory=16G --executor-memory=4G --conf spark.yarn.executor.memoryOverhead=2048 --conf spark.yarn.jar=maprfs:///axp/platform/cloak/lib/cloak-spark-1.2.0-SNAPSHOT.jar --conf spark.sql.parquet.binaryAsString=true --conf \"spark.executor.extraJavaOptions=-XX:MaxDirectMemorySize=512m -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=30 -XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark\" --conf \"spark.driver.extraJavaOptions=-XX:MaxDirectMemorySize=512m -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=30 -XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark\" --conf spark.network.timeout=240s --conf spark.akka.frameSize=1024 --conf spark.rpc.numRetries=5 --conf spark.speculation=true --conf spark.locality.wait=1s --conf spark.akka.threads=16 --conf spark.yarn.jar=maprfs:///axp/platform/cloak/lib/cloak-spark-1.2.0-SNAPSHOT.jar --jars /axp/platform/cloak/lib/cloak-spark-1.2.0-SNAPSHOT.jar --driver-class-path /axp/platform/cloak/lib/cloak-spark-1.2.0-SNAPSHOT.jar \"$@\"\n",
    "cor_com_code_parm.py\n",
    " \n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "# Module name: tst_arg.py\n",
    "\n",
    "# Without enter parm will use default setting to point to production folder and schema\n",
    "run_type = 'prod'\n",
    "yymm = (pd.Period(datetime.today(), 'M') - 1).strftime('%y%m')\n",
    "path_code = \"/axp/buanalytics/csramp/dev/cor/code_prod/\"\n",
    "path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "sch_nm = \"pcr_cor\"\n",
    "print (\"argument no = \", len(sys.argv))\n",
    "if len(sys.argv) >= 2:\n",
    "   run_type =  sys.argv[1].lower()\n",
    "\n",
    "if  run_type.lower() == 'dev':\n",
    "   path_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "    path_data = \"/axp/buanalytics/csramp/dev/cor/data/\"\n",
    "    sch_nm = \"yuedb\"\n",
    "\n",
    "\n",
    "if len(sys.argv) >= 3:\n",
    "   yymm =  sys.argv[2]\n",
    "\n",
    "print(\" run_type\" , run_type, 'yymm', yymm, '\\n', 'path_code', path_code, '\\n', 'path_data', path_data)\n",
    "\n",
    "cor_com_code_init.py\n",
    " \n",
    "# load code to set up environmental variables to run SqlSpark/pyspark\n",
    "f_set_spark = path_code + \"cor_com_code_set_spark_sql.py\"\n",
    "exec(compile(open(f_set_spark , \"rb\").read(), f_set_spark, 'exec' )) # use exec to include the common files\n",
    "\n",
    "# load code to import non pyspark/sqlspark related package/libraries\n",
    "f_import = path_code + \"cor_com_code_import.py\"\n",
    "exec(compile(open(f_import, \"rb\").read(), f_import, 'exec' )) # use exec to include the common files\n",
    "\n",
    "# load functions\n",
    "f_func = path_code + \"cor_com_func.py\"\n",
    "exec(compile(open(f_func, \"rb\").read(), f_func, 'exec' )) # use exec to include the common files\n",
    "\n",
    "cor_com_code_set_spark_sql.py\n",
    " \n",
    "import os\n",
    "os.environ['SPARK_HOME']='/opt/mapr/spark/spark/'\n",
    "os.environ['PYSPARK_PYTHON']='/opt/python/python27/bin/python'\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import HiveContext, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, coalesce, col, concat\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf()\\\n",
    ".setMaster(\"yarn-client\")\\\n",
    ".setSparkHome('/opt/mapr/spark/spark/')\\\n",
    ".set('spark.executor.memory', '8g')\\\n",
    ".set('spark.driver.memory', '8g')\\\n",
    ".set('spark.yarn.jar','maprfs:///axp/platform/cloak/lib/cloak-spark-1.2.0-SNAPSHOT.jar')\\\n",
    ".setAll([('spark.io.compression.codec', 'lz4'), ('spark.sql.parquet.binaryasstring', 'true')])\\\n",
    ".set(\"spark.driver.extraClassPath\",\"/axp/platform/cloak/lib/cloak-spark-1.2.0-SNAPSHOT.jar\")\\\n",
    ".set('spark.shuffle.service.enabled', 'true') \\\n",
    ".set('spark.dynamicAllocation.enabled', 'true') \\\n",
    ".set(\"spark.dynamicAllocation.minExecutors\", \"10\") \\\n",
    ".set(\"spark.dynamicAllocation.maxExecutors\", \"Integer.MAX_VALUE\") \\\n",
    ".set(\"spark.dynamicAllocation.initialExecutors\", \"20\") \\\n",
    ".set(\"spark.executor.instances\", \"20\") \\\n",
    ".set(\"spark.jars\",\"/axp/platform/cloak/lib/cloak-spark-1.2.0-SNAPSHOT.jar,/axp/platform/mlplat/app/lib/thirdpartyjars/spark-libs/commons-csv-1.4.jar,/axp/platform/mlplat/app/lib/thirdpartyjars/spark-libs/spark-csv_2.10-1.5.0.jar\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "cor_com_code_defect_tbl_sch.py\n",
    " \n",
    "from pyspark.sql.types import *\n",
    "field = [StructField(\"toc_se10\"              ,StringType(),  True),\\\n",
    "         StructField(\"se10\"                  ,StringType(),  True),\\\n",
    "         StructField(\"se_lgl_ent_type_cd\"    ,StringType(),  True),\\\n",
    "         StructField(\"ent_grp\"               ,IntegerType(), True),\\\n",
    "         StructField(\"ent_cd_dsc\"            ,StringType(),  True),\\\n",
    "         StructField(\"rol_typ_cd\"            ,StringType(),  True),\\\n",
    "         StructField(\"rol_typ_grp\"           ,StringType(),  True),\\\n",
    "         StructField(\"rol_typ_cd_dsc\"        ,StringType(),  True),\\\n",
    "         StructField(\"mis_sumy\"              ,StringType(),  True),\\\n",
    "         StructField(\"mis_sumy_det\"          ,StringType(),  True)\\\n",
    "         ]\n",
    "schema = StructType(field)\n",
    "\n",
    "cor_com_code_import.py\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, re, gzip, sys, calendar\n",
    "\n",
    "from datetime import datetime, date\n",
    "from functools import reduce\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "\n",
    "cor_ext_bld_bus_ctc_tbl.py\n",
    " \n",
    "pgm = \"cor_ext_bld_bus_ctc_tbl.py\"\n",
    "print (\"start of \" + pgm )\n",
    "# Invoke code to get parm to determine the location of the folder, schema name and yymm\n",
    "path_parm_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_parm_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "\n",
    "#load codefor Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "\n",
    "job_start_time = time.time()\n",
    "\n",
    "#dedup\n",
    "\n",
    "bus_all_tbl     = sch_nm + \".cor_wrk_bus_all\"\n",
    "q_rmv_fer= \"select * from {0} where trim(ctc_nm) not in ('FFR POLICY AUTOMATION')\".format(bus_all_tbl)\n",
    "print(\"q_rmv_fer\", q_rmv_fer)\n",
    "sqlContext.sql(q_rmv_fer).registerTempTable('tmp_bus_ctc_tbl')\n",
    "\n",
    "bus_dedup_a_tbl = sch_nm + \".cor_wrk_bus_dedup_a\"\n",
    "q_drop = 'drop table {0}'.format(bus_dedup_a_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "cr_cor_wrk_bus_dedup_a = \"\"\"\n",
    "        create table   {0} as\n",
    "        select  * from\n",
    "       (select bus.*,            \n",
    "               row_number() over (partition by  ctry_cd, toc_se10, se10,\n",
    "                                                se_lgl_ent_type_cd, first_rspbl_cd, scnd_rspbl_cd, third_rspbl_cd, fourth_rspbl_cd, fifth_rspbl_cd                                               \n",
    "                                      order  by ctry_cd, toc_se10, se10,\n",
    "                                                se_lgl_ent_type_cd, first_rspbl_cd, scnd_rspbl_cd, third_rspbl_cd, fourth_rspbl_cd, fifth_rspbl_cd,                                              \n",
    "                                                cstone_last_updatetm  desc, end_dt desc) as seqnum\n",
    "      from   tmp_bus_ctc_tbl  bus\n",
    "      ) tmp\n",
    "      where seqnum = 1\"\"\".format(bus_dedup_a_tbl)\n",
    "print(\"SQL for dedupping\", cr_cor_wrk_bus_dedup_a)\n",
    "sqlContext.sql(cr_cor_wrk_bus_dedup_a )\n",
    "f_get_tbl_cnt(bus_dedup_a_tbl, '')\n",
    "\n",
    "\n",
    "#eliminate inactive SE10\n",
    "bus_dedup_tbl = sch_nm + \".cor_wrk_bus_dedup\"\n",
    "q_drop = 'drop table {0}'.format(bus_dedup_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "cr_cor_wrk_bus_dedup =\"\"\"\n",
    "        create table {0}  as\n",
    "        select   regexp_replace(abbr_nm,' ','_') as abbr_nm, bus.*  from {1} bus\n",
    "        left outer join cstonedb3.crt_country cty\n",
    "        on    bus.ctry_cd  = cty.ctry_id\n",
    "        where bus.end_dt = '9999-12-31' \"\"\".format(bus_dedup_tbl, bus_dedup_a_tbl)\n",
    "print(\"SQL to eliminate inactive SE10: \", cr_cor_wrk_bus_dedup)\n",
    "sqlContext.sql(cr_cor_wrk_bus_dedup )\n",
    "f_get_tbl_cnt(bus_dedup_tbl, '')\n",
    "\n",
    "\n",
    "#add entity group & role type\n",
    "\n",
    "\n",
    "rol_tbl  = sch_nm + \".cor_lup_bus_role\"\n",
    "ent_tbl  = sch_nm + \".cor_lup_leg_ent\"\n",
    "\n",
    "bus_ctc_pre_tbl = sch_nm + \".cor_wrk_bus_ctc_pre\"\n",
    "q_drop = 'drop table {0}'.format(bus_ctc_pre_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "q_cr_bus_ctc = \"\"\"\n",
    "        create table {0} as\n",
    "        select ent.ent_grp,\n",
    "               rol1.bus_rol_grp_abb as bus_rol_grp_abb_1,\n",
    "               rol2.bus_rol_grp_abb as bus_rol_grp_abb_2,                  \n",
    "               rol3.bus_rol_grp_abb as bus_rol_grp_abb_3,\n",
    "               rol4.bus_rol_grp_abb as bus_rol_grp_abb_4,\n",
    "               rol5.bus_rol_grp_abb as bus_rol_grp_abb_5,              \n",
    "               bus.*\n",
    "        from {1}  bus        \n",
    "        left outer join {2} rol1  \n",
    "        on   trim(first_rspbl_cd) = rol1.bus_rol_cd\n",
    "        \n",
    "        left outer join {3} rol2  \n",
    "        on   trim(scnd_rspbl_cd) = rol2.bus_rol_cd  \n",
    "        \n",
    "        left outer join {4} rol3  \n",
    "        on   trim(third_rspbl_cd) = rol3.bus_rol_cd\n",
    "        \n",
    "        left outer join {5} rol4  \n",
    "        on   trim(fourth_rspbl_cd) = rol4.bus_rol_cd\n",
    "        \n",
    "        left outer join {6} rol5  \n",
    "        on   trim(fifth_rspbl_cd) = rol5.bus_rol_cd\n",
    "        \n",
    "        left outer join {7} ent\n",
    "        on  (lcase(bus.abbr_nm) = lcase(ent.cty)\n",
    "        and  trim(se_lgl_ent_type_cd) =  ent.ent_cd)        \n",
    "    \"\"\".format(bus_ctc_pre_tbl, bus_dedup_tbl, rol_tbl, rol_tbl, rol_tbl, rol_tbl, rol_tbl, ent_tbl )\n",
    "print(\"SQL to entity group & role type: \", q_cr_bus_ctc)\n",
    "sqlContext.sql( q_cr_bus_ctc )\n",
    "f_get_tbl_cnt(bus_ctc_pre_tbl, '')\n",
    "\n",
    "#add rol_typ\n",
    "bus_ctc_tbl = sch_nm + \".cor_wrk_bus_ctc\"\n",
    "q_drop = 'drop table {0}'.format(bus_ctc_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "\n",
    "q_cr_bus_ctc = \"\"\"\n",
    "        create table {0}  as\n",
    "         select *,  \n",
    "                concat( coalesce(bus_rol_grp_abb_1,\"\"), coalesce(bus_rol_grp_abb_2,\"\"),  coalesce(bus_rol_grp_abb_3,\"\"),  coalesce(bus_rol_grp_abb_4,\"\"),  coalesce(bus_rol_grp_abb_5,\"\") ) as rol_typ\n",
    "         from  {1}  sort by ctry_cd, se10\n",
    "       \n",
    "         \"\"\".format(bus_ctc_tbl, bus_ctc_pre_tbl)\n",
    "print(\"Sql to add rol_typ: \", q_cr_bus_ctc)\n",
    "sqlContext.sql( q_cr_bus_ctc )\n",
    "\n",
    "job_end_time = time.time()\n",
    "\n",
    "elapse_time (job_start_time, job_end_time, pgm)\n",
    "print (\"end of \" + pgm )\n",
    "\n",
    "cor_ext_bld_mernm_stscd_tbl.py\n",
    "gm = \"cor_ext_bld_mernm_stscd_tbl.py\"\n",
    "\n",
    "# Invoke code to get parm to determine the location of the folder and yymm\n",
    "path_parm_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_parm_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "#load code for Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "\n",
    "job_start_time = time.time()\n",
    "end_setup_date = f_dte_for_yymm(yymm, 'l')  # get the last day of yymm\n",
    "start_setup_date = '2018-10-12'\n",
    "print (\"start_setup_date is\", start_setup_date , \"end_setup_date is\", end_setup_date  )\n",
    "\n",
    "#create the driver table\n",
    "\n",
    "mernm_tbl  =  sch_nm + \".cor_wrk_mernm_stscd\"\n",
    "q_drop = 'drop table if exists {0}'.format(mernm_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "q_cr_mernm = \"\"\"create table {0} as\n",
    "                select mer.se10, mer.se_dba_nm, mer.se_sta_cd,  mer.se_sta_cd_ds\n",
    "                  from cstonedb3.gms_merchant_char mer,\n",
    "                       cstonedb3.gmdl_mer_spc spc\n",
    "                 where spc.spcl_prog_cd in ('002','020')\n",
    "                   and trim(spc.se10) = trim(mer.se10)\n",
    "                   and mer.se_setup_dt between  '{1}' and '{2}'\n",
    "                 group by mer.se10, mer.se_dba_nm, mer.se_sta_cd,  mer.se_sta_cd_ds\n",
    "            \"\"\".format(mernm_tbl, start_setup_date, end_setup_date )\n",
    "print(\"SQL to create the driver table : \", q_cr_mernm)\n",
    "sqlContext.sql(q_cr_mernm)\n",
    "f_get_tbl_cnt(mernm_tbl, '')\n",
    "\n",
    "job_end_time = time.time()\n",
    "\n",
    "elapse_time (job_start_time, job_end_time, pgm)\n",
    "print (\"end of \" + pgm )\n",
    "\n",
    "\n",
    " \n",
    "##  t_nw_bus_role.py\n",
    " \n",
    "# From Rajan RAJAN CHATTERJEE <RAJAN.CHATTERJEE@aexp.com> and it is not working\n",
    "print (\"Start of cor_val_bus_type.py\")\n",
    "\n",
    "################################################################################################################################################\n",
    "#ini - Initialization\n",
    "################################################################################################################################################\n",
    "\n",
    "# Invoke code to get parm to determine the location of the folder, schema name and yymm\n",
    "path_parm_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_parm_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "#load codefor Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "\n",
    "job_start_time = time.time()\n",
    "#load code to define the schema for defect table\n",
    "f_defect_tbl = path_code + \"cor_com_code_defect_tbl_sch.py\"\n",
    "exec(compile(open(f_defect_tbl, \"rb\").read(), f_defect_tbl, 'exec' ))\n",
    "\n",
    "\n",
    "#build key dictionary for group role type description, the key is the abbrevaitionn of role group\n",
    "d_bus_rol_grp_dsc = { 'S': 'Signer',\n",
    "                   'B': 'Beneficial Owner',\n",
    "                   'D': 'Director',\n",
    "                   'A': 'Authorized Contact'}\n",
    "\n",
    "f_lup  = path_data + \"cor_lup_leg_ent.csv\"\n",
    "df_lup = pd.read_csv(f_lup)\n",
    "l_val = df_lup['ent_cd_dsc'].tolist()\n",
    "l_key = df_lup['ent_cd'].tolist()\n",
    "d_ent_dsc = dict (zip(l_key, l_val))\n",
    "\n",
    "#ini3 - dynamically create the variables for required business role group\n",
    "dsc = \"Dynamically create the variables for required business role group.\"\n",
    "start_time = time.time()    # need to import time\n",
    "bus_rule_tbl = sch_nm + \".cor_lup_bus_rule\"\n",
    "q_cty = \"select cty from {0}  group by cty\".format(bus_rule_tbl)\n",
    "_l_cty = sqlContext.sql(q_cty).collect()\n",
    "for i in range(len(_l_cty)):\n",
    "    cty = _l_cty[i][\"cty\"]\n",
    "    q_ent_grp = 'select ent_grp from {0} where  cty= \"{1}\"  group by ent_grp sort by ent_grp'.format(bus_rule_tbl, cty)\n",
    "    _l_ent_grp = sqlContext.sql(q_ent_grp).collect()\n",
    "    for i  in range(len(_l_ent_grp)):\n",
    "        ent_grp = _l_ent_grp[i][\"ent_grp\"]\n",
    "        #print ('ent_grp', ent_grp)\n",
    "\n",
    "        q_rol_typ = 'select ent_grp, rol_typ from {0} where  cty= \"{1}\" and ent_grp = {2} and req_cd = \"M\"  '.format(bus_rule_tbl, cty, ent_grp)\n",
    "        #print (\"q_rol_typ\", q_rol_typ)\n",
    "        df_q = sqlContext.sql(q_rol_typ)\n",
    "        #df_q = sqlContext.sql(\"select ent_grp, rol_typ_desc, rol_typ from yuedb.cor_lup_bus_rule  req_cd=1 and cty='UK'\")\n",
    "        cty_ent_grp = cty.lower() + str(ent_grp)\n",
    "        # globals()['req_rol_typ_ent_{0}'.format(cty_ent_grp ) ] = df_q.groupby(\"ent_grp\").agg(F.collect_set(\"rol_typ\")).collect ()\n",
    "        l_rol_typ =  df_q.groupby(\"ent_grp\").agg(F.collect_set(\"rol_typ\")).collect()\n",
    "        globals()['req_rol_typ_ent_{0}'.format(cty_ent_grp ) ] =  l_rol_typ [0][\"collect_set(rol_typ)\"]\n",
    "end_time = time.time()  # need to import time\n",
    "elapse_time (start_time, end_time, dsc)\n",
    "\n",
    "################################################################################################################################################\n",
    "#Main process starts here\n",
    "################################################################################################################################################\n",
    "\n",
    "#yymm=str(datetime.today().strftime('%y%m'))\n",
    "bus_ctc_tbl = sch_nm + \".cor_bus_ctc_\" + yymm\n",
    "\n",
    "se10 = '9999999999'\n",
    "q_sdf = 'select toc_se10, se_lgl_ent_type_cd, se10, ent_grp, rol_typ, abbr_nm from {0} where  se10 <= \"{1}\" '.format(bus_ctc_tbl, se10)\n",
    "\n",
    "sdf = sqlContext.sql(q_sdf).toPandas()\n",
    "\n",
    "start_time_all_cty = time.time()\n",
    "_l_cty = sdf.abbr_nm.unique().tolist()\n",
    "_l_mis_ary=[]\n",
    "for cty in _l_cty:\n",
    "    start_time = time.time()\n",
    "\n",
    "    sc_ary =sdf.loc[sdf.abbr_nm==cty][\"se10\"].unique().tolist()\n",
    "    dsc_cty =  str(len(sc_ary)) + \" se10 had been validated for \" + cty + \".\"\n",
    "    print (\"Start to validate missing business role for \" + cty + \" with \" + str(len(sc_ary)) + \" se10. \")\n",
    "    sdf_rol = sdf[[\"se10\",\"ent_grp\",\"rol_typ\"]].groupby('se10').agg({'rol_typ':lambda x: list(x), 'ent_grp':lambda y: list(y)[0]})\n",
    "    for _se10 in sc_ary:\n",
    "\n",
    "        l_rol_typ=sdf_rol[sdf_rol.index==str(_se10).strip()]['rol_typ'].tolist()[0]\n",
    "        ent_grp =  sdf_rol[sdf_rol.index==str(_se10).strip()]['ent_grp'].tolist()[0]\n",
    "        cty_ent_grp= cty.lower() + str(ent_grp)\n",
    "        req_rol_typ_ent = globals()['req_rol_typ_ent_{0}'.format(cty_ent_grp) ]\n",
    "        mis_rol_typ = list(set(req_rol_typ_ent) - set(l_rol_typ ))  #val1.5 - validate if role_typwe is missing\n",
    "        if len(mis_rol_typ) > 0:\n",
    "             _toc_se10 = sdf.loc[sdf.se10==str(_se10).strip()]['toc_se10'].values[0]\n",
    "             _se_lgl_ent_type_cd= sdf.loc[sdf.se10==str(_se10).strip()]['se_lgl_ent_type_cd'].values[0]\n",
    "             _ent_cd_dsc = d_ent_dsc[int(_se_lgl_ent_type_cd)] + \"- \" + _se_lgl_ent_type_cd\n",
    "             _rol_typ_grp    = \"   \"\n",
    "             _rol_typ_cd     = \"   \"\n",
    "             _rol_typ_cd_dsc = \"   \"\n",
    "             l_mis_rol_typ_dsc  = list(map(lambda x: d_bus_rol_grp_dsc[x], mis_rol_typ))   # convert rol_typ abbreviation to description. eg, ['S', 'D'] would be ['Signer', 'Director']\n",
    "             mis_sumy_det  = \"Missing Role : \" + \",\".join(l_mis_rol_typ_dsc)\n",
    "             \n",
    "             mis_sumy      = \"\"\n",
    "             _l_mis_ary.append((_toc_se10, _se10, _se_lgl_ent_type_cd, ent_grp, _ent_cd_dsc, _rol_typ_cd, _rol_typ_grp, _rol_typ_cd_dsc,  mis_sumy, mis_sumy_det ))\n",
    " \n",
    "    end_time = time.time()  \n",
    "    elapse_time (start_time, end_time, dsc_cty)\n",
    "print(\"_l_mis_ary\",_l_mis_ary[0:10])\n",
    "len(sc_ary)\n",
    "dsc_all_cty = \"Validate business role for all countries with \" + str ( len(_l_mis_ary)) + \" defect records.\"\n",
    "end_time_all_cty = time.time()  # need to import time\n",
    "elapse_time (start_time_all_cty, end_time_all_cty, dsc_all_cty)       \n",
    "\n",
    "df_defect = sqlContext.createDataFrame(sc.parallelize(_l_mis_ary), schema)\n",
    "sqlContext.registerDataFrameAsTable(df_defect, \"cor_defect\")\n",
    "\n",
    "defect_tbl= sch_nm + \".cor_wrk_defect_bus_rolex_\" + yymm\n",
    "\n",
    "q_drop = \"drop table if exists {0}\".format(defect_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "q_cr_tbl = \"create table {0} as select * from cor_defect\".format(defect_tbl)\n",
    "sqlContext.sql(q_cr_tbl)\n",
    "job_end_time = time.time()         \n",
    "job_dsc = \"Validate business role.\"\n",
    "elapse_time (job_start_time, job_end_time, job_dsc)\n",
    "print (\"end of cor_val_bus_type.py\")        \n",
    "cor_val_bus_role.py\n",
    " \n",
    "# Invoke code to get parm to determine the location of the folder, schema name and yymm\n",
    "path_parm_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_parm_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "\n",
    "#load codefor Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "\n",
    "job_start_time = time.time()\n",
    "#load code to define the schema for defect table\n",
    "f_defect_tbl = path_code + \"cor_com_code_defect_tbl_sch.py\"\n",
    "exec(compile(open(f_defect_tbl, \"rb\").read(), f_defect_tbl, 'exec' ))\n",
    "\n",
    "\n",
    "#build key dictionary for group role type description, the key is the abbrevaitionn of role group\n",
    "d_bus_rol_grp_dsc = { 'S': 'Signer',\n",
    "                   'B': 'Beneficial Owner',\n",
    "                   'D': 'Director',\n",
    "                   'A': 'Authorized Contact'}\n",
    "\n",
    "f_lup  = path_data + \"cor_lup_leg_ent.csv\"\n",
    "df_lup = pd.read_csv(f_lup)\n",
    "l_val = df_lup['ent_cd_dsc'].tolist()\n",
    "l_key = df_lup['ent_cd'].tolist()\n",
    "d_ent_dsc = dict (zip(l_key, l_val))\n",
    "\n",
    "#ini3 - dynamically create the variables for required business role group\n",
    "dsc = \"Dynamically create the variables for required business role group.\"\n",
    "start_time = time.time()    # need to import time\n",
    "bus_rule_tbl = sch_nm + \".cor_lup_bus_rule\"\n",
    "q_cty = \"select cty from {0}  group by cty\".format(bus_rule_tbl)\n",
    "_l_cty = sqlContext.sql(q_cty).collect()\n",
    "for i in range(len(_l_cty)):\n",
    "    cty = _l_cty[i][\"cty\"]\n",
    "    q_ent_grp = 'select ent_grp from {0} where  cty= \"{1}\"  group by ent_grp sort by ent_grp'.format(bus_rule_tbl, cty)\n",
    "    _l_ent_grp = sqlContext.sql(q_ent_grp).collect()\n",
    "    for i  in range(len(_l_ent_grp)):\n",
    "        ent_grp = _l_ent_grp[i][\"ent_grp\"]\n",
    "        #print ('ent_grp', ent_grp)\n",
    "\n",
    "        q_rol_typ = 'select ent_grp, rol_typ from {0} where  cty= \"{1}\" and ent_grp = {2} and req_cd = \"M\"  '.format(bus_rule_tbl, cty, ent_grp)\n",
    "        #print (\"q_rol_typ\", q_rol_typ)\n",
    "        df_q = sqlContext.sql(q_rol_typ)\n",
    "        #df_q = sqlContext.sql(\"select ent_grp, rol_typ_desc, rol_typ from yuedb.cor_lup_bus_rule  req_cd=1 and cty='UK'\")\n",
    "        cty_ent_grp = cty.lower() + str(ent_grp)\n",
    "        # globals()['req_rol_typ_ent_{0}'.format(cty_ent_grp ) ] = df_q.groupby(\"ent_grp\").agg(F.collect_set(\"rol_typ\")).collect ()\n",
    "        l_rol_typ =  df_q.groupby(\"ent_grp\").agg(F.collect_set(\"rol_typ\")).collect()\n",
    "        globals()['req_rol_typ_ent_{0}'.format(cty_ent_grp ) ] =  l_rol_typ [0][\"collect_set(rol_typ)\"]\n",
    "end_time = time.time()  # need to import time\n",
    "elapse_time (start_time, end_time, dsc)\n",
    "         \n",
    "################################################################################################################################################\n",
    "#  Main process starts here\n",
    "################################################################################################################################################\n",
    "\n",
    "bus_ctc_tbl = sch_nm + \".cor_bus_ctc_\" + yymm\n",
    "#bus_ctc_tbl = \"yuedb.cor_bus_ctc_\" + yymm   # table with index & rol_typ\n",
    "\n",
    "se10 = '9999999999'\n",
    "_sdf = 'select toc_se10, se10, ent_grp, rol_typ, abbr_nm, se_lgl_ent_type_cd from {0} where se10 <= \"{1}\" '.format(bus_ctc_tbl, se10)\n",
    "#q_sdf = 'select toc_se10, se10, ent_grp, rol_typ, abbr_nm, se_lgl_ent_type_cd from {0} where abbr_nm = \"SPAIN\" and se10 <= \"{1}\" '.format(bus_ctc_tbl, se10)\n",
    "\n",
    "sdf = sqlContext.sql(q_sdf).cache()  # cache doesn't benifit the performance\n",
    "#sdf = sqlContext.sql(q_sdf)\n",
    "\n",
    "\n",
    "start_time_all_cty = time.time()\n",
    "_l_cty = sdf.select(\"abbr_nm\" ).distinct().collect()  \n",
    "_l_mis_ary=[]\n",
    "for _cty in _l_cty:\n",
    "    cty  = _cty.__getitem__(\"abbr_nm\")  \n",
    "    start_time = time.time()\n",
    "\n",
    "    sc_ary =sdf.where(sdf.abbr_nm  == cty).select(\"se10\").distinct().collect()\n",
    "    dsc_cty =  str(len(sc_ary)) + \" se10 had been validated for \" + cty + \".\"\n",
    "                   \n",
    "    for _sc in sc_ary:  \n",
    "        _se10 = _sc.__getitem__(\"se10\")\n",
    "        _l_rol_typ = sdf.where(sdf.se10 == _se10).select([\"ent_grp\", \"rol_typ\", \"toc_se10\", \"se_lgl_ent_type_cd\" ]).collect()         \n",
    "        l_rol_typ=[] #val1.3  - Create a List all the role types for the SE from the the query result\n",
    "    \n",
    "        for _rol_typ  in _l_rol_typ:\n",
    "            rol_typ = list(_rol_typ.__getitem__(\"rol_typ\"))\n",
    "            l_rol_typ.append(rol_typ)\n",
    "            \n",
    "        l_rol_typ = sum(l_rol_typ, [])    #val1.4 -  Create Flat Lists Out Of Lists\n",
    "        ent_grp =  _l_rol_typ[0][\"ent_grp\"]\n",
    "        cty_ent_grp= cty.lower() + str(ent_grp)\n",
    "        req_rol_typ_ent = globals()['req_rol_typ_ent_{0}'.format(cty_ent_grp) ]\n",
    "        mis_rol_typ = list(set(req_rol_typ_ent) - set(l_rol_typ ))  #val1.5 - validate if role_typwe is missing\n",
    "        if len(mis_rol_typ) > 0:\n",
    "             _toc_se10            = _l_rol_typ[0][\"toc_se10\"]      \n",
    "             _se_lgl_ent_type_cd  = _l_rol_typ[0][\"se_lgl_ent_type_cd\"]    \n",
    "             _ent_cd_dsc     = d_ent_dsc[int(_se_lgl_ent_type_cd)] + \"- \" + _se_lgl_ent_type_cd\n",
    "             _rol_typ_grp    = \"   \"\n",
    "             _rol_typ_cd     = \"   \"\n",
    "             _rol_typ_cd_dsc = \"   \"\n",
    "             l_mis_rol_typ_dsc  = list(map(lambda x: d_bus_rol_grp_dsc[x], mis_rol_typ))   # convert rol_typ abbreviation to description. eg, ['S', 'D'] would be ['Signer', 'Director']\n",
    "             mis_sumy_det  = \"Missing Role : \" + \",\".join(l_mis_rol_typ_dsc)\n",
    "             \n",
    "             mis_sumy      = \"\"\n",
    "             _l_mis_ary.append((_toc_se10, _se10, _se_lgl_ent_type_cd, ent_grp, _ent_cd_dsc, _rol_typ_cd, _rol_typ_grp, _rol_typ_cd_dsc,  mis_sumy, mis_sumy_det ))\n",
    " \n",
    "    end_time = time.time()  \n",
    "    elapse_time (start_time, end_time, dsc_cty)\n",
    "print(\"_l_mis_ary\",_l_mis_ary[0:10])\n",
    "len(sc_ary)\n",
    "dsc_all_cty = \"Validate business role for all countries with \" + str ( len(_l_mis_ary)) + \" defect records.\"\n",
    "end_time_all_cty = time.time()  # need to import time\n",
    "elapse_time (start_time_all_cty, end_time_all_cty, dsc_all_cty)         \n",
    "\n",
    "df_defect = sqlContext.createDataFrame(sc.parallelize(_l_mis_ary), schema)\n",
    "sqlContext.registerDataFrameAsTable(df_defect, \"cor_defect\")\n",
    "\n",
    "defect_tbl= sch_nm + \".cor_wrk_defect_bus_role_\" + yymm\n",
    "\n",
    "q_drop = \"drop table if exists {0}\".format(defect_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "q_cr_tbl = \"create table {0} as select * from cor_defect\".format(defect_tbl)\n",
    "sqlContext.sql(q_cr_tbl)\n",
    "f_get_tbl_cnt(defect_tbl, \"\")\n",
    "job_end_time = time.time()         \n",
    "job_dsc = \"Validate business role.\"\n",
    "elapse_time (job_start_time, job_end_time, job_dsc)\n",
    "print (\"end of cor_val_bus_type.py\")      \n",
    "\n",
    "cor_val_bus_data.py\n",
    "pgm = \"cor_val_bus_data.py\"\n",
    "print (\"Start of \" + pgm)\n",
    "################################################################################################################################################\n",
    "#ini - Initialization\n",
    "################################################################################################################################################\n",
    "\n",
    "# Invoke code to get parm to determine the location of the folder, schema name and yymm\n",
    "path_parm_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_parm_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "#load code for Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "job_start_time = time.time()\n",
    "\n",
    "dsc = \"Initialization.\"\n",
    "start_time = time.time()    # need to import time\n",
    "\n",
    "\n",
    "def f_isnull(value):\n",
    "    if value: return False\n",
    "    else: return True\n",
    "\n",
    "#load code to define the schema for defect table\n",
    "f_defect_tbl = path_code + \"cor_com_code_defect_tbl_sch.py\"\n",
    "exec(compile(open(f_defect_tbl, \"rb\").read(), f_defect_tbl, 'exec' ))\n",
    "\n",
    "#build key dictionary for group role type description, the key is the abbrevaitionn of role group\n",
    "d_bus_rol_grp_dsc = { 'S': 'Signer',\n",
    "                   'B': 'Beneficial Owner',\n",
    "                   'D': 'Director',\n",
    "                   'A': 'Authorized Contact'}\n",
    "\n",
    "f_lup  = path_data + \"cor_lup_leg_ent.csv\"\n",
    "df_lup = pd.read_csv(f_lup)\n",
    "l_val = df_lup['ent_cd_dsc'].tolist()\n",
    "l_key = df_lup['ent_cd'].tolist()\n",
    "d_ent_dsc = dict (zip(l_key, l_val))\n",
    "\n",
    "f_lup  = path_data + \"cor_lup_bus_role.csv\"\n",
    "df_lup = pd.read_csv(f_lup)\n",
    "l_val  = df_lup['bus_rol_dsc'].tolist()\n",
    "l_key  = df_lup['bus_rol_cd'].tolist()\n",
    "d_bus_rol_dsc  = dict (zip(l_key, l_val))\n",
    "\n",
    "f_lup  = path_data + \"cor_lup_corns_fld.csv\"\n",
    "df_lup = pd.read_csv(f_lup)\n",
    "l_val = df_lup['bus_fld_nam'].tolist()\n",
    "l_key = df_lup['corns_fld_nam'].tolist()\n",
    "d_fld_nam  = dict (zip(l_key, l_val))\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#ini - Get country name  \n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "_bus_ctc_tbl = sch_nm + \".cor_bus_ctc_\" + yymm\n",
    "f_get_tbl_cnt(_bus_ctc_tbl, \"\")\n",
    "\n",
    "bus_ctc_tbl = sqlContext.table(_bus_ctc_tbl).select(\"abbr_nm\").toPandas()\n",
    "_l_ctry_nm = bus_ctc_tbl.abbr_nm.unique().tolist()   # will be used by loop later e.g. ['AUSTRALIA', 'DENMARK', 'NORWAY', 'SPAIN', 'UK'] ;\n",
    "#_l_ctry_nm = [str.encode('ascii') for str in _l_ctry_nm]   # convert from unicode  to ascii; having 'unicode' object is not callable for the following code\n",
    "#s_ctry_nm_in = str(tuple(_l_ctry_nm))  # convert the list to string and will feed qry e.g \"('AUSTRALIA', 'DENMARK', 'NORWAY', 'SPAIN', 'UK')\"\n",
    "s_ctry_nm_in = str(tuple(_l_ctry_nm)).replace(\"u'\", \"'\").replace(',)', ')')  # convert the list to string and will feed qry e.g \"('AUSTRALIA', 'DENMARK', 'NORWAY', 'SPAIN', 'UK')\"\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#ini - dynamically create variables with suffix as rule id. e.g. if rule_id =  uk2a  \n",
    "#      \n",
    "#      - val_s_var_fld_uk2a   : a string with required fields for validation  .e.g. 'CTC_NM, BIRTH_DT, RESIDE_AD_LINE1, .... RESIDE_CTRY_CD'\n",
    "#      - val_no_var_fld_uk2a : no of required fields\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "_s_req_fld = \"\"\n",
    "NoneType = type(None)\n",
    "bus_rule_tbl = sch_nm + \".cor_lup_bus_rule\"\n",
    "q_bus_rule = \"select *  from {0} where cty in {1} \".format(bus_rule_tbl, s_ctry_nm_in)\n",
    "#print(\"q_bus_rule \", q_bus_rule, \"\\n\", \"s_ctry_nm_in\" , s_ctry_nm_in , '\\n', \"_l_ctry_nm\", _l_ctry_nm, \"tuple(_l_ctry_nm\",  tuple(_l_ctry_nm ))\n",
    "bus_rul = sqlContext.sql(q_bus_rule ).collect()\n",
    "for i in range(len(bus_rul)):\n",
    "\n",
    "    rul_id = bus_rul[i][\"rul_id\"].lower()      # get the rule id\n",
    "    s_req_fld = bus_rul[i][\"req_fld\"]        # return a string with variable(s) specified in reg_fld. e.g. 'CTC_NM  BIRTH_DT  RESIDE_AD_LINE1 .... RESIDE_CTRY_CD'\n",
    "    globals()['val_req_cd_{0}'.format( rul_id)] =  bus_rul[i][\"req_cd\"].lower()\n",
    " \n",
    "    # return a list of fields required for validation  \n",
    "    globals()['val_no_var_fld_{0}'.format( rul_id) ] = 0\n",
    "    if type(s_req_fld) != NoneType:\n",
    "       _s_req_fld = _s_req_fld + \" \" + s_req_fld   # type = string       \n",
    "       l_req_fld = s_req_fld.split()             # convert the string to a list, e.g.   ['CTC_NM', 'BIRTH_DT', 'RESIDE_AD_LINE1', ....  'RESIDE_CTRY_CD']\n",
    "       globals()['val_no_var_fld_{0}'.format( rul_id)]  =  len(l_req_fld)\n",
    "\n",
    "       s_req_fld = re.sub(' +', ' ', s_req_fld.strip() ).replace(\" \", \", \") # change duplicate spaces to a single space and replace space to a comma e.g. 'CTC_NM, BIRTH_DT, RESIDE_AD_LINE1, .... RESIDE_CTRY_CD'\n",
    "       globals()['val_s_var_fld_{0}'.format( rul_id)]    =  s_req_fld     # sting with fields required for validation. will be used in a  query later\n",
    "       globals()['val_l_var_fld_{0}'.format( rul_id)]    =  l_req_fld     # list with fields required for validation .e.g.  ['CTC_NM', 'BIRTH_DT', 'RESIDE_AD_LINE1', ....  'RESIDE_CTRY_CD']\n",
    "\n",
    "l_req_fld = list(set(re.sub(' +', ' ', _s_req_fld).split()))  # convert a string to a list; convert the list to a set to dedup; convert set to a list\n",
    "s_req_fld = ', '.join(l_req_fld)  # return a string like 'ID_DOC_ID, ID_DOC_EXPR_DT, ...'\n",
    "\n",
    "end_time = time.time()  # need to import time\n",
    "elapse_time (start_time, end_time, dsc)\n",
    "########################################################################################################################################################\n",
    "#  Main process starts here\n",
    "########################################################################################################################################################\n",
    "\n",
    "start_time = time.time()\n",
    "dsc = 'Validate business data:'\n",
    "se10 = '9999999999'  # for testing, set up the se10 to different number for testing\n",
    "\n",
    "wrk_var     = \"toc_se10, se10, abbr_nm, se_lgl_ent_type_cd, ent_grp, bus_rol_grp_abb_1, first_rspbl_cd as rspbl_cd_1,  bus_rol_grp_abb_2, scnd_rspbl_cd as rspbl_cd_2,\" + \\\n",
    "             \"bus_rol_grp_abb_3, third_rspbl_cd as rspbl_cd_3, bus_rol_grp_abb_4, fourth_rspbl_cd as rspbl_cd_4,  bus_rol_grp_abb_5, fifth_rspbl_cd as rspbl_cd_5, bus_ctc_id, \" + s_req_fld\n",
    "\n",
    "wrk_tbl = sch_nm + \".cor_wrk_bus_ctc\" + yymm\n",
    "\n",
    "q_drop = 'drop table if exists {0}'.format(wrk_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "\n",
    "q_cr_wrk_tbl = 'create table {0} as select {1} from {2}  where abbr_nm in {3} and  se10 <= \"{4}\"'.format(wrk_tbl, wrk_var, _bus_ctc_tbl, s_ctry_nm_in, se10)\n",
    "sqlContext.sql(q_cr_wrk_tbl)\n",
    "\n",
    "idx_nm = \"wrk_ctc_idx_cmp\" + yymm\n",
    "f_cr_idx(wrk_tbl, idx_nm, \"(abbr_nm, se10)\", \"cmp\")\n",
    "\n",
    "_bus_ctc_tbl = wrk_tbl\n",
    "q_bus_ctc = 'select * from  {0} sort by abbr_nm, toc_se10, se10'.format(_bus_ctc_tbl)\n",
    "#q_bus_ctc = 'select * from  {0}  where abbr_nm in {1} and  se10 <= \"{2}\" sort by abbr_nm, toc_se10, se10'.format(_bus_ctc_tbl, s_ctry_nm_in, se10)  #Testing\n",
    "sdf_all =  sqlContext.sql(q_bus_ctc).cache()\n",
    "print (  sdf_all.count(),  \" rows to process \")\n",
    "_l_mis_fld_ary = []\n",
    "_max_rol_typ_no = 5 # for eact contract ID, the max role type is 5 with index from 1 to 5\n",
    "_max_rol_typ_no = _max_rol_typ_no + 1\n",
    "#_l_ctry_nm = ['HUNGARY']   #Testing\n",
    " \n",
    "sel_item = [\"toc_se10\", \"se10\",\"abbr_nm\", \"se_lgl_ent_type_cd\", \"ent_grp\", \"bus_rol_grp_abb_1\", \"rspbl_cd_1\",  \"bus_rol_grp_abb_2\",\"rspbl_cd_2\",  \"bus_rol_grp_abb_3\", \"rspbl_cd_3\",  \n",
    "            \"bus_rol_grp_abb_4\", \"rspbl_cd_4\",   \"bus_rol_grp_abb_5\", \"rspbl_cd_5\", \"bus_ctc_id\"]\n",
    "\n",
    "for _cty in _l_ctry_nm:\n",
    "    _l_se_cty  = sdf_all.where( sdf_all.abbr_nm  == _cty ).select(sel_item).toPandas()  # all rows within a country\n",
    "    \n",
    "    print (\"Records \", len(_l_se_cty),  \"for country \", _cty )\n",
    "    dsc_cty =  str(len(_l_se_cty )) + \" records  had been validated for \" + _cty + \".\"\n",
    "    ratio = 0.2\n",
    "    cnt = 0.0\n",
    "    tot_cnt = len(_l_se_cty)\n",
    "    cty_start_time = time.time()\n",
    "    cty_time_start = cty_start_time  \n",
    "    for ix in range(tot_cnt):\n",
    "        cnt = cnt + 1\n",
    "        per_cnt = cnt / tot_cnt  \n",
    "        if per_cnt  >= ratio:  \n",
    "           cty_end_time = time.time()\n",
    "           elapsed =  cty_end_time -  cty_start_time\n",
    "           print (\"Complete %5.2f.\" %( per_cnt), \"It took %3f second.\" %(elapsed))\n",
    "           ratio = ratio + 0.2\n",
    "           cty_start_time = cty_end_time        \n",
    "\n",
    "           \n",
    "        _se10 = _l_se_cty.loc[ix, \"se10\"]\n",
    "        bus_ctc_id      = _l_se_cty.loc[ix, \"bus_ctc_id\"]\n",
    "        _toc_se10       = _l_se_cty.loc[ix, \"toc_se10\"]\n",
    "        _rol_cd_1       = _l_se_cty.loc[ix, \"rspbl_cd_1\"]\n",
    "        _rol_grp_abb_1  = _l_se_cty.loc[ix, \"bus_rol_grp_abb_1\"]\n",
    "        _rol_cd_2       = _l_se_cty.loc[ix, \"rspbl_cd_2\"]\n",
    "        _rol_grp_abb_2  = _l_se_cty.loc[ix, \"bus_rol_grp_abb_2\"]\n",
    "        _rol_cd_3       = _l_se_cty.loc[ix, \"rspbl_cd_3\"]        \n",
    "        _rol_grp_abb_3  = _l_se_cty.loc[ix, \"bus_rol_grp_abb_3\"]\n",
    "        _rol_cd_4       = _l_se_cty.loc[ix, \"rspbl_cd_4\"]\n",
    "        _rol_grp_abb_4  = _l_se_cty.loc[ix, \"bus_rol_grp_abb_4\"]\n",
    "        _rol_cd_5       = _l_se_cty.loc[ix, \"rspbl_cd_5\"]\n",
    "        _rol_grp_abb_5  = _l_se_cty.loc[ix, \"bus_rol_grp_abb_5\"]  \n",
    "       \n",
    "        for j in range(1, _max_rol_typ_no):\n",
    "              _rol_typ_cd         = globals()['_rol_cd_{0}'.format(j) ]    \n",
    "              _rol_typ_grp        = globals()['_rol_grp_abb_{0}'.format(j) ]\n",
    "              _rol_grp_abb        = globals()['_rol_grp_abb_{0}'.format(j) ]\n",
    "              # the _rol_typ_cd might defined as '   ', or Null\n",
    "              if  ( _rol_typ_cd  != None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Type and _rol_typ_cd  != '   ' and   _rol_grp_abb  != None):           \n",
    "                  cty_nm      =  _cty.lower()\n",
    "                  _ent_grp           = _l_se_cty.loc[ix, \"ent_grp\"]\n",
    "                  _se_lgl_ent_type_cd= _l_se_cty.loc[ix, \"se_lgl_ent_type_cd\"]\n",
    "                  _ent_cd_desc  = d_ent_dsc[int(_se_lgl_ent_type_cd)] + \"- \" + _se_lgl_ent_type_cd  \n",
    "                  _rol_typ_cd_dsc = d_bus_rol_dsc[int(_rol_typ_cd) ]  \n",
    "                  rul_id      =  cty_nm + str(_ent_grp) +  _rol_typ_grp.lower()\n",
    "                  req_cd      =  globals()['val_req_cd_{0}'.format( rul_id) ]  \n",
    "                  if (req_cd != 'o' ):  \n",
    "                      s_val_fld   =  globals()['val_s_var_fld_{0}'.format( rul_id)]\n",
    "                      l_val_fld   =  ['se10' ] + globals()['val_l_var_fld_{0}'.format( rul_id)]  \n",
    "                      no_val_fld  =  globals()['val_no_var_fld_{0}'.format( rul_id) ]\n",
    "               \n",
    "                      l_bus_data =   sdf_all.where( sdf_all.se10 == _se10).where(sdf_all.toc_se10 == _toc_se10).where(sdf_all.bus_ctc_id   == bus_ctc_id ).select(l_val_fld).toPandas()\n",
    "                                                              \n",
    "                      #fld_len  = no_val_fld\n",
    "                      fld_data = l_bus_data.iloc[0, 1:no_val_fld + 1]   # return type as tuple; get all the values for the fields  related to field ID from SQL result get all the values required ftype (fld_datal_data = [ x if x is None else x.strip() for x in list(fld_data) ]\n",
    "                      l_data = [ x if x is None else x.strip() for x in list(fld_data) ]\n",
    "                      data_isnull    = list(map(f_isnull,l_data))\n",
    "                      no_fld_eq_null = sum(map(f_isnull,l_data))\n",
    "                      #print(_se10, \"fld_data\", fld_data, \"no_fld_eq_null\", no_fld_eq_null, \"no_fld_eq_null\", no_fld_eq_null , '\\n',  \"no_val_fld\", no_val_fld, \"req_cd\", req_cd)\n",
    "                                      \n",
    "                      if (req_cd == 'm' and no_fld_eq_null > 0 ) or (req_cd == 'c' and no_fld_eq_null > 0 and no_fld_eq_null  !=  no_val_fld) :\n",
    "                                                \n",
    "                         mis_sumy= \"\"\n",
    "                         mis_sumy_det = \"Missing Field: \"\n",
    "                         iz = 0\n",
    "                         for iy in range(no_val_fld ):\n",
    "                              \n",
    "                             if data_isnull[iy]:\n",
    "                                iz  = iz + 1\n",
    "                                fld_name = s_val_fld.strip().split(',')[iy]                  \n",
    "                                mis_sumy = mis_sumy + fld_name\n",
    "                                if iz == 1:\n",
    "                                   mis_sumy_det = mis_sumy_det + d_fld_nam[fld_name.strip()]\n",
    "                                else:    \n",
    "                                   mis_sumy_det = mis_sumy_det + \", \" + d_fld_nam[fld_name.strip()]\n",
    "                                             \n",
    "                         _l_mis_fld_ary.append((_toc_se10, _se10, _se_lgl_ent_type_cd, _ent_grp, _ent_cd_desc, _rol_typ_cd, _rol_typ_grp, _rol_typ_cd_dsc, mis_sumy, mis_sumy_det ))\n",
    "      \n",
    "    end_time = time.time()\n",
    "    elapse_time (cty_time_start, end_time, dsc_cty)                        \n",
    "\n",
    "end_time = time.time()  # need to import time\n",
    "elapse_time (start_time, end_time, dsc)\n",
    "\n",
    "#vav - load data\n",
    "df_defect = sqlContext.createDataFrame(sc.parallelize(_l_mis_fld_ary), schema)\n",
    "sqlContext.registerDataFrameAsTable(df_defect, \"cor_defect\")\n",
    "sqlContext.registerDataFrameAsTable(df_defect, \"cor_defect\")\n",
    "defect_tbl= sch_nm + \".cor_wrk_defect_bus_data_\" + yymm\n",
    "\n",
    "q_drop = \"drop table if exists {0}\".format(defect_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "q_cr_tbl = \"create table {0} as select * from cor_defect\".format(defect_tbl)\n",
    "sqlContext.sql(q_cr_tbl)\n",
    "\n",
    "f_get_tbl_cnt(defect_tbl, \"\")\n",
    "job_end_time = time.time()\n",
    "job_dsc = \"Validate business data for all countries.\"\n",
    "elapse_time (job_start_time, job_end_time, job_dsc)\n",
    "print (\"End of \" + pgm)\n",
    "cor_rpt_mrg_defect_tbl.py\n",
    "pgm = \"cor_rpt_mrg_defect_tbl.py\"\n",
    "path_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "pgm = \"cor_bld_defect_rpt.py\"\n",
    "print (\"Start of \" + pgm)\n",
    "################################################################################################################################################\n",
    "#ini - Initialization\n",
    "################################################################################################################################################\n",
    "\n",
    "# Invoke code to get parm to determine the location of the folder, schema name and yymm\n",
    "path_parm_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_parm_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "#load code for Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "job_start_time = time.time()\n",
    "u_get_mis_sumy_for_mis_role= udf(f_get_mis_sumy_for_mis_role)\n",
    "\n",
    "################################################################################################################################################\n",
    "#  Main process starts here\n",
    "################################################################################################################################################\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Combine business role defect table and business data defect table into a defect table\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "bus_ctc_tbl = sch_nm + \".cor_bus_ctc_\" + yymm\n",
    "wrk_var     = \"dfct.toc_se10 ,dfct.se10, ctc.se_dba_nm,  dfct.ent_cd_dsc, dfct.rol_typ_cd, dfct.rol_typ_cd_dsc, dfct.mis_sumy_det, dfct.rol_typ_grp\"\n",
    "dfct_tbl    = sch_nm + \".cor_defect_\" + yymm\n",
    "dfct_role_tbl  =  sch_nm + \".cor_wrk_defect_bus_role_\" + yymm\n",
    "q_bus_role = \"select * from {0}\".format(dfct_role_tbl)\n",
    "sdf_bus_role = sqlContext.sql (q_bus_role)\n",
    "sdf_bus_role_new = sdf_bus_role.drop(\"mis_sumy\").withColumn('mis_sumy',u_get_mis_sumy_for_mis_role(\"mis_sumy_det\") ).select(\"toc_se10\", \"se10\", \"se_lgl_ent_type_cd\", \"ent_grp\", \"ent_cd_dsc\",  \"rol_typ_cd\", \"rol_typ_grp\", \"rol_typ_cd_dsc\", \"mis_sumy\", \"mis_sumy_det\")\n",
    "\n",
    "sdf_bus_role_new.registerTempTable('tmp_bus_rol_tbl')\n",
    "dfct_role_tbl = \"tmp_bus_rol_tbl\"\n",
    "dfct_data_tbl  =  sch_nm + \".cor_wrk_defect_bus_data_\" + yymm\n",
    "q_drop = 'drop table {0}'.format(dfct_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "\n",
    "q_cr_dfct_tbl  = \"\"\" create table {0}  as  select * from\n",
    "                    (select  * from {1}\n",
    "                     union all\n",
    "                     select  *  from {2}\n",
    "                     ) tmp\n",
    "                \"\"\".format(dfct_tbl,  dfct_role_tbl, dfct_data_tbl )\n",
    "sqlContext.sql(q_cr_dfct_tbl)\n",
    "f_get_tbl_cnt(dfct_tbl, \"\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Combine a table to get the merchant name for reporting pupose\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "dfct_mer_nm_tbl = sch_nm + \".cor_wrk_dfct_mer_nm_\" + yymm\n",
    "q_drop = 'drop table {0}'.format (dfct_mer_nm_tbl)\n",
    "sqlContext.sql(q_drop)\n",
    "\n",
    "q_cr_dfct_mer_nm = \"\"\"create table {0} as\n",
    "                    select {1}\n",
    "                    from  {2} dfct\n",
    "                    left  outer join {3} ctc on dfct.se10 = ctc.se10\n",
    "                    group by {1}\"\"\".format(dfct_mer_nm_tbl, wrk_var, dfct_tbl, bus_ctc_tbl)\n",
    "sqlContext.sql(q_cr_dfct_mer_nm )\n",
    "\n",
    "f_get_tbl_cnt(dfct_mer_nm_tbl, '')\n",
    "job_end_time = time.time()\n",
    "elapse_time (job_start_time, job_end_time, pgm)\n",
    "print (\"end of \" + pgm )      \n",
    "                    \n",
    "cor_rpt_defect_rpt.py\n",
    "pgm = \"cor_rpt_defect_rpt.py\"\n",
    "print (\"Start of \" + pgm)\n",
    "################################################################################################################################################\n",
    "#ini - Initialization\n",
    "################################################################################################################################################\n",
    "\n",
    "# Invoke code to get parm to determine the location of the folder, schema name and yymm\n",
    "path_parm_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_parm_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "#load code for Initialization\n",
    "print(\" bef init\" ,  'path_code', path_code, '\\n', 'path_data', path_data)\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "job_start_time = time.time()\n",
    "print(\" aft init\" ,  'path_code', path_code, '\\n', 'path_data', path_data)\n",
    "\n",
    "################################################################################################################################################\n",
    "#  Main process starts here\n",
    "################################################################################################################################################\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Create a dataframe from the defect table required for excel file\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "dfct_tbl = sch_nm +  \".cor_wrk_dfct_mer_nm_\" + yymm\n",
    "\n",
    "q_sel = \"select * from {0}\".format(dfct_tbl)\n",
    "sdf_dfct= sqlContext.sql(q_sel)\n",
    "_l_mis_ary = sdf_dfct.collect()\n",
    "\n",
    "labels =  [\"Pay Manager SE10\", \"Merchant SE10\",  \"Merchant Name\",  \"Legal Entity\", \"Role Code\",  \\\n",
    "           \"Role Code Description\",  \"Quality  Summary (Missing fields)\", \"_rol_typ_grp\" ]\n",
    "\n",
    "df_from_list=pd.DataFrame.from_records(_l_mis_ary, columns = labels)              # create a pd  dataframe\n",
    "df_from_list.sort_values(['Merchant SE10', '_rol_typ_grp' ], inplace = True)     \n",
    "#df_from_list.sort(['Merchant SE10', '_rol_typ_grp' ], inplace = True)            # \"AttributeError: 'DataFrame' object has no attribute 'sort'; use 'sort_values' instead\n",
    "df_from_list.drop(['_rol_typ_grp'], axis=1, inplace=True)                         # drop no neeed columns; cause drop() got an unexpected keyword argument 'inplace'\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Build up a look up dictionary to get ctry_abbr_nm from toc_se10\n",
    "# - There is no corresponding table for cor_lup_partner_grp.csv which is used by reporting only\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print(\" bef f_lup\" ,  'path_code', path_code, '\\n', 'path_data', path_data)\n",
    "\n",
    "f_lup  = path_data + \"cor_lup_partner_grp.csv\"\n",
    "df_lup = pd.read_csv(f_lup)\n",
    "df_lup.sort_values(['partner' ],  inplace = True)\n",
    "\n",
    "l_val  = df_lup['ctry_abbr_nm'].tolist()\n",
    "l_key  = df_lup['toc_se10'].tolist()\n",
    "d_toc_se10_ctry  = dict (zip(l_key, l_val))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#create report\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "nxt_yymm = f_dte_for_yymm(yymm, 'n')\n",
    "mmddccyy = f_dte_for_yymm(nxt_yymm, 'f2')\n",
    "\n",
    "for _partner  in df_lup['partner'].unique():\n",
    "    df_lup_new = df_lup[df_lup['partner'] == _partner]\n",
    "    # print(\"df_lup_new\", df_lup_new, \"_partner\",  _partner)\n",
    "    _w_file_wrt_ind = 0\n",
    "    for _toc_se in  df_lup_new[\"toc_se10\"].unique():       \n",
    "        df_toc_se = df_from_list[df_from_list[\"Pay Manager SE10\"] == str(_toc_se)]\n",
    "        _w_count = df_from_list[df_from_list[\"Pay Manager SE10\"] == str(_toc_se)][\"Pay Manager SE10\"].count()\n",
    "       \n",
    "        if  _w_count > 0:\n",
    "            if _w_file_wrt_ind == 0 :\n",
    "               excel_file = path_data +  str(_partner) + \"_\" + str(mmddccyy) + \".xlsx\"\n",
    "               print (\"excel_file \", excel_file )\n",
    "               writer = pd.ExcelWriter(excel_file)\n",
    "               _w_file_wrt_ind = 1\n",
    "            ctry = d_toc_se10_ctry[_toc_se]                  \n",
    "            sheet_name =  str(_toc_se) + \"_\" + ctry            \n",
    "            df_toc_se.to_excel(writer,sheet_name, index=False)     \n",
    "               \n",
    "    if   _w_file_wrt_ind == 1:\n",
    "         writer.save()\n",
    "\n",
    "\n",
    "job_end_time = time.time()\n",
    "\n",
    "elapse_time (job_start_time, job_end_time, pgm)\n",
    "print (\"end of \" + pgm )       \n",
    "\n",
    "cor_prg_bld_trk.py\n",
    "\n",
    "pgm = \"cor_prg_bld_trk.py\"\n",
    "print (\"Start of \" + pgm)\n",
    "################################################################################################################################################\n",
    "#ini - Initialization\n",
    "################################################################################################################################################\n",
    "\n",
    "# Invoke code to get parm to determine the location of the folder and yymm\n",
    "path_parm_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_parm_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "#load code for Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "job_start_time = time.time()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Build dictionary\n",
    "# - Invoke function 'f_cr_dic_from_csv' to build dictionary d_fld_nam  to map the field name from CS to bus field name\n",
    "# - Invoke 'f_cr_dic_from_tbl' to get bus role description\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "csv_file  = path_data + 'cor_lup_corns_fld.csv'\n",
    "#global d_bus_rol_grp_dsc, d_bus_rol_grp_dsc\n",
    "d_fld_nam =  f_cr_dic_from_csv(csv_file, 'bus_fld_nam', 'corns_fld_nam')\n",
    "\n",
    "bus_rul_tbl = sch_nm + \".cor_lup_bus_rule\"\n",
    "d_bus_rol_grp_dsc =  f_cr_dic_from_tbl(bus_rul_tbl, \"rol_typ\", \"rol_typ_dsc\")\n",
    "\n",
    "################################################################################################################################################\n",
    "#  Main process starts here\n",
    "################################################################################################################################################\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Create a dataframe from the defect table required for excel file\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "cur_yymm = yymm\n",
    "prv_yymm = f_dte_for_yymm(yymm, 'p')\n",
    "print (\"process \" + yymm + \" data.\" + \" Previous month is \" + prv_yymm )\n",
    "\n",
    "\n",
    "cur_dfct_tbl = sch_nm + \".cor_defect_\" + str(cur_yymm)\n",
    "prv_dfct_tbl = sch_nm + \".cor_defect_\" + str(prv_yymm)\n",
    "prv_prg_trk_tbl =  sch_nm + \".cor_dfct_prg_trk_\"  + str (prv_yymm)\n",
    "\n",
    "\n",
    "q_sdf_dfct_key_cur =  \"select toc_se10, se10, rol_typ_cd  from {0} group by toc_se10, se10, rol_typ_cd\".format(cur_dfct_tbl)\n",
    "sdf_dfct_key_cur  =  sqlContext.sql(q_sdf_dfct_key_cur)   #sdf\n",
    "\n",
    "# will use toc_se10, se10, rol_typ_cd as key to check the completion and new defect\n",
    "q_sdf_dfct_key_prv =  \"select toc_se10, se10, rol_typ_cd  from {0} group by toc_se10, se10, rol_typ_cd\".format(prv_dfct_tbl)\n",
    "sdf_dfct_key_prv  =  sqlContext.sql(q_sdf_dfct_key_prv)   #sdf\n",
    "\n",
    "sdf_cmp_key_dfct  = sdf_dfct_key_prv.subtract(sdf_dfct_key_cur)  #\n",
    "sdf_cmp_key_dfct.registerTempTable('tmp_cmp_key_dfct_tbl')  #register temp table\n",
    "print(\"Count for toc_se10,  se10, rol_typ_cd  in previous defect report only = \",   f_get_tbl_cnt('tmp_cmp_key_dfct_tbl', ''))\n",
    "      \n",
    "sdf_new_key_dfct  = sdf_dfct_key_cur.subtract(sdf_dfct_key_prv)  #\n",
    "sdf_new_key_dfct.registerTempTable('tmp_new_key_dfct_tbl')  #register temp table\n",
    "print(\"Count for toc_se10,  se10, rol_typ_cd  in current defect report only = \",   f_get_tbl_cnt('tmp_new_key_dfct_tbl', ''))\n",
    "     \n",
    "\n",
    "sel_fld_cur = \"{0}.toc_se10, {0}.se10, {0}.se_lgl_ent_type_cd, {1} as dfct_srt_dte, {0}.ent_cd_dsc, {0}.rol_typ_cd   ,{0}.rol_typ_grp  ,{0}.rol_typ_cd_dsc  \".format('cur', prv_yymm)\n",
    "sel_fld_prv = \"{0}.toc_se10, {0}.se10, {0}.se_lgl_ent_type_cd, {1} as dfct_srt_dte, {0}.ent_cd_dsc, {0}.rol_typ_cd   ,{0}.rol_typ_grp  ,{0}.rol_typ_cd_dsc  \".format('prv', prv_yymm)\n",
    "\n",
    "#Create defect new table\n",
    "print(\"Create defect new table\")\n",
    "dfct_new_tbl =  sch_nm + \".cor_dfct_new_\"  + str (cur_yymm)\n",
    "q_dfct_new = \"select cur.* from {0} cur, {1} new  where  cur.toc_se10   = new.toc_se10 and cur.se10  = new.se10 and cur.rol_typ_cd = new.rol_typ_cd\".format(cur_dfct_tbl, \"tmp_new_key_dfct_tbl\")\n",
    "sqlContext.sql ( q_dfct_new ).registerTempTable('tmp_dfct_new_tbl')\n",
    "f_cr_tbl_selas (dfct_new_tbl, 'tmp_dfct_new_tbl', '' )\n",
    "\n",
    "\n",
    "#  get se_sta_cd_ds for merchant had been complete\n",
    "print(\"process defect(s) had been completed or canceled\")\n",
    "mernm_tbl  =  sch_nm + \".cor_wrk_mernm_stscd\"\n",
    "\n",
    "q_dfct_cmp = \"\"\"\n",
    "select {0}, ' ' as mis_sumy,\n",
    "case  \n",
    "  when mer.se_sta_cd_ds like ('%CANCEL%')  then 'CANCEL'\n",
    " else 'COMPLETE'\n",
    "end as mis_sumy_det\n",
    "from {1} prv,\n",
    "     {2} mer,\n",
    "     tmp_cmp_key_dfct_tbl  cmp\n",
    "where prv.se10 = mer.se10\n",
    " and prv.se10 = cmp.se10\n",
    " and prv.rol_typ_cd = cmp.rol_typ_cd\"\"\".format(sel_fld_prv, prv_dfct_tbl, mernm_tbl )  \n",
    "sdf_dfct_cmp =  sqlContext.sql(q_dfct_cmp) # sdf for defect completion\n",
    "\n",
    "#\n",
    "print(\"process common defect(s) + new defect for commom merchant \")\n",
    "q_sdf_com_key = \"\"\"select cur.toc_se10, cur.se10, cur.rol_typ_cd  from {0} cur, {1} prv\n",
    "                   where  cur.toc_se10   = prv.toc_se10 and cur.se10  = prv.se10 and cur.rol_typ_cd = prv.rol_typ_cd\n",
    "                   group by  cur.toc_se10,  cur.se10, cur.rol_typ_cd\n",
    "                \"\"\".format(cur_dfct_tbl, prv_dfct_tbl)\n",
    "\n",
    "sqlContext.sql (q_sdf_com_key).registerTempTable('tmp_com_key_tbl')\n",
    "print(\"Count for toc_se10,  se10, rol_typ_cd  in both current and previous defect reports = \",   f_get_tbl_cnt('tmp_new_key_dfct_tbl', '' ))     \n",
    "q_sdf_com_dfct = \"\"\"select {0}, cur.mis_sumy, cur.mis_sumy_det  from {1} cur, {2} com\n",
    "                    where cur.toc_se10   = com.toc_se10 and cur.se10  = com.se10 and cur.rol_typ_cd = com.rol_typ_cd\n",
    "                 \"\"\".format(sel_fld_cur, cur_dfct_tbl, \"tmp_com_key_tbl\")\n",
    "sdf_com_dfct = sqlContext.sql(q_sdf_com_dfct)\n",
    "\n",
    "f_union_all(sdf_dfct_cmp, sdf_com_dfct).registerTempTable('tmp_prg_trk_tbl')\n",
    "\n",
    "\n",
    "if f_chk_tbl_exist(prv_prg_trk_tbl) and f_get_tbl_cnt(prv_prg_trk_tbl, '') > 0  :\n",
    "   q_sdf_upd_dfct_srt_dte   = \"\"\" select  cur.toc_se10, cur.se10, cur.se_lgl_ent_type_cd,\n",
    "                                          coalesce( prv.dfct_srt_dte,  cur.dfct_srt_dte) as  dfct_srt_dte,\n",
    "                                          cur.ent_cd_dsc, cur.rol_typ_cd, cur.rol_typ_grp, cur.rol_typ_cd_dsc, cur.mis_sumy, cur.mis_sumy_det\n",
    "                                  from tmp_prg_trk_tbl cur\n",
    "                                  left outer join {0} prv\n",
    "                                  on cur.toc_se10   = prv.toc_se10 and cur.se10  = prv.se10 and cur.rol_typ_cd = prv.rol_typ_cd\n",
    "                             \"\"\".format(prv_prg_trk_tbl )\n",
    "   sqlContext.sql(q_sdf_upd_dfct_srt_dte).registerTempTable('tmp_prg_trk_tbl')\n",
    "\n",
    "prg_trk_tbl =  sch_nm + \".cor_dfct_prg_trk_\"  + str (cur_yymm)\n",
    "f_cr_tbl_selas (prg_trk_tbl, 'tmp_prg_trk_tbl', '' )\n",
    "   \n",
    "                                                                                                       \n",
    "job_end_time = time.time()\n",
    "elapse_time (job_start_time, job_end_time, pgm)                                                                                                       \n",
    "\n",
    "print (\"End of \" + pgm)\n",
    "\n",
    "cor_prg_trk_rpt.py\n",
    "pgm = \"cor_prg_trk_rpt.py\"\n",
    "print (\"Start of \" + pgm)\n",
    "\n",
    "# Invoke code to get parm to determine the location of the folder and yymm\n",
    "path_parm_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_parm_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "#load code for Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "\n",
    "################################################################################################################################################\n",
    "#  Main process starts here\n",
    "################################################################################################################################################\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Build up a look up dictionary to get ctry_abbr_nm from toc_se10\n",
    "# - There is no corresponding table for cor_lup_partner_grp.csv which is used by reporting only\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "csv_file  = path_data + \"cor_lup_partner_grp.csv\"\n",
    "d_toc_se10_ctry =  f_cr_dic_from_csv(csv_file, 'ctry_abbr_nm', 'toc_se10')\n",
    "df_lup = pd.read_csv(csv_file)\n",
    "df_lup.sort_values(['partner' ],  inplace = True)\n",
    "\n",
    "\n",
    "mer_tbl  =  sch_nm + \".cor_wrk_mernm_stscd\"\n",
    "prg_trk_tbl  =  sch_nm + \".cor_dfct_prg_trk_\"  + str (yymm)\n",
    "dfct_new_tbl =  sch_nm + \".cor_dfct_new_\"      + str (yymm)  \n",
    "\n",
    "sqlContext.udf.register(\"udf_get_mth_diff\", f_get_mth_diff)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# get the merchant name for progress tracking\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "q_sdf_trk_mer = \"\"\"select trk.toc_se10, trk.se10, mer.se_dba_nm, trk.ent_cd_dsc, trk.rol_typ_cd, trk.rol_typ_cd_dsc, trk.dfct_srt_dte, trk.mis_sumy_det, trk.rol_typ_grp, udf_get_mth_diff(dfct_srt_dte, '{2}' ) as mth_diff\n",
    "                     from {0} trk\n",
    "                     left outer join  {1} mer\n",
    "                     on    trk.se10 = mer.se10                     \n",
    "                     sort by trk.se10,trk.rol_typ_grp\n",
    "                 \"\"\".format(prg_trk_tbl, mer_tbl, yymm)\n",
    "print(\"q_sdf_trk_mer\", q_sdf_trk_mer)\n",
    "sdf_trk_mer = sqlContext.sql(q_sdf_trk_mer)\n",
    "\n",
    "\n",
    "_date = f_dte_for_yymm(yymm, 'd')\n",
    "ost_mmm_ccyy = \"Outstanding as of \" + calendar.month_abbr[_date.month] + '-' + str(_date.year)\n",
    "\n",
    "\n",
    "_l_mis_ary = sdf_trk_mer.collect()\n",
    "labels =  [\"Pay Manager SE10\", \"Merchant SE10\",  \"Merchant Name\",  \"Legal Entity\", \"Role Code\",  \\\n",
    "           \"Role Code Description\", \"Date of first Occurrence of defect\",  ost_mmm_ccyy,  \"rol_typ_grp\", \"mth_diff\" ]\n",
    "\n",
    "df_trk=pd.DataFrame.from_records(_l_mis_ary, columns = labels)              # create a pd  dataframe\n",
    "df_trk.sort_values(['Merchant SE10', 'rol_typ_grp' ], inplace = True)     \n",
    " \n",
    "df_trk.drop(['rol_typ_grp'], axis=1, inplace=True)\n",
    "\n",
    "df_prg_trk   = df_trk[ df_trk['mth_diff'] < '4' ]\n",
    "df_90p_trk = df_trk[df_trk['mth_diff']    > '3' ]\n",
    "\n",
    "if not pd.DataFrame(df_prg_trk).empty:\n",
    " df_prg_trk.drop(['mth_diff'], axis=1, inplace=True)\n",
    "                  \n",
    "if not pd.DataFrame(df_90p_trk).empty:\n",
    " df_90p_trk.drop(['mth_diff'], axis=1, inplace=True) \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# get the merchant name for new defect\n",
    "# Notes:\n",
    "#  - writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')  # Convert the dataframe to an XlsxWriter Excel object\n",
    "#    - engine='xlsxwriter' is required to define workbook and woeksheett to adjust the width of columns\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "q_sdf_dfct_new_mer = \"\"\"select dfct.toc_se10, dfct.se10, mer.se_dba_nm, dfct.ent_cd_dsc, dfct.rol_typ_cd, dfct.rol_typ_cd_dsc, dfct.mis_sumy_det, dfct.rol_typ_grp\n",
    "                     from {0} dfct       \n",
    "                     left outer join  {1} mer\n",
    "                     on    dfct.se10 = mer.se10                     \n",
    "                     sort by dfct.se10, dfct.rol_typ_grp\n",
    "                 \"\"\".format(dfct_new_tbl, mer_tbl)\n",
    "print(\"q_sdf_dfct_new_mer\", q_sdf_dfct_new_mer)\n",
    "_l_mis_ary = sqlContext.sql(q_sdf_dfct_new_mer).collect()\n",
    "\n",
    "\n",
    "labels =  [\"Pay Manager SE10\", \"Merchant SE10\",  \"Merchant Name\",  \"Legal Entity\", \"Role Code\",  \\\n",
    "           \"Role Code Description\",  \"Quality  Summary (Missing fields)\", \"rol_typ_grp\" ]\n",
    "\n",
    "df_new_dfct = pd.DataFrame.from_records(_l_mis_ary, columns = labels)              # create a pd  dataframe\n",
    "df_new_dfct.sort_values(['Merchant SE10', 'rol_typ_grp' ], inplace = True)    \n",
    "\n",
    "df_new_dfct.drop(['rol_typ_grp'], axis=1, inplace=True)  \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#create report\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "nxt_yymm = f_dte_for_yymm(yymm, 'n')\n",
    "mmddccyy = f_dte_for_yymm(nxt_yymm, 'f2')\n",
    "s_partner =   df_lup[df_lup [\"status\"] == 'A'][\"partner\"].unique()\n",
    "for _partner  in s_partner:\n",
    "    df_lup_new = df_lup[df_lup['partner'] == _partner]\n",
    "    print(\"df_lup_new\", df_lup_new, \"_partner\",  _partner)\n",
    "    _w_file_wrt_ind = 0\n",
    "    for _toc_se in  df_lup_new[\"toc_se10\"].unique():\n",
    "        df_toc_se_new_dfct = df_new_dfct[df_new_dfct[\"Pay Manager SE10\"] == str(_toc_se)]\n",
    "        df_toc_se_prg_trk  = df_prg_trk[df_prg_trk[\"Pay Manager SE10\"]   == str(_toc_se)]\n",
    "        df_toc_se_90p_trk  = df_90p_trk[df_90p_trk[\"Pay Manager SE10\"]   == str(_toc_se)]\n",
    "        \n",
    "        _w_count_new_dfct  = df_new_dfct[df_new_dfct[\"Pay Manager SE10\"] == str(_toc_se)][\"Pay Manager SE10\"].count()\n",
    "        _w_count_prg_trk   = df_prg_trk[df_prg_trk[\"Pay Manager SE10\"]   == str(_toc_se)][\"Pay Manager SE10\"].count()\n",
    "        _w_count_90p_trk   = df_90p_trk[df_90p_trk[\"Pay Manager SE10\"]   == str(_toc_se)][\"Pay Manager SE10\"].count()        \n",
    "        print(\"_w_count_new_dfct\", _w_count_new_dfct, \"_w_count_prg_trk\", _w_count_prg_trk)\n",
    "        if  _w_count_new_dfct > 0 or  _w_count_prg_trk > 0  or _w_count_90p_trk > 0 :\n",
    "            if _w_file_wrt_ind == 0 :\n",
    "               excel_file = path_data +  str(_partner) + \"_progress_\" + str(mmddccyy) + \".xlsx\"\n",
    "               print (\"excel_file \", excel_file )   \n",
    "               writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')  # Convert the dataframe to an XlsxWriter Excel object\n",
    "               _w_file_wrt_ind = 1\n",
    "            ctry = d_toc_se10_ctry[_toc_se]\n",
    "            \n",
    "            if _w_count_new_dfct > 0:\n",
    "               sheet_name =  str(_toc_se) + \"_\" + ctry + '_new_defect'\n",
    "               df_toc_se_new_dfct.to_excel(writer,sheet_name, index=False)\n",
    "               f_adj_col_sheet(df_toc_se_new_dfct, writer, sheet_name, 'True')\n",
    "               \n",
    "            if _w_count_prg_trk > 0:\n",
    "               sheet_name =  str(_toc_se) + \"_\" + ctry + '_progress'\n",
    "               df_toc_se_prg_trk.to_excel(writer,sheet_name, index=False)\n",
    "               f_adj_col_sheet(df_toc_se_prg_trk, writer, sheet_name, 'True')\n",
    "               \n",
    "            if _w_count_90p_trk > 0:\n",
    "               sheet_name =  str(_toc_se) + \"_\" + ctry + '_90plus'\n",
    "               df_toc_se_90p_trk.to_excel(writer,sheet_name, index=False)\n",
    "               f_adj_col_sheet(df_toc_se_90p_trk, writer, sheet_name, 'True')\n",
    "               \n",
    "\n",
    "    if   _w_file_wrt_ind == 1:\n",
    "         writer.save()\n",
    "    else:\n",
    "         excel_file = path_data +  str(_partner) + \"_progress_\" + str(mmddccyy) + \".xlsx\"\n",
    "         print (\"excel_file \", excel_file )\n",
    "         writer = pd.ExcelWriter(excel_file)\n",
    "         writer.save()\n",
    "\n",
    "\n",
    "pgm = \"cor_chg_trk_rpt.py\"\n",
    "print (\"End of \" + pgm)\n",
    "\n",
    "\n",
    "cor_chg_bld_trk.py\n",
    "pgm = \"cor_chg_bld_trk.py\"\n",
    "print (\"Start of \" + pgm)\n",
    "################################################################################################################################################\n",
    "#ini - Initialization\n",
    "################################################################################################################################################\n",
    "\n",
    "#load code for to get parm\n",
    "path_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "#path_data = \"/axp/buanalytics/csramp/dev/cor/data_prod/\"\n",
    "f_parm = path_code + \"cor_com_code_parm.py\"\n",
    "exec(compile(open(f_parm, \"rb\").read(), f_parm, 'exec' ))\n",
    "\n",
    "\n",
    "#load code for Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "job_start_time = time.time()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Register udf\n",
    "# - the functions are defined in  cor_com_func.py\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "sqlContext.udf.register(\"udf_com_fld\", f_com_fld)\n",
    "sqlContext.udf.register(\"udf_dif_fld\", f_dif_fld)\n",
    "sqlContext.udf.register(\"udf_bld_mis_sumy_det\", f_bld_mis_sumy_det)\n",
    "u_bld_mis_sumy_det= udf(f_bld_mis_sumy_det)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Build dictionary\n",
    "# - Invoke function 'f_cr_dic_from_csv' to build dictionary d_fld_nam  to map the field name from CS to bus field name\n",
    "# - Invoke 'f_cr_dic_from_tbl' to get bus role description\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "path_data = '/axp/buanalytics/csramp/dev/cor/data/'\n",
    "csv_file  = path_data + 'cor_lup_corns_fld.csv'\n",
    "#global d_bus_rol_grp_dsc, d_bus_rol_grp_dsc\n",
    "d_fld_nam =  f_cr_dic_from_csv(csv_file, 'bus_fld_nam', 'corns_fld_nam')\n",
    "\n",
    "bus_rul_tbl = \"pcr_cor.cor_lup_bus_rule\"\n",
    "d_bus_rol_grp_dsc =  f_cr_dic_from_tbl(bus_rul_tbl, \"rol_typ\", \"rol_typ_dsc\")\n",
    "\n",
    "################################################################################################################################################\n",
    "#  Main process starts here\n",
    "################################################################################################################################################\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Create a dataframe from the defect table required for excel file\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "yymm = (pd.Period(datetime.today(), 'M') - 1).strftime('%y%m')\n",
    "\n",
    "yymm = 1811\n",
    "sch_nm = \"yuedb\"\n",
    "cur_yymm= yymm\n",
    "prv_yymm= 1810   # will calculate from cur_yymm later\n",
    "\n",
    "cur_dfct_tbl = sch_nm + \".cor_defect_\" + str(cur_yymm)\n",
    "prv_dfct_tbl = sch_nm + \".cor_defect_\" + str(prv_yymm)\n",
    "\n",
    "#chg1\n",
    "cur_sumy = 1811\n",
    "prv_yymm = 1810\n",
    "\n",
    "cur_dfct_tbl = \"yuedb.csv_wrk_defect_1811\"\n",
    "prv_dfct_tbl = \"yuedb.csv_wrk_defect_1810\"\n",
    "\n",
    "q_sdf_dfct_key_cur =  \"select toc_se10, se10, rol_typ_cd  from {0}\".format(cur_dfct_tbl)\n",
    "sdf_dfct_key_cur  =  sqlContext.sql(q_sdf_dfct_key_cur)   #sdf\n",
    "\n",
    "# will use toc_se10, se10, rol_typ_cd as key to check the completion and new defect\n",
    "q_sdf_dfct_key_prv =  \"select toc_se10, se10, rol_typ_cd  from {0}\".format(prv_dfct_tbl)\n",
    "sdf_dfct_key_prv  =  sqlContext.sql(q_sdf_dfct_key_prv)   #sdf\n",
    "\n",
    "sdf_new_key_dfct  = sdf_dfct_key_cur.subtract(sdf_dfct_key_prv)  # 6\n",
    "sdf_cmp_key_dfct  = sdf_dfct_key_prv.subtract(sdf_dfct_key_cur)  #1\n",
    "sdf_cmp_key_dfct.registerTempTable('tmp_cmp_key_dfct_tbl')  #register temp table\n",
    "sdf_new_key_dfct.registerTempTable('tmp_new_key_dfct_tbl')  #register temp table\n",
    "\n",
    "from_where_cond = \"from {0}  cur, {1}  prv where  cur.toc_se10   = prv.toc_se10 and cur.se10  = prv.se10 and cur.rol_typ_cd = prv.rol_typ_cd\".format(cur_dfct_tbl, prv_dfct_tbl  )\n",
    "sel_fld_cur = \"{0}.toc_se10, {0}.se10, {0}.se_lgl_ent_type_cd, {1} as dfct_srt_dte, {0}.ent_cd_dsc, {0}.rol_typ_cd   ,{0}.rol_typ_grp  ,{0}.rol_typ_cd_dsc  \".format('cur', prv_yymm)\n",
    "sel_fld_prv = \"{0}.toc_se10, {0}.se10, {0}.se_lgl_ent_type_cd, {1} as dfct_srt_dte, {0}.ent_cd_dsc, {0}.rol_typ_cd   ,{0}.rol_typ_grp  ,{0}.rol_typ_cd_dsc  \".format('prv', prv_yymm)\n",
    "\n",
    "#Create defect new table\n",
    "print(\"Create defect new table\")\n",
    "dfct_new_tbl =  sch_nm + \".cor_dfct_new_\"  + str (cur_yymm)\n",
    "q_dfct_new = \"select cur.* from {0} cur, {1} new  where  cur.toc_se10   = new.toc_se10 and cur.se10  = new.se10 and cur.rol_typ_cd = new.rol_typ_cd\".format(cur_dfct_tbl, \"tmp_new_key_dfct_tbl\")\n",
    "sqlContext.sql ( q_dfct_new ).registerTempTable('tmp_dfct_new_tbl')\n",
    "f_cr_tbl_selas (dfct_new_tbl, 'tmp_dfct_new_tbl', '' )\n",
    "\n",
    "\n",
    "#  get se_sta_cd_ds for merchant had been complete\n",
    "print(\"process defect(s) had been completed or canceled\")\n",
    "q_dfct_cmp = \"\"\"\n",
    "select {0}, ' ' as mis_sumy,\n",
    "case  \n",
    "  when mer.se_sta_cd_ds in ('CANCEL')  then 'CANCEL'\n",
    " else 'COMPLETE'\n",
    "end as mis_sumy_det\n",
    "from {1} prv,\n",
    "     cstonedb3.gms_merchant_char mer,\n",
    "     tmp_cmp_key_dfct_tbl  cmp\n",
    "where prv.se10 = mer.se10\n",
    " and prv.se10 = cmp.se10\n",
    " and prv.rol_typ_cd = cmp.rol_typ_cd\"\"\".format(sel_fld_prv, prv_dfct_tbl)  \n",
    "sdf_dfct_cmp =  sqlContext.sql(q_dfct_cmp) # sdf for defect completion\n",
    "\n",
    "#\n",
    "print(\"process common defect(s) + new defect for commom merchant \")\n",
    "q_sdf_com_key = \"\"\"select cur.toc_se10, cur.se10, cur.rol_typ_cd  from {0} cur, {1} prv\n",
    "                   where  cur.toc_se10   = prv.toc_se10 and cur.se10  = prv.se10 and cur.rol_typ_cd = prv.rol_typ_cd\n",
    "                \"\"\".format(cur_dfct_tbl, prv_dfct_tbl)\n",
    "\n",
    "sqlContext.sql (q_sdf_com_key).registerTempTable('tmp_com_key_tbl')\n",
    "q_sdf_com_dfct = \"\"\"select {0}, cur.mis_sumy, cur.mis_sumy_det  from {1} cur, {2} com\n",
    "                    where cur.toc_se10   = com.toc_se10 and cur.se10  = com.se10 and cur.rol_typ_cd = com.rol_typ_cd\n",
    "                 \"\"\".format(sel_fld_cur, cur_dfct_tbl, \"tmp_com_key_tbl\")\n",
    "sdf_com_dfct = sqlContext.sql(q_sdf_com_dfct)\n",
    "\n",
    "f_union_all(sdf_dfct_cmp, sdf_com_dfct).registerTempTable('tmp_trk_90D_tbl')\n",
    "                                                                                                      \n",
    "trk_90d_tbl =  sch_nm + \".cor_dfct_trk_90d_\"  + str (cur_yymm)                                                                                                      \n",
    "f_cr_tbl_selas (trk_90d_tbl, 'tmp_trk_90D_tbl', '' )\n",
    "\n",
    "                                                                                                       \n",
    "job_end_time = time.time()\n",
    "elapse_time (job_start_time, job_end_time, pgm)                                                                                                       \n",
    "\n",
    "print (\"End of \" + pgm)\n",
    "\n",
    "cor_chg_trk_rpt.py\n",
    "In [ ]:\n",
    " \n",
    "pgm = \"cor_chg_trk_rpt.py\"\n",
    "print (\"Start of \" + pgm)\n",
    "path_data = \"/axp/buanalytics/csramp/dev/cor/data/\"\n",
    "path_code = \"/axp/buanalytics/csramp/dev/cor/code/\"\n",
    "\n",
    "\n",
    "\n",
    "#load code for Initialization\n",
    "f_init = path_code + \"cor_com_code_init.py\"\n",
    "exec(compile(open(f_init, \"rb\").read(), f_init, 'exec' ))\n",
    "\n",
    "mer_tbl =\"cstonedb3.gms_merchant_char\"\n",
    "trk_tbl = \"yuedb.cor_dfct_trk_90d_1811\"\n",
    "dfct_new_tbl = \"yuedb.cor_dfct_new_1811\"\n",
    "q_mer_nm = \"select mer.se10, mer.se_dba_nm from {1} trk, {2}  new, {0} mer  where  mer.se10 = trk.se10 or mer.se10 = new.se10 group by  mer.se10, mer.se_dba_nm\".format (mer_tbl, trk_tbl, dfct_new_tbl)\n",
    "\n",
    "\n",
    "pdf = sqlContext.sql(q_mer_nm).toPandas()\n",
    "l_key = pdf[se10].tolist()\n",
    "l_val = pdf[se_dba_nm].tolist()\n",
    "d_mer_nm = dict (zip(l_key, l_val))\n",
    "def f_get_mer_nm(se10):\n",
    "    return d_mer_nm[se10]\n",
    "\n",
    "\n",
    "\n",
    "u_get_mer_nm = udf(f_get_mer_nm)\n",
    "sdf_trk = sqlContext.sql( \"select * from yuedb.cor_dfct_trk_90d_1811\" ).withColumn('mer_nm', u_get_mer_nm(\"se10\") )\n",
    "\n",
    "l_mis_ary = sdf_trk.select(\"toc_se10\", \"se10\", \"mer_nm\", \"ent_cd_dsc\", \"rol_typ_grp\", \"rol_typ_cd_dsc\", \"dfct_srt_dte\", \"mis_sumy_det \", \"rol_typ_grp\"  ).collect()\n",
    "labels =  [\"Pay Manager SE10\", \"Merchant SE10\",  \"Merchant Name\",  \"Legal Entity\", \"Role Code\",  \\\n",
    "           \"Role Code Description\", \"Date of first Occurrence of defect\",  \"Outstanding as of NOV-2018\", \"_rol_typ_grp\" ]\n",
    "\n",
    "df_from_list=pd.DataFrame.from_records(_l_mis_ary, columns = labels)              # create a pd  dataframe\n",
    "df_from_list.sort_values(['Merchant SE10', '_rol_typ_grp' ], inplace = True)     \n",
    "#df_from_list.sort(['Merchant SE10', '_rol_typ_grp' ], inplace = True)            # \"AttributeError: 'DataFrame' object has no attribute 'sort'; use 'sort_values' instead\n",
    "df_from_list.drop(['_rol_typ_grp'], axis=1, inplace=True)  \n",
    "\n",
    "excel_file = path_data +  'tracking_90d ' + \"_\" + \"1811.xlsx\"\n",
    "print (\"excel_file \", excel_file )\n",
    "writer = pd.ExcelWriter(excel_file)\n",
    "df_from_list.to_excel(writer,tbl, index=False)\n",
    "writer.save()\n",
    "\n",
    "pgm = \"cor_chg_trk_rpt.py\"\n",
    "print (\"End of \" + pgm)\n",
    " \n",
    " \n",
    "# Example Network SSH script for\n",
    "# To get netmiko and xlsxwriter:\n",
    "# open command prompt and enter:\n",
    "#   - C:\\Users\\<ADS_ID_HERE>\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\n",
    "#   - Enter: pip install netmiko --proxy=proxy.aexp.com:8080\n",
    "#   - Enter: pip install xlsxwriter --proxy=proxy.aexp.com:8080\n",
    " \n",
    "import netmiko\n",
    "import xlsxwriter\n",
    "import threading\n",
    "import time\n",
    " \n",
    "hostlist = ['172.31.8.100', '172.31.0.224']\n",
    " \n",
    "cred = []\n",
    "file = open(r'C:\\Users\\mjureck\\PycharmProjects\\NAOI_OFFICIAL\\TRAINING\\Python_DataScience\\cred.txt', 'r')\n",
    "for line in file:\n",
    "    line = line.rstrip('\\n')\n",
    "    line = line.strip()\n",
    "    cred.append(line)\n",
    " \n",
    "file.close()\n",
    " \n",
    "username = cred[0]\n",
    "password = cred[1]\n",
    "ios_ver = 'cisco_ios'\n",
    " \n",
    "outputlist = []\n",
    " \n",
    "def thread(u, p):\n",
    "    threads = []\n",
    "    for ipaddress in hostlist:\n",
    "        t = threading.Thread(target=ssh, args=(ipaddress, username, password,), )\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "        print('Starting Connection to:', ipaddress)\n",
    "        time.sleep(1)\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    " \n",
    "def ssh(ipaddress, username, password):\n",
    "    try:\n",
    "        net_connect = netmiko.ConnectHandler(ip=ipaddress, username=username, password=password, secret=password,\n",
    "                                             device_type=ios_ver)\n",
    " \n",
    "        cmd = net_connect.send_command('show run | inc flow')\n",
    "        output = ipaddress + '###' + cmd\n",
    "        outputlist.append(output)\n",
    "        print(cmd)\n",
    " \n",
    "        net_connect.disconnect()\n",
    " \n",
    "    except Exception as e:\n",
    "        e = str(e)\n",
    "        print(e)\n",
    " \n",
    "thread(u=username, p=password)\n",
    " \n",
    "filename = 'TestOutput.xlsx'\n",
    "workbook = xlsxwriter.Workbook(filename)   # excel file\n",
    "ws = workbook.add_worksheet('Results')     # Worksheet\n",
    " \n",
    "headerformat = workbook.add_format({'bold': True,\n",
    "                                    'font_name': 'Calibri',\n",
    "                                    'font_size': 11,\n",
    "                                    'bg_color': 'gray',\n",
    "                                    'align': 'center',\n",
    "                                    'valign': 'vcenter',\n",
    "                                    'text_wrap': True})\n",
    "normal_format = workbook.add_format({'text_wrap': True})\n",
    " \n",
    "ws.set_column(0, 1, 30, normal_format)\n",
    "ws.autofilter('A1:B1')\n",
    "ws.freeze_panes(0, 1)\n",
    " \n",
    "ws.write(0, 0, 'IP Address', headerformat)  # write column to the header\n",
    "ws.write(0, 1, 'Result', headerformat)\n",
    " \n",
    " \n",
    "# write data to the spreadsheet\n",
    "row = 1 \n",
    "for item in outputlist:\n",
    "    item2 = item.split('###')\n",
    "    ws.write(row, 0, item2[0])\n",
    "    ws.write(row, 1, item2[1])\n",
    "    row += 1\n",
    " \n",
    "workbook.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
