{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imbed_L1_L2_Syndigo Mapping ML\n",
    "  - module name: imbed_L1_L2_Syndigo ML.ipynb\n",
    "  - Purpose: Combine Level 1 & Level 2 from  Syndigo to build model\n",
    "             - Apply TFIDF and W2vec imbedding methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_1():\n",
    "    pgm = inspect.currentframe().f_code.co_name  \n",
    "    start_time = time.time() \n",
    "    # Reading PIMMART data\n",
    "    pim_gtin_mapped = pd.read_csv(DBFR + \"PIM_Data_New_50_82Mn.csv\", dtype=object)\n",
    "    for i in ['SUBCOM_CD', 'DPT_CD', 'COM_CD','PMY_DPT_CD', 'REC_DPT_CD', 'ITM_ID', 'GTIN']:\n",
    "        pim_gtin_mapped[i] = pim_gtin_mapped[i].astype(np.float64)\n",
    "    \n",
    "    # Reading Syndigo 259K data\n",
    "    synd_ALL = pd.read_csv(DBFR + 'Syndigo_Final_ALL.csv', dtype='unicode') # 259k Syndigo Data\n",
    "    for i in ['SUBCOM_CD', 'DPT_CD', 'COM_CD', 'GTIN', 'ITM_ID', 'PMY_DPT_CD']:\n",
    "        synd_ALL[i] = synd_ALL[i].astype(np.float64)\n",
    "    \n",
    "    # Stripping spaces from all columns\n",
    "    df_obj = synd_ALL.select_dtypes(['object'])\n",
    "    synd_ALL[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    syndigo_mapped = synd_ALL\n",
    "    pimmart = pim_gtin_mapped\n",
    "    syndigo_mapped.drop_duplicates('GTIN', inplace = True)\n",
    "    syndigo_mapped['ITEM_SUBCOM_text'] = \\\n",
    "    (syndigo_mapped.VND_ECOM_DSC + ' ' + syndigo_mapped.SUBCOM_DSC).fillna('').str.lower()\n",
    "    #syndigo_mapped['Level 1'].value_counts()\n",
    "    #print(f'syndigo_mapped.shape {syndigo_mapped.shape}') \n",
    "    #print(f'syndigo_mapped.info() {syndigo_mapped.info()}') \n",
    "    #print(f'syndigo_mapped.head() {syndigo_mapped.head()}') \n",
    "    #print(f'syndigo_mapped.columns {syndigo_mapped.columns}') \n",
    "    \n",
    "    syndigo_mapped['l1_l2'] = syndigo_mapped['Level 1'] + ' + ' + syndigo_mapped['Level 2']\n",
    "    #print(f\"syndigo_mapped['l1_l2'].value_counts() {syndigo_mapped['l1_l2'].value_counts(dropna = False)}\")    \n",
    "    return syndigo_mapped, pim_gtin_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_2(samp_frac = 1, embed_typ = 'tfidf'):\n",
    "  \n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name \n",
    "    global syndigo_mapped, model\n",
    "    print(f' samp_frac = {samp_frac}') \n",
    "    print (f'Before sample syndigo_mapped.shape {syndigo_mapped.shape }')\n",
    "    syndigo_mapped = syndigo_mapped_bkup.copy()\n",
    "    \n",
    "    whole_frac = 1\n",
    "    if samp_frac  < whole_frac :   syndigo_mapped = syndigo_mapped.sample(frac = samp_frac,  random_state=42)   \n",
    "    print (f'After sample syndigo_mapped.shape {syndigo_mapped.shape }')    \n",
    "    \n",
    "   \n",
    "    if embed_typ == 'tfidf': \n",
    "        vect = TfidfVectorizer(ngram_range = (1,2), max_features = 50000) \n",
    "        X = vect.fit_transform(syndigo_mapped.ITEM_SUBCOM_text) #scipy.sparse._csr.csr_matrix\n",
    "    else:\n",
    "        model = KeyedVectors.load_word2vec_format(DBFO + 'w2vmodel_053123_PIM_ALL.bin', binary=True)\n",
    "        X = np.array(list(syndigo_mapped.ITEM_SUBCOM_text.apply(lambda x: get_item_vector(x.split(' ')))))\n",
    "\n",
    "    l1_l2_id_map = dict(zip(syndigo_mapped['l1_l2'].fillna('Other').unique(), range(syndigo_mapped['l1_l2'].fillna('Other').nunique())))\n",
    "    id_l1_l2_map = dict(zip(range(syndigo_mapped['l1_l2'].fillna('Other').nunique()), syndigo_mapped['l1_l2'].fillna('Other').unique()))\n",
    "    y  = syndigo_mapped['l1_l2'].fillna('Other').map(l1_l2_id_map)\n",
    "    \n",
    "    end_time = time.time()   \n",
    "    desc = f' Elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)\n",
    "    return  X, y, l1_l2_id_map, id_l1_l2_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build item vectors\n",
    "def get_item_vector(item_vocab):\n",
    "    vect = np.zeros_like(model.get_vector('chips'))\n",
    "    for word in item_vocab:\n",
    "        if word in model:\n",
    "            vect += model.get_vector(word)\n",
    "    return vect#/max(1,len(item_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_predict(test_size,c_parm, embed_typ):\n",
    "     start_time = time.time()\n",
    "     pgm = inspect.currentframe().f_code.co_name   \n",
    "     global A_train, A_test, B_train, B_test, X_train, X_test, y_train, y_test\n",
    "     A_train, A_test, B_train, B_test = train_test_split(syndigo_mapped.GTIN.tolist(), syndigo_mapped['l1_l2'].tolist(), test_size=test_size,  random_state=42)\n",
    "     \n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test_size,  random_state=42)\n",
    "     lr_tf = LogisticRegression(C = c_parm, multi_class = 'multinomial', solver = 'saga', n_jobs=-1)\n",
    "     \n",
    "     print(f\"Training starts for test_size = {test_size}, c_parm = {c_parm} \")\n",
    "     lr_tf.fit(X_train, y_train)\n",
    "     preds = lr_tf.predict(X_test)\n",
    "     #print(f\"type(preds) {type(preds)} len(preds) {len(preds)} \\n preds[0:10] {preds[0:10]}\")\n",
    "     preds_lrtf = preds\n",
    "     probs = lr_tf.predict_proba(X_test)\n",
    "     #print(f\"type(probs) {type(probs)} len(probs) {len(probs)} \\n probs[0:10] {probs[0:10]}\")   \n",
    "     preds_train = lr_tf.predict(X_train)\n",
    "     #print(f\"type(preds_train) {type(preds_train)} len(preds_train) {len(preds_train)} \\n preds_train[0:10] {preds_train[0:10]} \")   \n",
    "     probs_train = lr_tf.predict_proba(X_train)\n",
    "     #print(f\"type(probs_train) {type(probs_train)} len(probs_train) {len(probs_train)} \\n probs_train[0:10] {probs_train[0:10]}\")   \n",
    "     #print(classification_report(y_test, preds_test,labels = lr_tf.classes_, target_names = [id2_level_map[i] for i in lr_tf.classes_]))\n",
    "     \n",
    "    \n",
    "    \n",
    "     #print(\"Accuracy = \",accuracy_score(y_test,preds))\n",
    "     #print(\"Display TFIDF metrics\")\n",
    "     #print(classification_report(y_test, preds,labels = lr_tf.classes_, target_names = [id_l1_l2_map[i] for i in lr_tf.classes_]))\n",
    "     filename = path + f'L1_l2_LR_{embed_typ}_tst_siz{test_size}_C{c_parm}.pkl'\n",
    "   \n",
    "     pickle.dump(lr_tf, open(filename, 'wb'))\n",
    "\n",
    "     print(f\"Done Training \")\n",
    "     end_time = time.time()   \n",
    "     desc = f' Elapse_time for \"{pgm}\".' \n",
    "     elapse_time (  start_time, end_time, desc)\n",
    "     return preds, preds_lrtf, probs, preds_train, probs_train, lr_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(test_size, c_parm, embed_typ):\n",
    "     start_time = time.time() \n",
    "     pgm = inspect.currentframe().f_code.co_name\n",
    "     global f1_score_weighted, f1_score_macro,f1_score_micro \n",
    "     l_metrics = []\n",
    "     print(f'Validation for c_parm = {c_parm}, test_size = {test_size}, and embed_type {embed_typ} ' ) \n",
    "     accuracy = accuracy_score(y_test,preds)\n",
    "     print(f\"Accuracy Score = {accuracy}\")\n",
    "     \n",
    "     f1_score_weighted = f1_score(y_test, preds, average=\"weighted\")\n",
    "     f1_score_macro    = f1_score(y_test, preds, average='macro')\n",
    "     f1_score_micro    = f1_score(y_test, preds, average='micro')  \n",
    "     print (f\" f1_score_weighted = {f1_score_weighted}\")\n",
    "     print (f\" f1_score_macro    = {f1_score_macro   }\")\n",
    "     print (f\" f1_score_micro    = {f1_score_micro   }\")\n",
    "     # Compute the Multiclass ROC AUC score\n",
    "     #multi_class = 'multinomial'\n",
    "     #score = roc_auc_score(y_test, preds, multi_class= multi_class)\n",
    "     #print(f\"ROC AUC score for Multiclass= {multi_class}: {score:.2f}\")\n",
    "\n",
    "     print(\"Display metrics\")\n",
    "     test_metrics = pd.DataFrame(classification_report(y_test, preds,labels = lr_tf.classes_, target_names = [id_l1_l2_map[i] for i in lr_tf.classes_],  output_dict= True)).T\n",
    "     #test_metrics = pd.DataFrame(classification_report(results_test.Actuals, results_test.Predictions, output_dict= True)).T \n",
    "     #print(test_metrics)\n",
    "     #Stest_metrics.to_csv('L1_L2_Test_Metrics_yue.csv')\n",
    "     #accuracy = 1\n",
    "     tab_name = f'{embed_typ}_L1_L2_TM_tst_siz{test_size}_C{c_parm}'   \n",
    "     print(f'tab_name = {tab_name}')  \n",
    "     l_metrics = [embed_typ, test_size, c_parm, accuracy, f1_score_weighted, f1_score_macro,f1_score_micro ]\n",
    "     end_time = time.time()   \n",
    "     desc = f' Elapse_time for \"{pgm}\".' \n",
    "     elapse_time (  start_time, end_time, desc)    \n",
    "     return test_metrics, tab_name, l_metrics \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_append():\n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name     \n",
    "    test_new_proobs = []\n",
    "    \n",
    "    #print(f'len(preds)  {len(preds)}')\n",
    "    for i in range(len(preds)):\n",
    "        test_new_proobs.append(probs[i][np.argsort(probs[i])][::-1][:1].tolist())\n",
    "    test_new_proobs = [element for sublist in list(test_new_proobs) for element in sublist]\n",
    "    #print(f'test_new_proobs {test_new_proobs}')\n",
    "    \n",
    "    train_new_proobs = []\n",
    "    \n",
    "    #print(f'len(preds_train)  {len(preds_train)}')\n",
    "    for i in range(len(preds_train)):\n",
    "         test_new_proobs.append(probs_train[i][np.argsort(probs_train[i])][::-1][:1].tolist())\n",
    "    train_new_proobs = [element for sublist in list(train_new_proobs) for element in sublist]\n",
    "    #print(f'train_new_proobs {train_new_proobs}')  \n",
    "    \n",
    "    # process testLevels & trainLevelss \n",
    "    testLevels = []\n",
    "    for j in y_test:\n",
    "        testLevels.append([i for i in l1_l2_id_map if l1_l2_id_map[i]==j][0])\n",
    "    #print(f'testLevels {testLevels [0:10]}')\n",
    "    testLevelss = []\n",
    "    for j in preds:\n",
    "        testLevelss.append([i for i in l1_l2_id_map if l1_l2_id_map[i]==j][0])\n",
    "    #print(f'testLevelss {testLevelss [0:10]}')\n",
    "    \n",
    "    trainLevelss = []\n",
    "    for j in preds_train:\n",
    "        trainLevelss.append([i for i in l1_l2_id_map if l1_l2_id_map[i]==j][0])\n",
    "     \n",
    "    end_time = time.time()   \n",
    "    desc = f' Elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)    \n",
    "    return  test_new_proobs, train_new_proobs, testLevels, testLevelss, trainLevelss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_cr_df(test_size, c_parm, embed_typ):\n",
    "\n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name \n",
    "    print(\"---------------------------------------------\\FINAL COUNTS:\\n---------------------------------------------\")\n",
    "    print(\"1)\", len(A_test + A_train))\n",
    "    print(\"2)\", len(['Test']*len(A_test) + ['Train']*len(A_train)))\n",
    "    print(\"3)\", len(B_test + B_train))\n",
    "    print(\"4)\", len(testLevels + B_train))\n",
    "    print(\"5)\", len(testLevelss + trainLevelss))\n",
    "    print(\"6)\", int(len(test_new_proobs + train_new_proobs)))\n",
    "    \n",
    "    data = {\n",
    "        'GTIN' : A_test + A_train,\n",
    "        'Source': ['Test']*len(A_test) + ['Train']*len(A_train),\n",
    "        'Actual l1_l2' : B_test + B_train,\n",
    "        'Actuals' : testLevels + B_train,\n",
    "        'Predictions' : testLevelss + trainLevelss,\n",
    "        'Scores' : test_new_proobs + train_new_proobs,\n",
    "    \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.merge(pim_gtin_mapped[['GTIN', 'ITM_ID', 'PMY_DPT_CD', 'PMY_DPT_DSC', 'REC_DPT_CD',\n",
    "        'REC_DPT_DSC', 'DPT_CD', 'DPT_DSC', 'COM_CD', 'COM_DSC', 'SUBCOM_CD',\n",
    "        'SUBCOM_DSC', 'VND_ECOM_DSC']], on='GTIN', how='left')\n",
    "    df = df[['ITM_ID', 'GTIN', 'PMY_DPT_CD', 'PMY_DPT_DSC', 'REC_DPT_CD', 'REC_DPT_DSC', 'DPT_CD',\n",
    "        'DPT_DSC', 'COM_CD', 'COM_DSC', 'SUBCOM_CD', 'SUBCOM_DSC',\n",
    "        'VND_ECOM_DSC', 'Source', 'Actual l1_l2', 'Actuals', 'Predictions', 'Scores']]\n",
    "    #filename = 'Syndigo_Mapping_L1_L2_ML.csv'\n",
    "    tab_name = f'{embed_typ}_L1_L2_prd_tst_siz{test_size}_C{c_parm}'  \n",
    "    #df.to_csv(DBFO + filename, index=False)\n",
    "    #print(f\"{filename}  --file created successfully\")\n",
    "\n",
    "    end_time = time.time()   \n",
    "    desc = f'Elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)\n",
    "    return df, tab_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_excel_pred( excel_file, dict):\n",
    "#  Write classification report to excel\n",
    "#  save() is going to remove, use close() instead\n",
    "   from pandas import ExcelWriter\n",
    "   from pandas import ExcelFile\n",
    "   writer = pd.ExcelWriter(excel_file)\n",
    "   for key,  value in dict.items():\n",
    "       df   =  value[0] \n",
    "       tab  =  value[1] \n",
    "       df.to_excel(writer,tab)\n",
    "   writer.close()\n",
    "   return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_excel_metric( excel_file, dict, l_metric):\n",
    "#  Write classification report to excel\n",
    "#  save() is going to remove, use close() instead\n",
    "   from pandas import ExcelWriter\n",
    "   from pandas import ExcelFile\n",
    "   writer = pd.ExcelWriter(excel_file)\n",
    "   for key,  value in dict.items():\n",
    "       df   =  value[0] \n",
    "       tab  =  value[1] \n",
    "       df.to_excel(writer,tab)\n",
    "   df_pred = pd.DataFrame(l_metric, columns = ['embed_typ', 'test_size', 'c_parm', 'accuracy', 'f1_score_weighted', 'f1_score_macro',\n",
    "                                                'f1_score_micro', 'elapse_time'] )   \n",
    "   df_pred.to_excel(writer,'Consolidate Metrics')                                         \n",
    "   writer.close()\n",
    "   return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b64183-34df-46b6-b1c9-6bbe78776fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier,  RidgeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import re, time, inspect, pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format \n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# load  function of 'elapse_time'\n",
    "path_code = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Code\\\\'  \n",
    "f_com_code = path_code + \"com_code.py\"\n",
    "exec(compile(open(f_com_code , \"rb\").read(), f_com_code, 'exec' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBFS = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFO = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFM = \"/dbfs/FileStore/tables/MALLIK/\"\n",
    "DBFR = \"/dbfs/FileStore/tables/OFFSHORE_RESULTS/\"\n",
    "path = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Data\\\\'   \n",
    "DBFS = path\n",
    "DBFO = path\n",
    "DBFM = path\n",
    "DBFR = path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "syndigo_mapped, pim_gtin_mapped = prepare_data_1()\n",
    "syndigo_mapped_bkup = syndigo_mapped.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " samp_frac = 1\n",
      "Before sample syndigo_mapped.shape (77726, 25)\n",
      "After sample syndigo_mapped.shape (259085, 25)\n",
      "  Elapse_time for \"prepare_data_2\". It took 7.578682 seconds - 0hh:0mm:7ss.\n",
      " start time: Aug 29 2023 02:29:10  end time:  Aug 29 2023 02:29:17\n",
      "Training starts for test_size = 0.3, c_parm = 0.1 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 71.904077 seconds - 0hh:1mm:11ss.\n",
      " start time: Aug 29 2023 02:29:17  end time:  Aug 29 2023 02:30:29\n",
      "Validation for c_parm = 0.1, test_size = 0.3, and embed_type tfidf \n",
      "Accuracy Score = 0.8171782929778967\n",
      " f1_score_weighted = 0.7876267710987631\n",
      " f1_score_macro    = 0.1334649684000625\n",
      " f1_score_micro    = 0.8171782929778968\n",
      "Display metrics\n",
      "tab_name = tfidf_L1_L2_TM_tst_siz0.3_C0.1\n",
      "  Elapse_time for \"validation\". It took 0.237823 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 29 2023 02:30:29  end time:  Aug 29 2023 02:30:30\n",
      "  Elapse_time for \"proc_append\". It took 8.983813 seconds - 0hh:0mm:8ss.\n",
      " start time: Aug 29 2023 02:30:30  end time:  Aug 29 2023 02:30:39\n",
      "---------------------------------------------\\FINAL COUNTS:\n",
      "---------------------------------------------\n",
      "1) 259085\n",
      "2) 259085\n",
      "3) 259085\n",
      "4) 259085\n",
      "5) 259085\n",
      "6) 259085\n",
      " Elapse_time for \"proc_cr_df\". It took 5.151385 seconds - 0hh:0mm:5ss.\n",
      " start time: Aug 29 2023 02:30:39  end time:  Aug 29 2023 02:30:44\n",
      "  Elapse_time to build model for for test_size = 0.3, c_parm = 0.1, embed_typ= tfidf It took 86.292675 seconds - 0hh:1mm:26ss.\n",
      " start time: Aug 29 2023 02:29:17  end time:  Aug 29 2023 02:30:44\n",
      "Training starts for test_size = 0.3, c_parm = 1 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 83.790123 seconds - 0hh:1mm:23ss.\n",
      " start time: Aug 29 2023 02:30:44  end time:  Aug 29 2023 02:32:07\n",
      "Validation for c_parm = 1, test_size = 0.3, and embed_type tfidf \n",
      "Accuracy Score = 0.8900239302164012\n",
      " f1_score_weighted = 0.8824839217177973\n",
      " f1_score_macro    = 0.32670818114021144\n",
      " f1_score_micro    = 0.8900239302164012\n",
      "Display metrics\n",
      "tab_name = tfidf_L1_L2_TM_tst_siz0.3_C1\n",
      "  Elapse_time for \"validation\". It took 0.240393 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 29 2023 02:32:08  end time:  Aug 29 2023 02:32:08\n",
      "  Elapse_time for \"proc_append\". It took 9.262575 seconds - 0hh:0mm:9ss.\n",
      " start time: Aug 29 2023 02:32:08  end time:  Aug 29 2023 02:32:17\n",
      "---------------------------------------------\\FINAL COUNTS:\n",
      "---------------------------------------------\n",
      "1) 259085\n",
      "2) 259085\n",
      "3) 259085\n",
      "4) 259085\n",
      "5) 259085\n",
      "6) 259085\n",
      " Elapse_time for \"proc_cr_df\". It took 4.918671 seconds - 0hh:0mm:4ss.\n",
      " start time: Aug 29 2023 02:32:17  end time:  Aug 29 2023 02:32:22\n",
      "  Elapse_time to build model for for test_size = 0.3, c_parm = 1, embed_typ= tfidf It took 98.275632 seconds - 0hh:1mm:38ss.\n",
      " start time: Aug 29 2023 02:30:44  end time:  Aug 29 2023 02:32:22\n",
      "Training starts for test_size = 0.3, c_parm = 10 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 334.924811 seconds - 0hh:5mm:34ss.\n",
      " start time: Aug 29 2023 02:32:22  end time:  Aug 29 2023 02:37:57\n",
      "Validation for c_parm = 10, test_size = 0.3, and embed_type tfidf \n",
      "Accuracy Score = 0.8988112086045853\n",
      " f1_score_weighted = 0.8950390237807667\n",
      " f1_score_macro    = 0.46293021940870716\n",
      " f1_score_micro    = 0.8988112086045853\n",
      "Display metrics\n",
      "tab_name = tfidf_L1_L2_TM_tst_siz0.3_C10\n",
      "  Elapse_time for \"validation\". It took 0.252333 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 29 2023 02:37:57  end time:  Aug 29 2023 02:37:57\n",
      "  Elapse_time for \"proc_append\". It took 9.705999 seconds - 0hh:0mm:9ss.\n",
      " start time: Aug 29 2023 02:37:57  end time:  Aug 29 2023 02:38:07\n",
      "---------------------------------------------\\FINAL COUNTS:\n",
      "---------------------------------------------\n",
      "1) 259085\n",
      "2) 259085\n",
      "3) 259085\n",
      "4) 259085\n",
      "5) 259085\n",
      "6) 259085\n",
      " Elapse_time for \"proc_cr_df\". It took 4.963282 seconds - 0hh:0mm:4ss.\n",
      " start time: Aug 29 2023 02:38:07  end time:  Aug 29 2023 02:38:12\n",
      "  Elapse_time to build model for for test_size = 0.3, c_parm = 10, embed_typ= tfidf It took 349.925260 seconds - 0hh:5mm:49ss.\n",
      " start time: Aug 29 2023 02:32:22  end time:  Aug 29 2023 02:38:12\n",
      "Training starts for test_size = 0.3, c_parm = 100 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 380.949846 seconds - 0hh:6mm:20ss.\n",
      " start time: Aug 29 2023 02:38:12  end time:  Aug 29 2023 02:44:33\n",
      "Validation for c_parm = 100, test_size = 0.3, and embed_type tfidf \n",
      "Accuracy Score = 0.8903970357409361\n",
      " f1_score_weighted = 0.8881588788697581\n",
      " f1_score_macro    = 0.45985567862807447\n",
      " f1_score_micro    = 0.8903970357409361\n",
      "Display metrics\n",
      "tab_name = tfidf_L1_L2_TM_tst_siz0.3_C100\n",
      "  Elapse_time for \"validation\". It took 0.275219 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 29 2023 02:44:33  end time:  Aug 29 2023 02:44:33\n",
      "  Elapse_time for \"proc_append\". It took 9.716117 seconds - 0hh:0mm:9ss.\n",
      " start time: Aug 29 2023 02:44:33  end time:  Aug 29 2023 02:44:43\n",
      "---------------------------------------------\\FINAL COUNTS:\n",
      "---------------------------------------------\n",
      "1) 259085\n",
      "2) 259085\n",
      "3) 259085\n",
      "4) 259085\n",
      "5) 259085\n",
      "6) 259085\n",
      " Elapse_time for \"proc_cr_df\". It took 4.943432 seconds - 0hh:0mm:4ss.\n",
      " start time: Aug 29 2023 02:44:43  end time:  Aug 29 2023 02:44:48\n",
      "  Elapse_time to build model for for test_size = 0.3, c_parm = 100, embed_typ= tfidf It took 395.958704 seconds - 0hh:6mm:35ss.\n",
      " start time: Aug 29 2023 02:38:12  end time:  Aug 29 2023 02:44:48\n",
      " samp_frac = 1\n",
      "Before sample syndigo_mapped.shape (259085, 25)\n",
      "After sample syndigo_mapped.shape (259085, 25)\n",
      "  Elapse_time for \"prepare_data_2\". It took 10.519542 seconds - 0hh:0mm:10ss.\n",
      " start time: Aug 29 2023 02:44:48  end time:  Aug 29 2023 02:44:58\n",
      "Training starts for test_size = 0.3, c_parm = 0.1 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 2795.138315 seconds - 0hh:46mm:35ss.\n",
      " start time: Aug 29 2023 02:44:58  end time:  Aug 29 2023 03:31:34\n",
      "Validation for c_parm = 0.1, test_size = 0.3, and embed_type W2vec \n",
      "Accuracy Score = 0.8629930782492344\n",
      " f1_score_weighted = 0.8549327778467467\n",
      " f1_score_macro    = 0.3818464544547649\n",
      " f1_score_micro    = 0.8629930782492344\n",
      "Display metrics\n",
      "tab_name = W2vec_L1_L2_TM_tst_siz0.3_C0.1\n",
      "  Elapse_time for \"validation\". It took 0.248438 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 29 2023 03:31:34  end time:  Aug 29 2023 03:31:34\n",
      "  Elapse_time for \"proc_append\". It took 9.433603 seconds - 0hh:0mm:9ss.\n",
      " start time: Aug 29 2023 03:31:34  end time:  Aug 29 2023 03:31:43\n",
      "---------------------------------------------\\FINAL COUNTS:\n",
      "---------------------------------------------\n",
      "1) 259085\n",
      "2) 259085\n",
      "3) 259085\n",
      "4) 259085\n",
      "5) 259085\n",
      "6) 259085\n",
      " Elapse_time for \"proc_cr_df\". It took 4.810506 seconds - 0hh:0mm:4ss.\n",
      " start time: Aug 29 2023 03:31:43  end time:  Aug 29 2023 03:31:48\n",
      "  Elapse_time to build model for for test_size = 0.3, c_parm = 0.1, embed_typ= W2vec It took 2809.696129 seconds - 0hh:46mm:49ss.\n",
      " start time: Aug 29 2023 02:44:58  end time:  Aug 29 2023 03:31:48\n",
      "Training starts for test_size = 0.3, c_parm = 1 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 2817.059232 seconds - 0hh:46mm:57ss.\n",
      " start time: Aug 29 2023 03:31:48  end time:  Aug 29 2023 04:18:45\n",
      "Validation for c_parm = 1, test_size = 0.3, and embed_type W2vec \n",
      "Accuracy Score = 0.8630574067879474\n",
      " f1_score_weighted = 0.8550018676380433\n",
      " f1_score_macro    = 0.38623462416286264\n",
      " f1_score_micro    = 0.8630574067879474\n",
      "Display metrics\n",
      "tab_name = W2vec_L1_L2_TM_tst_siz0.3_C1\n",
      "  Elapse_time for \"validation\". It took 0.232829 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 29 2023 04:18:45  end time:  Aug 29 2023 04:18:45\n",
      "  Elapse_time for \"proc_append\". It took 10.001494 seconds - 0hh:0mm:10ss.\n",
      " start time: Aug 29 2023 04:18:45  end time:  Aug 29 2023 04:18:55\n",
      "---------------------------------------------\\FINAL COUNTS:\n",
      "---------------------------------------------\n",
      "1) 259085\n",
      "2) 259085\n",
      "3) 259085\n",
      "4) 259085\n",
      "5) 259085\n",
      "6) 259085\n",
      " Elapse_time for \"proc_cr_df\". It took 4.867584 seconds - 0hh:0mm:4ss.\n",
      " start time: Aug 29 2023 04:18:55  end time:  Aug 29 2023 04:19:00\n",
      "  Elapse_time to build model for for test_size = 0.3, c_parm = 1, embed_typ= W2vec It took 2832.201712 seconds - 0hh:47mm:12ss.\n",
      " start time: Aug 29 2023 03:31:48  end time:  Aug 29 2023 04:19:00\n",
      "Training starts for test_size = 0.3, c_parm = 10 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 2815.555140 seconds - 0hh:46mm:55ss.\n",
      " start time: Aug 29 2023 04:19:00  end time:  Aug 29 2023 05:05:56\n",
      "Validation for c_parm = 10, test_size = 0.3, and embed_type W2vec \n",
      "Accuracy Score = 0.863005943956977\n",
      " f1_score_weighted = 0.8549394834597648\n",
      " f1_score_macro    = 0.3822378757334378\n",
      " f1_score_micro    = 0.863005943956977\n",
      "Display metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tab_name = W2vec_L1_L2_TM_tst_siz0.3_C10\n",
      "  Elapse_time for \"validation\". It took 0.255900 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 29 2023 05:05:56  end time:  Aug 29 2023 05:05:56\n",
      "  Elapse_time for \"proc_append\". It took 9.727934 seconds - 0hh:0mm:9ss.\n",
      " start time: Aug 29 2023 05:05:56  end time:  Aug 29 2023 05:06:06\n",
      "---------------------------------------------\\FINAL COUNTS:\n",
      "---------------------------------------------\n",
      "1) 259085\n",
      "2) 259085\n",
      "3) 259085\n",
      "4) 259085\n",
      "5) 259085\n",
      "6) 259085\n",
      " Elapse_time for \"proc_cr_df\". It took 4.790055 seconds - 0hh:0mm:4ss.\n",
      " start time: Aug 29 2023 05:06:06  end time:  Aug 29 2023 05:06:11\n",
      "  Elapse_time to build model for for test_size = 0.3, c_parm = 10, embed_typ= W2vec It took 2830.361480 seconds - 0hh:47mm:10ss.\n",
      " start time: Aug 29 2023 04:19:00  end time:  Aug 29 2023 05:06:11\n",
      "Training starts for test_size = 0.3, c_parm = 100 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 2802.923455 seconds - 0hh:46mm:42ss.\n",
      " start time: Aug 29 2023 05:06:11  end time:  Aug 29 2023 05:52:54\n",
      "Validation for c_parm = 100, test_size = 0.3, and embed_type W2vec \n",
      "Accuracy Score = 0.86307027249569\n",
      " f1_score_weighted = 0.8549973794730806\n",
      " f1_score_macro    = 0.3836888006390127\n",
      " f1_score_micro    = 0.86307027249569\n",
      "Display metrics\n",
      "tab_name = W2vec_L1_L2_TM_tst_siz0.3_C100\n",
      "  Elapse_time for \"validation\". It took 0.264845 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 29 2023 05:52:54  end time:  Aug 29 2023 05:52:54\n",
      "  Elapse_time for \"proc_append\". It took 9.952194 seconds - 0hh:0mm:9ss.\n",
      " start time: Aug 29 2023 05:52:54  end time:  Aug 29 2023 05:53:04\n",
      "---------------------------------------------\\FINAL COUNTS:\n",
      "---------------------------------------------\n",
      "1) 259085\n",
      "2) 259085\n",
      "3) 259085\n",
      "4) 259085\n",
      "5) 259085\n",
      "6) 259085\n",
      " Elapse_time for \"proc_cr_df\". It took 4.927837 seconds - 0hh:0mm:4ss.\n",
      " start time: Aug 29 2023 05:53:04  end time:  Aug 29 2023 05:53:09\n",
      "  Elapse_time to build model for for test_size = 0.3, c_parm = 100, embed_typ= W2vec It took 2818.117434 seconds - 0hh:46mm:58ss.\n",
      " start time: Aug 29 2023 05:06:11  end time:  Aug 29 2023 05:53:09\n"
     ]
    }
   ],
   "source": [
    "l_c_parm = [0.1, 1, 10, 100] \n",
    "l_embed_typ = ['tfidf', 'W2vec']\n",
    "\n",
    "dic_metric={}\n",
    "dic_predict = {}\n",
    "l_metric=[]\n",
    "test_size = 0.3\n",
    "i = 0\n",
    "for embed_typ in l_embed_typ:\n",
    "    X, y, l1_l2_id_map, id_l1_l2_map = prepare_data_2(samp_frac = 1, embed_typ = embed_typ )\n",
    "    for c_parm in l_c_parm:\n",
    "        start_time = time.time()        \n",
    "        \n",
    "        preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.3, c_parm =c_parm,  embed_typ= embed_typ )\n",
    " \n",
    "        test_metrics, vtab, l_metrics  =  validation(test_size = 0.3, c_parm =c_parm, embed_typ=embed_typ) \n",
    "        key = 'test_metric' + str(i)\n",
    "        dic_metric [key] = (test_metrics, vtab)\n",
    "        \n",
    "        test_new_proobs, train_new_proobs, testLevels, testLevelss, trainLevelss = proc_append()\n",
    "        df_save, ftab  = proc_cr_df(test_size=0.3, c_parm=c_parm, embed_typ= embed_typ )\n",
    "        key = 'predict'+ str(i)\n",
    "        dic_predict[key] = (df_save, ftab)\n",
    "        i = i + 1   \n",
    "        end_time = time.time() \n",
    "        desc = f' Elapse_time to build model for for test_size = 0.3, c_parm = {c_parm}, embed_typ= {embed_typ}' \n",
    "        hh, mm, ss= elapse_time (  start_time, end_time, desc)\n",
    "                                 \n",
    "        l_metrics = l_metrics +  [f\"{hh:2d}hh:{mm:2d}mm:{ss:2d}ss\"]   \n",
    "        l_metric.append(l_metrics)\n",
    "dte = date.today().strftime('%m%d%y')                                  \n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_cls_rpt_size{test_size}_{dte}.xlsx'\n",
    "save_excel_metric( excel_file, dic_metric, l_metric)\n",
    "\n",
    "\n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_tst_size{test_size}_{dte}.xlsx'\n",
    "save_excel_pred( excel_file, dic_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = DBFO + f'L1_L2_syndigo_pred_tst_size{test_size}_082527.xlsx'\n",
    "save_excel( excel_file, dic_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_predict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3 \n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_cls_rpt_size{test_size}_082527.xlsx'\n",
    "save_excel_metric( excel_file, dic_metric, l_metric)\n",
    "\n",
    "\n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_tst_size{test_size}_082527.xlsx'\n",
    "save_excel_pred( excel_file, dic_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m multi_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultinomial\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC AUC score for Multiclass= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmulti_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:566\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_class must be in (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multiclass_roc_auc_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    570\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:638\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[1;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;124;03m\"\"\"Multiclass roc auc score.\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \n\u001b[0;32m    594\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m \n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# validation of the input y_score\u001b[39;00m\n\u001b[1;32m--> 638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[38;5;241m1\u001b[39m, \u001b[43my_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    640\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget scores need to be probabilities for multiclass \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc, i.e. they should sum up to 1.0 over classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# validation for multiclass parameter specifications\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "multi_class = 'multinomial'\n",
    "score = roc_auc_score(y_test, preds, multi_class= multi_class)\n",
    "print(f\"ROC AUC score for Multiclass= {multi_class}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Syndigo Mapping MachineLearning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
