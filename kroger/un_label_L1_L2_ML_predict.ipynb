{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# un_label_L1_L2_ML_predict\n",
    "  - module name: un_label_L1_L2_ML_predict.ipynb\n",
    "  - Purpose: Load pickle file and predict unlable data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_1():\n",
    "    pgm = inspect.currentframe().f_code.co_name  \n",
    "    start_time = time.time() \n",
    "    # Reading PIMMART data\n",
    "    pim_gtin_mapped = pd.read_csv(DBFR + \"PIM_Data_New_50_82Mn.csv\", dtype=object) # (5082212, 24)\n",
    "    \n",
    "    print(f'Before drop pim_gtin_mapped.shape {pim_gtin_mapped.shape}')    \n",
    "    SUBCOMS_excluded = ['INVENTORY VALUES','MISC SALES TRANS','MISC SALES TRANS (NON TAX)','CENTRAL SUPPLIES', \\\n",
    "                    'MISC SALES TRANS (TX TAXABLE)','COUPON','MISCELLANEOUS INCOME','CRV/EXCISE TAX NT/NF', \\\n",
    "                    'CRV DEPOSIT NT/F','CRV DEPOSIT T/NF','MISCELLANEOUS REFUNDS','CUSTOMER EXPENSES','ROUND UP COUPONS']\n",
    "    pim_gtin_mapped = pim_gtin_mapped[~pim_gtin_mapped.SUBCOM_DSC.isin(SUBCOMS_excluded)]   \n",
    "    print(f'After drop pim_gtin_mapped.shape {pim_gtin_mapped.shape}')\n",
    "    \n",
    "    for i in ['SUBCOM_CD', 'DPT_CD', 'COM_CD','PMY_DPT_CD', 'REC_DPT_CD', 'ITM_ID', 'GTIN']:\n",
    "         pim_gtin_mapped[i] = pim_gtin_mapped[i].astype(np.float64)\n",
    "    \n",
    "    # drop 'Level 1' ~ 'Level 10'\n",
    "    l_rmv_col =['ITM_ID_y','Level 1', 'Level 2',\n",
    "       'Level 3', 'Level 4', 'Level 5', 'Level 6', 'Level 7', 'Level 8',\n",
    "       'Level 9', 'Level 10']\n",
    "    pim_gtin_mapped.drop(columns = l_rmv_col, inplace = True ) \n",
    "\n",
    "    pim_gtin_mapped.drop_duplicates('GTIN', inplace = True)\n",
    "    pim_gtin_mapped['ITEM_SUBCOM_text'] = \\\n",
    "    (pim_gtin_mapped.VND_ECOM_DSC + ' ' + pim_gtin_mapped.SUBCOM_DSC).fillna('').str.lower()\n",
    "    #syndigo_mapped['Level 1'].value_counts()\n",
    "    #print(f'syndigo_mapped.shape {syndigo_mapped.shape}') \n",
    "    #print(f'syndigo_mapped.info() {syndigo_mapped.info()}') \n",
    "    #print(f'syndigo_mapped.head() {syndigo_mapped.head()}') \n",
    "    #print(f'syndigo_mapped.columns {syndigo_mapped.columns}') \n",
    "    # Memory issue; Divid pim_gtin_mapped into two df for predecting\n",
    "    pim_gtin1, pim_gtin2, pim_gtin3, pim_gtin4 =  np.array_split(pim_gtin_mapped, 4)  \n",
    "    return pim_gtin_mapped, pim_gtin1, pim_gtin2, pim_gtin3, pim_gtin4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_2(pim_gtin, embed_typ = 'tfidf'):\n",
    "  \n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name \n",
    "    global model\n",
    "  \n",
    "    if embed_typ == 'tfidf': \n",
    "        vect = TfidfVectorizer(ngram_range = (1,2), max_features = 50000) \n",
    "        X = vect.fit_transform(pim_gtin.ITEM_SUBCOM_text) #scipy.sparse._csr.csr_matrix\n",
    "    else:\n",
    "        model = KeyedVectors.load_word2vec_format(DBFO + 'w2vmodel_053123_PIM_ALL.bin', binary=True)\n",
    "        X = np.array(list(pim_gtin.ITEM_SUBCOM_text.apply(lambda x: get_item_vector(x.split(' ')))))\n",
    "\n",
    "    \n",
    "    end_time = time.time()   \n",
    "    desc = f'Process DF {df} with elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)\n",
    "    return  X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build item vectors\n",
    "def get_item_vector(item_vocab):\n",
    "    vect = np.zeros_like(model.get_vector('chips'))\n",
    "    for word in item_vocab:\n",
    "        if word in model:\n",
    "            vect += model.get_vector(word)\n",
    "    return vect#/max(1,len(item_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_pred(embed_typ):\n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name \n",
    "    save_model =  DBFR + f'L1_l2_LR_{embed_typ}_tst_siz0.3_C100_083123.pkl'\n",
    "    print (f' proc_pred for embed_typ = {embed_typ}')\n",
    "    with open(save_model , 'rb') as f:\n",
    "         model = pickle.load(f)\n",
    "    preds = list(model.predict(X))    \n",
    "    probx =model.predict_proba(X)\n",
    "    probs = []\n",
    "    for i in range(len(preds)):\n",
    "      probs.append(probx[i][np.argsort(probx[i])][::-1][:1])\n",
    "    probs = [element for sublist in list(probs) for element in sublist]\n",
    "    \n",
    "    data = {'Predict': preds, 'Scores': probs}\n",
    "    df_pred= pd.DataFrame(data)\n",
    "    df_pred['Predict'] = df_pred['Predict'].fillna('Other').map(id_l1_l2_map) \n",
    "    df_merg = pd.merge( pim_gtin_mapped, df_pred, left_index=True, right_index=True)\n",
    "\n",
    "    end_time = time.time() \n",
    "    desc = f' Elapse_time to predict un-label data for embed_typ= {embed_typ}' \n",
    "    hh, mm, ss= elapse_time (  start_time, end_time, desc)\n",
    "                                   \n",
    "    return df_merg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_save(df,embed_typ):\n",
    "   dte = date.today().strftime('%m%d%y')  \n",
    "   file_name = DBFR + f'l1_l2_unlabel_pred_map_{embed_typ}_{dte}.csv'\n",
    "   df.to_csv(file_name, index=None)\n",
    "   return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b64183-34df-46b6-b1c9-6bbe78776fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier,  RidgeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import re, time, inspect, pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.options.display.float_format = '{:20,.4f}'.format \n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# load  function of 'elapse_time'\n",
    "path_code = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Code\\\\'  \n",
    "f_com_code = path_code + \"com_code.py\"\n",
    "exec(compile(open(f_com_code , \"rb\").read(), f_com_code, 'exec' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBFS = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFO = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFM = \"/dbfs/FileStore/tables/MALLIK/\"\n",
    "DBFR = \"/dbfs/FileStore/tables/OFFSHORE_RESULTS/\"\n",
    "path = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Data\\\\'   \n",
    "DBFS = path\n",
    "DBFO = path\n",
    "DBFM = path\n",
    "DBFR = path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop pim_gtin_mapped.shape (5082212, 24)\n",
      "After drop pim_gtin_mapped.shape (5061471, 24)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pim_gtin4copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m pim_gtin2_bkup \u001b[38;5;241m=\u001b[39m  pim_gtin2\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      5\u001b[0m pim_gtin3_bkup \u001b[38;5;241m=\u001b[39m  pim_gtin3\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m----> 6\u001b[0m pim_gtin4_bkup \u001b[38;5;241m=\u001b[39m  \u001b[43mpim_gtin4copy\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pim_gtin4copy' is not defined"
     ]
    }
   ],
   "source": [
    "pim_gtin_mapped, pim_gtin1, pim_gtin2,pim_gtin3, pim_gtin4 = prepare_data_1()\n",
    "pim_gtin_mapped_bkup =  pim_gtin_mapped.copy()\n",
    "pim_gtin1_bkup =  pim_gtin1.copy()\n",
    "pim_gtin2_bkup =  pim_gtin2.copy()\n",
    "pim_gtin3_bkup =  pim_gtin3.copy()\n",
    "pim_gtin4_bkup =  pim_gtin4.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim_gtin4_bkup =  pim_gtin4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " process pim _gtin1 for W2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Elapse_time to predict un-label data for embed_typ= W2vec It took 36.851799 seconds - 0hh:0mm:36ss.\n",
      " start time: Aug 31 2023 21:16:42  end time:  Aug 31 2023 21:17:19\n",
      " process pim _gtin2 for W2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Elapse_time to predict un-label data for embed_typ= W2vec It took 31.331971 seconds - 0hh:0mm:31ss.\n",
      " start time: Aug 31 2023 21:19:27  end time:  Aug 31 2023 21:19:59\n",
      " process pim _gtin3 for W2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Elapse_time to predict un-label data for embed_typ= W2vec It took 34.802090 seconds - 0hh:0mm:34ss.\n",
      " start time: Aug 31 2023 21:22:14  end time:  Aug 31 2023 21:22:49\n",
      " process pim _gtin4 for W2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Elapse_time to predict un-label data for embed_typ= W2vec It took 29.999414 seconds - 0hh:0mm:29ss.\n",
      " start time: Aug 31 2023 21:25:06  end time:  Aug 31 2023 21:25:36\n"
     ]
    }
   ],
   "source": [
    " \n",
    "l_embed_typ = ['tfidf', 'W2vec']\n",
    "l_embed_typ = ['W2vec']\n",
    "c_parm = 100\n",
    "l_df= [pim_gtin1, pim_gtin2, pim_gtin3, pim_gtin4 ]\n",
    "\n",
    "\n",
    "#l_embed_typ = ['tfidf', 'W2vec']\n",
    "\n",
    "map_csv = pd.read_csv(DBFR + \"id_l1_l2_map.csv\")\n",
    "l_key = map_csv['index']\n",
    "l_val = map_csv['l1_l2']\n",
    "id_l1_l2_map =  dict (zip(l_key, l_val))\n",
    "\n",
    "samp_frac = 0.5\n",
    "\n",
    "for embed_typ in l_embed_typ:\n",
    "    i = 1\n",
    "    for df in l_df: \n",
    "        print (f' process pim _gtin{i} for {embed_typ}' )\n",
    "        X= prepare_data_2(df, embed_typ = embed_typ )\n",
    "        globals()[f'df{i}'] = proc_pred(embed_typ)\n",
    "        i = i + 1 \n",
    "    globals()[f'df_{embed_typ}_cont'] =  pd.concat([df1, df2, df3, df4])\n",
    "    proc_save(globals()[f'df_{embed_typ}_cont'], embed_typ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ITM_ID', 'GTIN', 'PMY_DPT_CD', 'PMY_DPT_DSC', 'REC_DPT_CD',\n",
       "       'REC_DPT_DSC', 'DPT_CD', 'DPT_DSC', 'COM_CD', 'COM_DSC', 'SUBCOM_CD',\n",
       "       'SUBCOM_DSC', 'VND_ECOM_DSC', 'ITEM_SUBCOM_text', 'Predict',\n",
       "       'Accuracy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_W2vec_cont.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5012719, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_W2vec_cont.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_W2vec_cont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAccuracy: Scores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:5573\u001b[0m, in \u001b[0;36mDataFrame.rename\u001b[1;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[0;32m   5454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrename\u001b[39m(\n\u001b[0;32m   5455\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5456\u001b[0m     mapper: Renamer \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5464\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5465\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5466\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5467\u001b[0m \u001b[38;5;124;03m    Alter axes labels.\u001b[39;00m\n\u001b[0;32m   5468\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5571\u001b[0m \u001b[38;5;124;03m    4  3  6\u001b[39;00m\n\u001b[0;32m   5572\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rename\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5577\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5579\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5581\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:1104\u001b[0m, in \u001b[0;36mNDFrame._rename\u001b[1;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         missing_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1098\u001b[0m             label\n\u001b[0;32m   1099\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m index, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(replacements)\n\u001b[0;32m   1100\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m indexer[index] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1101\u001b[0m         ]\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m new_index \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_axis_nocheck(new_index, axis\u001b[38;5;241m=\u001b[39maxis_no, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1106\u001b[0m result\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6416\u001b[0m, in \u001b[0;36mIndex._transform_index\u001b[1;34m(self, func, level)\u001b[0m\n\u001b[0;32m   6414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_tuples(items, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames)\n\u001b[0;32m   6415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 6416\u001b[0m     items \u001b[38;5;241m=\u001b[39m [func(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m]\n\u001b[0;32m   6417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(items, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   6414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_tuples(items, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames)\n\u001b[0;32m   6415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 6416\u001b[0m     items \u001b[38;5;241m=\u001b[39m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m]\n\u001b[0;32m   6417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(items, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'set' object is not callable"
     ]
    }
   ],
   "source": [
    "df_W2vec_cont.rename(columns = {'Accuracy: Scores'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W2vec_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W2vec_cont = df_W2vec_cont.rename(columns= {'Accuracy: Probality'})\n",
    "\n",
    "rankings_pd.rename(columns = {'test':'TEST', 'odi':'ODI',\n",
    "                              't20':'T20'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model =  DBFR + 'L1_l2_LR_tfidf_tst_siz0.3_C100.pkl'\n",
    "with open(save_model , 'rb') as f:\n",
    "     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X)  # max(pred)+  200; type[array([ 1, 20, 12, ...,  6, 20,  6], dtype=int64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.fillna('Other').map(id_l1_l2_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predx = list(preds)\n",
    "probx = list(probs)\n",
    "type(predx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {'preds': predx, 'probs': probx}\n",
    "df_pred= pd.DataFrame(data)\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X)  #numpy.ndarray; len = 1524664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Data\\\\'\n",
    "map_csv = pd.read_csv(path + \"id_l1_l2_map.csv\")\n",
    "l_val = map_csv['index']\n",
    "l_key = map_csv['l1_l2'].tolist()\n",
    "id_l1_l2_map =  dict (zip(l_key, l_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_c_parm = a0.1, 1, 10, 100] \n",
    "l_embed_typ = ['tfidf', 'W2vec']\n",
    "\n",
    "dic_metric={}\n",
    "dic_predict = {}\n",
    "l_metric=[]\n",
    "test_size = 0.3\n",
    "i = 0\n",
    "for embed_typ in l_embed_typ:\n",
    "    X, y, l1_l2_id_map, id_l1_l2_map = prepare_data_2(samp_frac = 1, embed_typ = embed_typ )\n",
    "    for c_parm in l_c_parm:\n",
    "        start_time = time.time()        \n",
    "        \n",
    "        preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.3, c_parm =c_parm,  embed_typ= embed_typ )\n",
    " \n",
    "        test_metrics, vtab, l_metrics  =  validation(test_size = 0.3, c_parm =c_parm, embed_typ=embed_typ) \n",
    "        key = 'test_metric' + str(i)\n",
    "        dic_metric [key] = (test_metrics, vtab)\n",
    "        \n",
    "        test_new_proobs, train_new_proobs, testLevels, testLevelss, trainLevelss = proc_append()\n",
    "        df_save, ftab  = proc_cr_df(test_size=0.3, c_parm=c_parm, embed_typ= embed_typ )\n",
    "        key = 'predict'+ str(i)\n",
    "        dic_predict[key] = (df_save, ftab)\n",
    "        i = i + 1   \n",
    "        end_time = time.time() \n",
    "        desc = f' Elapse_time to build model for for test_size = 0.3, c_parm = {c_parm}, embed_typ= {embed_typ}' \n",
    "        hh, mm, ss= elapse_time (  start_time, end_time, desc)\n",
    "                                 \n",
    "        l_metrics = l_metrics +  [f\"{hh:2d}hh:{mm:2d}mm:{ss:2d}ss\"]   \n",
    "        l_metric.append(l_metrics)\n",
    "dte = date.today().strftime('%m%d%y')                                  \n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_cls_rpt_size{test_size}_{dte}.xlsx'\n",
    "save_excel_metric( excel_file, dic_metric, l_metric)\n",
    "\n",
    "\n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_tst_size{test_size}_{dte}.xlsx'\n",
    "save_excel_pred( excel_file, dic_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = DBFO + f'L1_L2_syndigo_pred_tst_size{test_size}_082527.xlsx'\n",
    "save_excel( excel_file, dic_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_predict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3 \n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_cls_rpt_size{test_size}_082527.xlsx'\n",
    "save_excel_metric( excel_file, dic_metric, l_metric)\n",
    "\n",
    "\n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_tst_size{test_size}_082527.xlsx'\n",
    "save_excel_pred( excel_file, dic_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class = 'multinomial'\n",
    "score = roc_auc_score(y_test, preds, multi_class= multi_class)\n",
    "print(f\"ROC AUC score for Multiclass= {multi_class}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim_gtin_mapped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim_gtin_mapped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim_gtin_mapped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Syndigo Mapping MachineLearning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
