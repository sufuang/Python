{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "  - 3. Syndigo Mapping MachineLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_no_nan(x):\n",
    "    return x.dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrcut_dic():\n",
    "    key = unique_no_nan(syndigo_mapped['Level 1'])\n",
    "    value = list( map(lambda x : re.sub('&|/', '', x), key))\n",
    "    value = list( map(lambda x : re.sub(' +', '_', x), value))\n",
    "    value = list(map(str.lower, value))\n",
    "    #value = ['-'.join(['tfidf'+ str]), value]\n",
    "\n",
    "    print(f'key: {key}') \n",
    "    print(f'value: {value}')\n",
    "    return dict(zip(key, value)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_1():\n",
    "    # Reading PIMMART data\n",
    "    pim_gtin_mapped = pd.read_csv(DBFR + \"PIM_Data_New_50_82Mn.csv\", dtype=object)\n",
    "    for i in ['SUBCOM_CD', 'DPT_CD', 'COM_CD','PMY_DPT_CD', 'REC_DPT_CD', 'ITM_ID', 'GTIN']:\n",
    "        pim_gtin_mapped[i] = pim_gtin_mapped[i].astype(np.float64)\n",
    "    \n",
    "    # Reading Syndigo 259K data\n",
    "    synd_ALL = pd.read_csv(DBFR + 'Syndigo_Final_ALL.csv', dtype='unicode') # 259k Syndigo Data\n",
    "    for i in ['SUBCOM_CD', 'DPT_CD', 'COM_CD', 'GTIN', 'ITM_ID', 'PMY_DPT_CD']:\n",
    "        synd_ALL[i] = synd_ALL[i].astype(np.float64)\n",
    "    \n",
    "    # Stripping spaces from all columns\n",
    "    df_obj = synd_ALL.select_dtypes(['object'])\n",
    "    synd_ALL[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    syndigo_mapped = synd_ALL\n",
    "    pimmart = pim_gtin_mapped\n",
    "    syndigo_mapped.drop_duplicates('GTIN', inplace = True)\n",
    "    syndigo_mapped['ITEM_SUBCOM_text'] = \\\n",
    "    (syndigo_mapped.VND_ECOM_DSC + ' ' + syndigo_mapped.SUBCOM_DSC).fillna('').str.lower()\n",
    "    #syndigo_mapped['Level 1'].value_counts()\n",
    "    #print(f'syndigo_mapped.shape {syndigo_mapped.shape}') \n",
    "    #print(f'syndigo_mapped.info() {syndigo_mapped.info()}') \n",
    "    #print(f'syndigo_mapped.head() {syndigo_mapped.head()}') \n",
    "    #print(f'syndigo_mapped.columns {syndigo_mapped.columns}') \n",
    "    \n",
    "    print(f\"syndigo_mapped['Level 1'].value_counts() {syndigo_mapped['Level 1'].value_counts(dropna = False)}\")    \n",
    "    return syndigo_mapped, pim_gtin_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_2(level__1):\n",
    "    level_1 = level__1\n",
    "    subset_df = syndigo_mapped[syndigo_mapped['Level 1'] == level_1]\n",
    "    #print(f'1. type(subset_df)= {type(subset_df)}, len(subset_df)= {len(subset_df)} subset_df.shape {subset_df.shape}  \\n subset_df.head {subset_df.head()}') \n",
    "    series = syndigo_mapped[syndigo_mapped['Level 1']==level__1]['Level 2'].value_counts(ascending=True)\n",
    "    #print(f' series {series} \\n type(series)= {type(series)}, len(series)= {len(series)} \\n series.tolist() {series.tolist()} ')      \n",
    "    if series.tolist():\n",
    "        subset_df = subset_df.drop(subset_df[subset_df['Level 2'].isin(series[series==1].index.tolist())].index)\n",
    "    #print(f'2. type(subset_df)= {type(subset_df)}, len(subset_df)= {len(subset_df)} subset_df.shape {subset_df.shape}  \\n subset_df.head {subset_df.head()}')\n",
    "    \n",
    "    vect_subset = TfidfVectorizer(ngram_range = (1,2), max_features = 50000)\n",
    "    X_subset = vect_subset.fit_transform(subset_df.ITEM_SUBCOM_text) #scipy.sparse._csr.csr_matrix\n",
    "    level2_id_map = dict(zip(subset_df['Level 2'].fillna('Other').unique(), range(subset_df['Level 2'].fillna('Other').nunique())))\n",
    "    id2_level_map = dict(zip(range(subset_df['Level 2'].fillna('Other').nunique()), subset_df['Level 2'].fillna('Other').unique()))\n",
    "    y_subset  = subset_df['Level 2'].fillna('Other').map(level2_id_map)\n",
    "    \n",
    "    return  X_subset, y_subset, subset_df, level2_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_predict(level__1):\n",
    "     global A_train, A_test, B_train, B_test, X_train, X_test, y_train, y_test\n",
    "     A_train, A_test, B_train, B_test = train_test_split(subset_df.GTIN.tolist(), subset_df['Level 2'].tolist(), test_size=0.1, stratify=y_subset, random_state=42)\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size= 0.1, stratify=y_subset, random_state=42)\n",
    "     #lr_tf = LogisticRegressionCV(cv = 5, multi_class = 'multinomial', solver = 'saga', n_jobs=-1, max_iter=10000000)\n",
    "     lr_tf = LogisticRegression(C = 100, multi_class = 'multinomial', solver = 'saga', n_jobs=-1)\n",
    "     \n",
    "     print(f\"Training - \\\"{level__1}\\\"\")\n",
    "     lr_tf.fit(X_train, y_train)\n",
    "     preds = lr_tf.predict(X_test)\n",
    "     #print(f\"type(preds) {type(preds)} len(preds) {len(preds)} \\n preds[0:10] {preds[0:10]}\")\n",
    "     preds_lrtf = preds\n",
    "     probs = lr_tf.predict_proba(X_test)\n",
    "     #print(f\"type(probs) {type(probs)} len(probs) {len(probs)} \\n probs[0:10] {probs[0:10]}\")   \n",
    "     preds_train = lr_tf.predict(X_train)\n",
    "     #print(f\"type(preds_train) {type(preds_train)} len(preds_train) {len(preds_train)} \\n preds_train[0:10] {preds_train[0:10]} \")   \n",
    "     probs_train = lr_tf.predict_proba(X_train)\n",
    "     #print(f\"type(probs_train) {type(probs_train)} len(probs_train) {len(probs_train)} \\n probs_train[0:10] {probs_train[0:10]}\")   \n",
    "     #print(classification_report(y_test, preds_test,labels = lr_tf.classes_, target_names = [id2_level_map[i] for i in lr_tf.classes_]))\n",
    "     print(f\"Done Training - \\\"{level__1}\\\"\")\n",
    "     return preds, preds_lrtf, probs, preds_train, probs_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_append():\n",
    "    \n",
    "    test_new_proobs = []\n",
    "    \n",
    "    #print(f'len(preds)  {len(preds)}')\n",
    "    for i in range(len(preds)):\n",
    "        test_new_proobs.append(probs[i][np.argsort(probs[i])][::-1][:1].tolist())\n",
    "    test_new_proobs = [element for sublist in list(test_new_proobs) for element in sublist]\n",
    "    #print(f'test_new_proobs {test_new_proobs}')\n",
    "    \n",
    "    train_new_proobs = []\n",
    "    \n",
    "    #print(f'len(preds_train)  {len(preds_train)}')\n",
    "    for i in range(len(preds_train)):\n",
    "         test_new_proobs.append(probs_train[i][np.argsort(probs_train[i])][::-1][:1].tolist())\n",
    "    train_new_proobs = [element for sublist in list(train_new_proobs) for element in sublist]\n",
    "    #print(f'train_new_proobs {train_new_proobs}')  \n",
    "    \n",
    "    # process testLevels & trainLevelss \n",
    "    testLevels = []\n",
    "    for j in y_test:\n",
    "        testLevels.append([i for i in level2_id_map if level2_id_map[i]==j][0])\n",
    "    #print(f'testLevels {testLevels [0:10]}')\n",
    "    testLevelss = []\n",
    "    for j in preds:\n",
    "        testLevelss.append([i for i in level2_id_map if level2_id_map[i]==j][0])\n",
    "    #print(f'testLevelss {testLevelss [0:10]}')\n",
    "    \n",
    "    trainLevelss = []\n",
    "    for j in preds_train:\n",
    "        trainLevelss.append([i for i in level2_id_map if level2_id_map[i]==j][0])\n",
    "    \n",
    "    return  test_new_proobs, train_new_proobs, testLevels, testLevelss, trainLevelss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_save(level__1, filenamee):\n",
    "    print(\"---------------------------------------------\\FINAL COUNTS:\\n---------------------------------------------\")\n",
    "    print(\"1)\", len(A_test + A_train))\n",
    "    print(\"2)\", len(['Test']*len(A_test) + ['Train']*len(A_train)))\n",
    "    print(\"3)\", len(B_test + B_train))\n",
    "    print(\"4)\", len(testLevels + B_train))\n",
    "    print(\"5)\", len(testLevelss + trainLevelss))\n",
    "    print(\"6)\", int(len(test_new_proobs + train_new_proobs)))\n",
    "    \n",
    "    data = {\n",
    "        'GTIN' : A_test + A_train,\n",
    "        'Source': ['Test']*len(A_test) + ['Train']*len(A_train),\n",
    "        'Actual Level 2' : B_test + B_train,\n",
    "        'Actuals' : testLevels + B_train,\n",
    "        'Predictions' : testLevelss + trainLevelss,\n",
    "        'Scores' : test_new_proobs + train_new_proobs,\n",
    "        'L1 Name' : level__1\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.merge(pim_gtin_mapped[['GTIN', 'ITM_ID', 'PMY_DPT_CD', 'PMY_DPT_DSC', 'REC_DPT_CD',\n",
    "        'REC_DPT_DSC', 'DPT_CD', 'DPT_DSC', 'COM_CD', 'COM_DSC', 'SUBCOM_CD',\n",
    "        'SUBCOM_DSC', 'VND_ECOM_DSC']], on='GTIN', how='left')\n",
    "    df = df[['ITM_ID', 'GTIN', 'PMY_DPT_CD', 'PMY_DPT_DSC', 'REC_DPT_CD', 'REC_DPT_DSC', 'DPT_CD',\n",
    "        'DPT_DSC', 'COM_CD', 'COM_DSC', 'SUBCOM_CD', 'SUBCOM_DSC',\n",
    "        'VND_ECOM_DSC', 'Source', 'Actual Level 2', 'Actuals', 'Predictions', 'Scores', 'L1 Name']]\n",
    "    df.to_csv(DBFO + filenamee + '.csv', index=False)\n",
    "    print(f\"{filenamee}.csv   --file created successfully\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b64183-34df-46b6-b1c9-6bbe78776fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier,  RidgeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1a5c73-5356-4b00-89a8-c87c775a321a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBFS = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFO = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFM = \"/dbfs/FileStore/tables/MALLIK/\"\n",
    "DBFR = \"/dbfs/FileStore/tables/OFFSHORE_RESULTS/\"\n",
    "path = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Data\\\\'   \n",
    "DBFS = path\n",
    "DBFO = path\n",
    "DBFM = path\n",
    "DBFR = path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syndigo_mapped['Level 1'].value_counts() Food / Beverages                            131593\n",
      "Health & Beauty                              62259\n",
      "Beer / Wine / Spirits                        17992\n",
      "Cleaning & Janitorial                         9084\n",
      "Livestock & Pet Supplies                      6325\n",
      "Kitchen & Bathroom                            5268\n",
      "Home & Venue Decoration                       3924\n",
      "Toys / Games / Hobbies                        3191\n",
      "Gardening & Outdoors                          2208\n",
      "Childcare                                     2111\n",
      "Office Supplies                               1671\n",
      "Electronics                                   1499\n",
      "Apparel                                       1283\n",
      "Lighting & Fans                               1252\n",
      "Tobacco Products                              1252\n",
      "NaN                                           1132\n",
      "Not classified                                1018\n",
      "Hardware                                       923\n",
      "Arts & Crafts                                  803\n",
      "Automotive                                     778\n",
      "Appliances                                     689\n",
      "Sports & Outdoor Recreation Equipment          684\n",
      "Hospitality Supplies                           496\n",
      "Tools                                          474\n",
      "Books & Videos                                 377\n",
      "Electrical                                     215\n",
      "Heating / Ventilation / Air Conditioning       196\n",
      "Plumbing & Water Service                       172\n",
      "Paints & Coatings                               60\n",
      "Building Supplies                               40\n",
      "Power Sports                                    39\n",
      "Furniture                                       37\n",
      "Marine                                          16\n",
      "Flooring                                        13\n",
      "Material Handling                                6\n",
      "Agricultural Equipment                           3\n",
      "Musical Instruments                              1\n",
      "Power Transmission & Motion Control              1\n",
      "Name: Level 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "syndigo_mapped, pim_gtin_mapped = prepare_data_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: ['Food / Beverages' 'Health & Beauty' 'Beer / Wine / Spirits'\n",
      " 'Kitchen & Bathroom' 'Cleaning & Janitorial' 'Home & Venue Decoration'\n",
      " 'Livestock & Pet Supplies' 'Toys / Games / Hobbies' 'Childcare'\n",
      " 'Tobacco Products' 'Apparel' 'Gardening & Outdoors'\n",
      " 'Sports & Outdoor Recreation Equipment' 'Office Supplies'\n",
      " 'Plumbing & Water Service' 'Lighting & Fans' 'Appliances' 'Electronics'\n",
      " 'Not classified' 'Automotive' 'Tools' 'Hardware' 'Books & Videos'\n",
      " 'Arts & Crafts' 'Electrical' 'Heating / Ventilation / Air Conditioning'\n",
      " 'Hospitality Supplies' 'Furniture' 'Power Sports'\n",
      " 'Power Transmission & Motion Control' 'Marine' 'Flooring'\n",
      " 'Paints & Coatings' 'Building Supplies' 'Musical Instruments'\n",
      " 'Material Handling' 'Agricultural Equipment']\n",
      "value: ['food_beverages', 'health_beauty', 'beer_wine_spirits', 'kitchen_bathroom', 'cleaning_janitorial', 'home_venue_decoration', 'livestock_pet_supplies', 'toys_games_hobbies', 'childcare', 'tobacco_products', 'apparel', 'gardening_outdoors', 'sports_outdoor_recreation_equipment', 'office_supplies', 'plumbing_water_service', 'lighting_fans', 'appliances', 'electronics', 'not_classified', 'automotive', 'tools', 'hardware', 'books_videos', 'arts_crafts', 'electrical', 'heating_ventilation_air_conditioning', 'hospitality_supplies', 'furniture', 'power_sports', 'power_transmission_motion_control', 'marine', 'flooring', 'paints_coatings', 'building_supplies', 'musical_instruments', 'material_handling', 'agricultural_equipment']\n"
     ]
    }
   ],
   "source": [
    "dic_level1= constrcut_dic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - \"Food / Beverages\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iny2819\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Training - \"Food / Beverages\"\n",
      "---------------------------------------------\\FINAL COUNTS:\n",
      "---------------------------------------------\n",
      "1) 131593\n",
      "2) 131593\n",
      "3) 131593\n",
      "4) 131593\n",
      "5) 131593\n",
      "6) 131593\n",
      "food_beverages.csv   --file created successfully\n"
     ]
    }
   ],
   "source": [
    "#dic_test={\"Hospitality Supplies\":\"hospitality_spplies\", \"Tobacco Products\":\"bacco_products\"}\n",
    "#dic_test={\"Food / Beverages\":\"food_beverages\"}\n",
    "\n",
    "for level__1, filenamee in dic_level1.items():\n",
    "    X_subset, y_subset, subset_df, level2_id_map =  prepare_data_2(level__1)\n",
    "    preds, preds_lrtf, probs, preds_train, probs_train = proc_predict(level__1)\n",
    "    test_new_proobs, train_new_proobs, testLevels, testLevelss, trainLevelss = proc_append()\n",
    "    proc_save(level__1, filenamee) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[810047600510.0,\n",
       " 44300107304.0,\n",
       " 11110851277.0,\n",
       " 719212799243.0,\n",
       " 819597011685.0,\n",
       " 27000380406.0,\n",
       " 819215020754.0,\n",
       " 11150026000.0,\n",
       " 72220900491.0,\n",
       " 43000077412.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(A_train) # list\n",
    "A_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22045063666.0,\n",
       " 72830006019.0,\n",
       " 20735420041.0,\n",
       " 795404134047.0,\n",
       " 70074630601.0,\n",
       " 41224705562.0,\n",
       " 78000113464.0,\n",
       " 41757025434.0,\n",
       " 767778001027.0,\n",
       " 620133001707.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(A_test) # list\n",
    "A_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beverages',\n",
       " 'Grocery',\n",
       " 'Grocery',\n",
       " 'Grocery',\n",
       " 'Grocery',\n",
       " 'Grocery',\n",
       " 'Beverages',\n",
       " 'Grocery',\n",
       " 'Grocery',\n",
       " 'Bakery / Deli']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(B_train) # list\n",
    "B_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Syndigo Mapping MachineLearning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
