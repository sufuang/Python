{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions for Python, Hive, PySpark, Hive and Excel\n",
    "## Introduction\n",
    "The functions are generic and handy. You might use some of them for ad-hoc purpose or facilitating/speeding up your works.\n",
    "## Common Functions\n",
    "1. Date/Time related functions\n",
    " - f_get_hh_mm_ss : Convert total seconds into hours, minutes, and seconds\n",
    " - elapse_time : Get the elapse_time and invoke the function f_get_hh_mm_ss to convert total seconds into hours, minutes, and seconds\n",
    " - f_dte_for_yymm : Base on yymm to return last of yymm, previous month, next month, etc.\n",
    " - f_get_mth_diff : get month difference between starting month and ending \n",
    " \n",
    "2. Database related functions\n",
    " - f_drop_tb : Drop a table\n",
    " - f_alt_tb : Rename a table\n",
    " - f_drop_idxmm : Drop an index\n",
    " - f_rename_tbl_idx : Rename a table or an index\n",
    " - f_cr_idx : Invoke function 'f_drop_idx' to drop an index and create an index\n",
    " - f_get_tbl_cnt : Get table count with/without condition\n",
    " - f_cr_tbl_selas : Use 'select as' to create a target table from a source table with/without condition\n",
    " - f_chk_tbl_exist : Check if table exists\n",
    " - f_get_tbl_diff : Get the count and Spark dataframe for the difference of two tables\n",
    " - f_cr_tbl_from_csv : Create a table from a csv file\n",
    " - f_cr_dic_from_tbl : Create a dictionary from a table\n",
    " - f_cr_exl_fr_tbl : Create an excel file from a list of table(s)\n",
    " - f_cr_exl_fr_a_tbl : Create an excel file from a table with/without condition\n",
    " - f_union_all : Merge pyspark \n",
    " \n",
    "3. Excel related function\n",
    " - f_adj_col_sheet : Based on the column names and content of columns to adjust the spreadsheet column length\n",
    " \n",
    "4. Miscellaneous functions\n",
    " - f_chk_df_is_empty : Check if Python dataframe is empty\n",
    " - f_rm_extra_space : Replace multiple spaces inside the string with a single space and remove both leading and trailing space\n",
    " - f_union_all : Merge PySpark dataframe row-wise\n",
    " - f_cr_dic_from_csv : Create a dictionary from a csv file\n",
    " - f_str_com_word : Return a string of common words between two strings\n",
    " - f_str_dif_wrd : Return a string of words in first string and not in the second string\n",
    " \n",
    "## Notes\n",
    "Before invoking the function(s), please import packages and libraries in the very beginning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import udf, struct, coalesce, col, concat\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row,  DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import datetime              \n",
    "#import csv\n",
    "import gzip, re\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import calendar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Date/Time related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_hh_mm_ss(tot_second):\n",
    "    def f_get_hh_mm_ss(tot_second): \n",
    "    \"\"\"\n",
    "      module name : f_get_hh_mm_ss\n",
    "      purpose     : Convert total seconds into hours, minutes, and seconds \n",
    "      parameter\n",
    "         tot_second   : total seconds; any numeric value, integer or floating\n",
    "      note            : Will convert the floating number to integer and drop the values after the decimal point\n",
    "      Example         : f_get_hh_mm_ss(3618.52)  # return (1, 0, 18)which is 1 hour and 18 seconds      \n",
    "    \"\"\"   \n",
    "    tot_second = int(tot_second)\n",
    "    min  = 60\n",
    "    hour = 60 * 60\n",
    "    day  = 60 * 60 * 24     \n",
    "    hh =  tot_second // hour\n",
    "    mm = (tot_second - (hh * hour)) // min\n",
    "    ss =  tot_second  - ((hh * hour) + (mm * min))\n",
    "    return  hh, mm, ss\n",
    "\n",
    "def elapse_time (start_time, end_time, dsc):\n",
    "    \"\"\"\n",
    "     module name  : elapse_time\n",
    "     purpose      : Get elapse time and print it as hh:mm:yy \n",
    "     parameter    : \n",
    "       start_time : Start time in second;  any numeric value, integer or floating e.g. time.time()\n",
    "       end_time   : End Time in second  ;  any numeric value; integer or floating e.g. time.time()\n",
    "       desc       : Description \n",
    "     notes        :\n",
    "                   - The calling function is required to \"import time\n",
    "                   - Will convert the start_time and end_time to integers\n",
    "                   - will call function \"f_get_hh_mm_ss\" to convert the elapse_time to hours, minutes, and seconds\n",
    "                   - Will call strftime function from time module to display the timestamp for start time and end time \n",
    "     example1    :\n",
    "                   elapse_time(0, 2400.17, \"Test completed.\")\n",
    "                   - The result would be\n",
    "                     Test completed. It took 3701.170000 seconds - 1hh:1mm:41ss.\n",
    "                     start time: Dec 31 1969 17:00:00  end time:  Dec 31 1969 18:01:41\n",
    "     example2   :              \n",
    "                   cty_start_time = time.time()  # type : floating\n",
    "                   time.sleep(5)\n",
    "                   cty_end_time = time.time()\n",
    "                   elapse_time (  cty_start_time, cty_end_time, \"Test completed.\")\n",
    "                   - The result would be\n",
    "                     Test completed. It took 5.001574 seconds - 0hh:0mm:5ss.\n",
    "                     start time: Jan 19 2019 20:20:14  end time:  Jan 19 2019 20:20:19                 \n",
    "    \"\"\"    \n",
    "    elapsed =  end_time -  start_time\n",
    "    hh, mm, ss = f_get_hh_mm_ss(elapsed)\n",
    "    print (\" %s It took %3f seconds - %uhh:%umm:%uss.\" %(dsc,elapsed, hh, mm, ss)) \n",
    "    print (\" start time:\", time.strftime(\"%b %d %Y %H:%M:%S\", time.localtime(start_time)), \" end time: \",  time.strftime(\"%b %d %Y %H:%M:%S\", time.localtime(end_time)))\n",
    "\n",
    "def f_dte_for_yymm(yymm, dte_typ):\n",
    "    \"\"\"\n",
    "      module name : f_dte_for_yymm  \n",
    "      purpose     : Base on yymm and dte_typ to return the date information, e.g. previous month, the first of the month  \n",
    "      Parameter   :\n",
    "          yymm - String or an integer to contain yymm\n",
    "                 - yy is the year\n",
    "                 - mm is the month\n",
    "          dte_typ - type to determine date information to return (not case sensitive)\n",
    "            L/l   : Return the last day of yymm\n",
    "            P/p   : Return previous month of yymm\n",
    "            N/n   : Return next month of yymm\n",
    "            F1/f1 : Return the first day of yymm with the format ccyy-mm-dd\n",
    "            F2/f2 : Return the first day of yymm with the format mmddccyy\n",
    "            D/d   : Return date object \n",
    "      Note: \n",
    "        Function Period is required to 'import pandas as pd'\n",
    "        datetime.date and datetime.date.strftime are required to 'import datetime'\n",
    "      Examples\n",
    "        f_dte_for_yymm(1901, 'l')     # Last date of 2019, january    (Return str  '2019-01-31')\n",
    "        f_dte_for_yymm(1901, 'p')     # Last date of 2019, january    (Return str  '1812')\n",
    "        f_dte_for_yymm(1812, 'n')     # Next month of 2018, December  (Return str  '1901')\n",
    "        f_dte_for_yymm(1901, 'd')     # datetime.date(2019, 2, 1)     (Return datetime.date object with value 2019-02-01)\n",
    "        f_dte_for_yymm(1902, 'f1')    # First day of the month with the format ccyy-mm-dd (Return '2019-02-01')\n",
    "        f_dte_for_yymm(1902, 'f2')    # First day of the month with the format mmddccyy   (Return '02012019')\n",
    "    \"\"\"    \n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    yymm = str(yymm)   # in case yymm was provided as an integer    \n",
    "    ccyy = '20' + yymm [:2]\n",
    "    mm = yymm[2:4]\n",
    "\n",
    "    if dte_typ.lower() == 'l':      \n",
    "       if mm == '12' :\n",
    "         return ccyy + '-' + mm + '-' + '31'\n",
    "       else:\n",
    "          return  (pd.Period(datetime.date(int(ccyy), int(mm)+1, 1), 'D') - 1).strftime(\"%C%y-%m-%d\")    \n",
    "    elif dte_typ.lower() == 'f1':\n",
    "       return datetime.date(int(ccyy), int(mm), 1).strftime(\"%C%y-%m-%d\")\n",
    "    elif dte_typ.lower() == 'f2':\n",
    "       return datetime.date(int(ccyy), int(mm), 1).strftime(\"%m%d%C%y\")                          \n",
    "    elif dte_typ.lower() == 'p': \n",
    "       return  (pd.Period(datetime.date( int(ccyy), int(mm), 1), 'M') - 1).strftime('%y%m')  \n",
    "    elif dte_typ.lower() == 'n': \n",
    "       return  (pd.Period(datetime.date( int(ccyy), int(mm), 1), 'M') + 1).strftime('%y%m')\n",
    "    elif dte_typ.lower() == 'd':    \n",
    "       return datetime.date(int(ccyy), int(mm), 1)\n",
    "    else:\n",
    "       print (\"The dte_typ is \", dte_typ, \"Valid dte_typ for f_dte_for_yymm should be L/l, P/p, N/n, F1/f1, F2/f2, D/d. \")\n",
    "       sys.exit(1)\n",
    "\n",
    "def  f_get_mth_diff(s_yymm, e_yymm):\n",
    "     \"\"\"\n",
    "       module name : f_get_mth_diff\n",
    "       purpose     : Get month difference between starting month and ending month\n",
    "                     -  Will return the month difference between s_yymm and e_yymm as an integer\n",
    "       parameter   :\n",
    "         s_yymm    : Starting month with yymm  format where yy is the year and mm is the month  \n",
    "                     - It can be numeric or string.\n",
    "         e_yymm    : Ending month with yymm format where yy is the year and mm is the month  \n",
    "                    - It can be numeric or string.       \n",
    "       Notes       : if starting month > ending month, the  month difference would be negative                   \n",
    "       example1    : f_get_mth_diff(1811, 1904)\n",
    "                     - Starting month is 2018, November\n",
    "                     - Ending   month is 2019, Aprial\n",
    "                     - return 5 (% months difference)\n",
    "       example2    : f_get_mth_diff('1811', 1904)\n",
    "                     - return 5 \n",
    "    \"\"\"    \n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    s_yymm = str(s_yymm)   # in case s_yymm was provided as an integer    \n",
    "    s_ccyy = '20' + s_yymm [:2]\n",
    "    s_mm   = s_yymm[2:4]\n",
    "     \n",
    "    e_yymm = str(e_yymm)   # in case e_yymm was provided as an integer    \n",
    "    e_ccyy = '20' + e_yymm [:2]\n",
    "    e_mm   = e_yymm[2:4]\n",
    "     \n",
    "    s_dte = datetime.date( int(s_ccyy), int(s_mm), 1)  #datetime.date object\n",
    "    e_dte = datetime.date( int(e_ccyy), int(e_mm), 1)  #datetime.date object\n",
    "     \n",
    "    return  pd.Period(e_dte,'M') - pd.Period(s_dte, 'M') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def f_drop_tb(tbl_nm):\n",
    "    \"\"\"\n",
    "      module name : f_drop_tb\n",
    "      purpose     : Drop a table\n",
    "      parameter   :\n",
    "        tbl_nm    : Table name \n",
    "      example1    : f_drop_tb(\"lookup.post_code\") # drop table lookup.post_code\n",
    "      example2    : tbl_nm = \"lookup.post_code\"  \n",
    "                    f_drop_tb(tbl_nm)             # drop table lookup.post_code   \n",
    "    \"\"\"     \n",
    "    q_drop = \"drop table if exists {0}\".format(tbl_nm)\n",
    "    sqlContext.sql(q_drop)\n",
    "    print(q_drop)\n",
    "\n",
    "def f_alt_tb(src_tbl_nm, trg_tbl_nm):\n",
    "    \"\"\"\n",
    "      module name : f_alt_tb\n",
    "      purpose     : Rename a table \n",
    "      parameter   :\n",
    "       src_tbl_nm : Old table name\n",
    "       trg_tbl_nm : New table name\n",
    "      example1    : f_alt_tb(\"lookup.post_code\", \"lookup.post_code_bkup \") # Change table name from lookup.post_code to lookup.post_code_bkup\n",
    "      example2    : tbl_nm_old = \"lookup.post_code\"\n",
    "                    tbl_nm_new = \"lookup.post_code_bkup\"\n",
    "                    f_alt_tb(tbl_nm_old, tbl_nm_new)             \n",
    "    \"\"\"      \n",
    "    q_alt = \"alter table {0} rename to {1}\".format(src_tbl_nm, trg_tbl_nm)\n",
    "    sqlContext.sql(q_alt)\n",
    "    print(q_alt)\n",
    "\n",
    "def f_drop_idx(tbl_nm, idx_nm) :\n",
    "    \"\"\"\n",
    "      module name : f_drop_idx\n",
    "      purpose     : Drop an index on a table\n",
    "      parameter   :\n",
    "       tbl_nm     : Table name which the index had been created\n",
    "       idx_nm     : Index name to be dropped             \n",
    "    \"\"\"    \n",
    "    q_drp_idx = 'DROP INDEX IF EXISTS {0} on {1}'.format(idx_nm,tbl_nm )\n",
    "    sqlContext.sql(q_drp_idx)\n",
    "    print(q_drp_idx)\n",
    "\n",
    "def f_rename_tbl_idx(obj_nm_old, obj_nm_new, obj_typ) :\n",
    "    \"\"\"\n",
    "      module name : f_rename_tbl_idx\n",
    "      purpose     : Rename a table or an index \n",
    "      parameter   :\n",
    "       obj_nm_old : Old table/index name\n",
    "       obj_nm_new : New table/index name\n",
    "       obj_type   : Object type  (not case sensitive)\n",
    "         \"ind\"    : Will rename object \"index\"\n",
    "                    Otherwise, rename object \"table\"              \n",
    "    \"\"\"        \n",
    "    if obj_typ.lower() == 'ind':\n",
    "       _obj_typ = 'index'\n",
    "    else: _obj_typ= 'table'\n",
    "    q_rename = \"alter {0} {1} rename to {2}\".format(_obj_typ, obj_nm_old, obj_nm_new )  \n",
    "    sqlContext.sql(q_rename)\n",
    "    print(q_rename)\n",
    "\n",
    "def f_cr_idx(tbl_nm, idx_nm, key_var, idx_typ) :\n",
    "    \"\"\"\n",
    "      module name : f_cr_idx\n",
    "      purpose     : Invoke function 'f_drop_idx' to drop an index and create an index \n",
    "      parameter   :\n",
    "       tbl_nm     : table name which index had been created or would be created on\n",
    "      idx_nm     : Index name\n",
    "       key_var    : Create the index on the colunms defined in key_var\n",
    "       idx_typ    : Define index.handler.class.name  (not case sensitive)\n",
    "         'bmp'    : The index.handler.class.name would be 'BITMAP'  (\n",
    "         Otherwise: The index.handler.class.name would be \"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler\"\n",
    "       example\n",
    "                   bus_ctc_tbl = \"pcr_cor.cor_bus_ctc_1812\"\n",
    "                   idx_nm = \"bus_ctc_idx_cmp_1812\" \n",
    "                   f_cr_idx(bus_ctc_tbl, idx_nm, \"(abbr_nm, se10)\", \"cmp\")\n",
    "                    - will create the table pcr_cor__cor_bus_ctc_1812_bus_ctc_idx_cmp_1812__ for index\n",
    "                     - on column abbr_nm and se10\n",
    "                    - The index handler is   \"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler\"                    \n",
    "    \"\"\"     \n",
    "    f_drop_idx(tbl_nm, idx_nm)\n",
    "    if idx_typ.lower() == 'bmp':  as_stmt = 'BITMAP'  (used for columns with few distinct values)\n",
    "    else: as_stmt = 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'   \n",
    "\n",
    "    # Create an empty index \n",
    "    q_cr_idx = \"\"\" CREATE INDEX {0}\n",
    "    ON TABLE {1}  {2}\n",
    "    AS \"{3}\"\n",
    "    WITH DEFERRED REBUILD\"\"\".format(idx_nm, tbl_nm, key_var, as_stmt)\n",
    "    sqlContext.sql(q_cr_idx)\n",
    "    print(q_cr_idx)\n",
    "    \n",
    "    #Build index structure and load data to index table\n",
    "    q_alt_idx = \"ALTER INDEX {0}   on {1}   REBUILD\".format( idx_nm, tbl_nm)\n",
    "    sqlContext.sql(q_alt_idx)\n",
    "    print(q_alt_idx)\n",
    "\n",
    "def f_get_tbl_cnt(tbl_nm, cond_stmt) :\n",
    "    \"\"\"\n",
    "      module name : f_get_tbl_cnt(\n",
    "      purpose     : Get table count with/without condition \n",
    "      parameter   :\n",
    "       tbl_nm     : Table name\n",
    "       cond_stmt  : where/Condition statement\n",
    "      return      :\n",
    "       cnt        : Count of the table with/without condition\n",
    "    \"\"\"\n",
    "    q_get_cnt = 'select count(*) as cnt from {0} {1} '.format(tbl_nm, cond_stmt)\n",
    "    cnt = sqlContext.sql(q_get_cnt).collect()[0][\"cnt\"]\n",
    "    print(\"count = \", cnt, ' for ', q_get_cnt )\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def f_cr_tbl_selas(trg_tbl, src_tbl, cond_stmt):\n",
    "    \"\"\"\n",
    "      module name : f_cr_tbl_selas\n",
    "      purpose     : Use select as to create a target table from a source table \n",
    "      parameter   :\n",
    "        trg_tbl   : Table name to be created\n",
    "        src_tbl   : Source table to get data\n",
    "        cond_stmt : Where/Condition statement to create a table\n",
    "      Notes       : will invoke f_drop_tb to drop a table and invoke f_get_tbl_cnt to get count\n",
    "    \"\"\"         \n",
    "    f_drop_tb(trg_tbl)\n",
    "    q_cr_tbl = \"create table {0} as select * from {1} {2}\".format(trg_tbl, src_tbl, cond_stmt)  \n",
    "    sqlContext.sql(q_cr_tbl)\n",
    "    \n",
    "    f_get_tbl_cnt(trg_tbl, \"\") \n",
    "    print(\" Create table \", trg_tbl,  \" from \", src_tbl, cond_stmt ) \n",
    "\n",
    "def f_chk_tbl_exist (sch_tbl_nm):\n",
    "    \"\"\"\n",
    "      module name : f_chk_tbl_exist \n",
    "      purpose     : Check if table exists. \n",
    "      parameter   :\n",
    "      sch_tbl_nm  : Schema name and table name \n",
    "    \n",
    "      notes       : \n",
    "          - The function will return 'True' if the table exists. Otherwise, return 'False'\n",
    "          - Hive only accepts lowercase schema name and table name\n",
    "            - The function will convert the sch_tbl_nm to lowercase\n",
    "      Example     : f_chk_tbl_exist( \"cstonedb3.gms_merchant_char\")\n",
    "        sch_nm =  'cstonedb3'   \n",
    "        tbl_nm =  'gms_merchant_char'  \n",
    "    \"\"\"    \n",
    "    sch_tbl_nm = sch_tbl_nm.lower()\n",
    "    sch_nm = sch_tbl_nm.split(\".\")[0]   \n",
    "    tbl_nm = sch_tbl_nm.split(\".\")[1] \n",
    "    if tbl_nm  in sqlContext.tableNames(sch_nm):\n",
    "       return True\n",
    "    else: return False  \n",
    "def f_get_tbl_diff(tbl_nm_1, tbl_nm_2, var_nm, cond_stmt, diff_tbl_nm):       \n",
    "    \"\"\"\n",
    "      module name  : f_get_tbl_diff\n",
    "      purpose      : Get the count and spark dataframe for the difference of two tables\n",
    "                     - save the data extracted from tbl_nm_1 to spark dataframe sdf_tbl_1\n",
    "                     - save the data extracted from tbl_nm_2 to spark dataframe sdf_tbl_2\n",
    "                     - Use subtract function to get the difference between sdf_tbl_1 and sdf_tbl_2 \n",
    "      parameter    :\n",
    "       tbl_nm_1    : Table name 1\n",
    "       tbl_nm_2    : Table name 2\n",
    "       var_nm      : string of variable name(s) to compare the difference\n",
    "                     - use '*' if no specific variable to compare    \n",
    "       cond_stmt   : Condition statement including group by, sort by\n",
    "       diff_tbl_nm : Table name to save the differences. Schema name is part of table name  \n",
    "                     - if diff_tbl_nm = '' will not save the difference to a table\n",
    "      return       :\n",
    "       diff_cnt    : Count of difference\n",
    "       sdf_diff    : PySpark data frame to save the difference\n",
    "       \n",
    "      note         : \n",
    "          The function will return the count of difference and the spark dataframe to save the difference\n",
    "          \n",
    "    \"\"\"    \n",
    "    diff_tbl_nm.strip()   # Removes both leading and trailing characters\n",
    "    fnc_nm = 'f_chk_tbl_dif'\n",
    "    tbl_not_exist = 0\n",
    "    if len(diff_tbl_nm.split(\".\")) < 2 and  diff_tbl_nm is not '':\n",
    "       print (\"MSG from \" + fnc_nm + \": \" + diff_tbl_nm + \" is incorrect. It might miss the schema name.\" ) \n",
    "       tbl_not_exist = 1\n",
    "    \n",
    "    if not f_chk_tbl_exist (tbl_nm_1):\n",
    "       print (\"MSG from \" + fnc_nm + \": \" + tbl_nm_1, \" does not exist \") \n",
    "       tbl_not_exist = 1\n",
    "\n",
    "    if not f_chk_tbl_exist (tbl_nm_2):\n",
    "       print (\"MSG from \" + fnc_nm + \": \" + tbl_nm_2, \" does not exist \") \n",
    "       tbl_not_exist = 1  \n",
    "        \n",
    "    if  tbl_not_exist == 1:\n",
    "        print (\"ERR from \" + fnc_nm + \": At least  one of the input tables doesn't exist or the output table name is incorrect. \")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    q_sdf_tbl_1 = \"select {0} from {1} {2}\".format(var_nm, tbl_nm_1, cond_stmt)\n",
    "    print(\" q_sdf_tbl_1\",  q_sdf_tbl_1)\n",
    "    sdf_tbl_1   = sqlContext.sql(q_sdf_tbl_1)\n",
    "    \n",
    "    \n",
    "    q_sdf_tbl_2 = \"select {0} from {1} {2}\".format(var_nm, tbl_nm_2, cond_stmt)\n",
    "    sdf_tbl_2   = sqlContext.sql(q_sdf_tbl_2)   \n",
    "    \n",
    "    print(\" q_sdf_tbl_2\",  q_sdf_tbl_2)\n",
    "    \n",
    "    sdf_diff = sdf_tbl_1.subtract(sdf_tbl_2)  # Get the difference between sdf_tbl_1 and sdf_tbl_2\n",
    "    diff_cnt =  sdf_diff.count()\n",
    "\n",
    "    # If diff_tbl_nm has value, save the difference to a table\n",
    "    if diff_cnt > 0 and diff_tbl_nm is not '':\n",
    "       sdf_diff.registerTempTable(\"tmp_diff_tbl\")\n",
    "      f_cr_tbl_selas (diff_tbl_nm, 'tmp_diff_tbl', '' )\n",
    "    return diff_cnt, sdf_diff    \n",
    "\n",
    "\n",
    "def f_cr_tbl_from_csv(csv_file, tbl_nm, databricks_csv):\n",
    "    \"\"\"\n",
    "      module name  : f_cr_tbl_from_csv\n",
    "      purpose      : Create a table from a csv file \n",
    "      parameter    :\n",
    "       csv_file    : CSV name including path name \n",
    "       tbl_nm      : Table name including schema name\n",
    "       databricks_csv: Indicator\n",
    "          True     : will use \" com.databricks.spark.csv\" to save the csv file to a Spark dataframe\n",
    "         Otherwise: Use panda  \n",
    "        \n",
    "      example      :\n",
    "      \n",
    "      notes        : - Null\n",
    "                       -  com.databricks.spark.csv will save 'NULL' as 'NULL'\n",
    "                       -  If save CSV file to a panda dataframe and save it into table will save 'NULL' as 'NaN'\n",
    "                       -  databricks_csv = 'True' is the suggested way to create a table\n",
    "                     - ClassNotFoundException: Failed to find data source: com.databricks.spark.csv. \n",
    "                       - If submit the calling program with 'True'for databricks_csv will get ClassNotFoundException error message and job will stop\n",
    "                         â€œsh /axp/platform/cloak/bin/spark/cloak-sparksubmit calling_code\"\n",
    "                       -  Avoid the error message\n",
    "                          - Define sphjar environmental variable\n",
    "spkjar=\"--master yarn-client --jars /axp/platform/cloak/lib/cloak-spark-1.2.0-SNAPSHOT.jar,/axp/platform/mlplat/app/lib/thirdpartyjars/spark-libs/commons-csv-1.4.jar,/axp/platform/mlplat/app/lib/thirdpartyjars/spark-libs/spark-csv_2.10-1.5.0.jar\" \n",
    "export HADOOP_CLASSPATH=/axp/platform/cloak/lib/cloak-hive-1.2.0-SNAPSHOT.jar; \n",
    "                          - Use \"spark-submit $spkjar calling_code\" to submit your job to create a table from a csv via com.databricks.spark.csv\n",
    "                                                                                                                                                                                           \n",
    "    \"\"\"    \n",
    "    print(\" def csv file\", csv_file, \"tbl_nm \", tbl_nm,   \"databricks_csv \", databricks_csv)  \n",
    "    if databricks_csv == \"True\" :   \n",
    "       sdf_csv = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "           .options(header='true', inferschema='true') \\\n",
    "           .option(\"treatEmptyValuesAsNulls\", \"true\")  \\\n",
    "           .load(os.path.realpath(csv_file))\n",
    "       print(\"use databricks_csv\")\n",
    "    else:\n",
    "       pdf_csv = pd.read_csv(csv_file)\n",
    "       sdf_csv = sqlContext.createDataFrame(pdf_csv)\n",
    "       print(\"not use databricks_csv\")\n",
    "    \n",
    "    sqlContext.registerDataFrameAsTable(sdf_csv, \"sdf_csv_tbl\") # register the dataframe as a temp table\n",
    "    #drop table\n",
    "    f_drop_tb(tbl_nm)                                        \n",
    "    q_cr_tbl = \"create table {0} as select * from sdf_csv_tbl\".format(tbl_nm )\n",
    "    print(\"q_cr_tbl :\", q_cr_tbl)\n",
    "    sqlContext.sql(q_cr_tbl)\n",
    "    f_get_tbl_cnt(tbl_nm, '')\n",
    "    print(\"create table \", tbl_nm ,\" from the csv file :\", csv_file)\n",
    "\n",
    " \n",
    "def  f_cr_dic_from_tbl(tbl_nam, key_nam, val_nam,):    \n",
    "    \"\"\"\n",
    "      module name : f_cr_dic_from_tbl\n",
    "      purpose     : Create/Return a dictionary from a table \n",
    "      parameter   :\n",
    "        ctbl_nam  : Table name with schema\n",
    "        key_nam   : Variable to define key   for a dictionary\n",
    "        val_nam   : Variable to define value for a dictionary\n",
    "      \n",
    "      example     :  \n",
    "        d_bus_rol_grp_dsc =  f_cr_dic_from_tbl('pcr_cor.cor_lup_bus_rule', 'rol_typ', 'rol_typ_dsc')\n",
    "        - Create a dictionary d_bus_rol_grp_dsc from the table pcr_cor.cor_lup_bus_rule with key as rol_typ and value as rol_typ_dsc\n",
    "     \"\"\" \n",
    "     q_pdf = \"select {0}, {1} from {2} group by {0}, {1}\". format( key_nam,  val_nam, tbl_nam)\n",
    "     pdf = sqlContext.sql(q_pdf).toPandas()   # Create a panda dataframe\n",
    "     l_key = pdf[key_nam].tolist()\n",
    "     l_val = pdf[val_nam].tolist()\n",
    "     return dict (zip(l_key, l_val))\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    " module name : f_cr_exl_fr_tbl\n",
    " purpose : Create an excel file from a list of table(s) without condition \n",
    "  parm\n",
    "    excel_file: Excel file with extension as 'xlxs'\n",
    "    l_tbl     : List of table name\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def f_cr_exl_fr_tbl(excel_file, l_tbl):    \n",
    "    \"\"\"\n",
    "      module name : f_cr_exl_fr_tbl\n",
    "      purpose     : Create an excel file from a list of table(s) without condition \n",
    "      parameter   :\n",
    "        excel_file: Excel file with extension as 'xlxs'\n",
    "        l_tbl     : List of table name\n",
    "      notes       :\n",
    "        - Create a spreadsheet with table name as a tab\n",
    "    \"\"\"    \n",
    "    \n",
    "    writer = pd.ExcelWriter(excel_file)\n",
    "    for tbl in l_tbl:\n",
    "        q_pdf = \"select * from {0}\".format(tbl)\n",
    "        print(\"q_pdf\", q_pdf)\n",
    "        sdf = sqlContext.sql(q_pdf)\n",
    "        sdf.head(10)\n",
    "        pdf = sqlContext.sql(q_pdf).toPandas()\n",
    "        pdf.to_excel(writer,tbl, index=False) \n",
    "    writer.save()                   \n",
    "\n",
    "def f_cr_exl_fr_a_tbl(excel_file, tbl_nam, cond_stmt):\n",
    "    \"\"\"\n",
    "      module name : f_cr_exl_fr_a_tbl\n",
    "      purpose     : Create an excel file from a table with/without condition \n",
    "      parameter   :\n",
    "       excel_file : Excel file with extension as 'xlxs'\n",
    "       tbl_nam    : Table name\n",
    "       cond_stmt  : Filter out condition\n",
    "      Note: \n",
    "          The function is cloned from f_cr_exl_fr_tbl which can handle more than one tables without condition\n",
    "    \"\"\"    \n",
    "    writer = pd.ExcelWriter(excel_file)\n",
    "    q_pdf = \"select * from {0} {1} \".format(tbl_nam, cond_stmt)\n",
    "    print(\"q_pdf\", q_pdf)\n",
    "    sdf = sqlContext.sql(q_pdf)\n",
    "    pdf = sqlContext.sql(q_pdf).toPandas()\n",
    "    pdf.to_excel(writer,tbl_nam, index=False) \n",
    "    writer.save()\n",
    "\n",
    "Excel related function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel related function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_adj_col_sheet(pdf_nm, excel_writer,sheet_nm, head_fmt_ind):    \n",
    "    \"\"\"\n",
    "      module name : f_adj_col_sheet\n",
    "      purpose     : Based on the column names and content of columns to adjust the spreadsheet column length\n",
    "                     - Return workbook with adjusted column length \n",
    "      parameters     :\n",
    "        pdf_nm       : panda data frame name for the excel sheet\n",
    "        excel_writer : XlsxWriter Excel object\n",
    "        sheet_nm     : sheet name\n",
    "        head_fmt_ind : indictor to define head format for the workbook  \n",
    "      Note: \n",
    "        The calling program needs to define the xcel_writer with engine='xlsxwriter', e.g.\n",
    "        - writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
    "    \"\"\"    \n",
    "    workbook  = excel_writer.book\n",
    "    worksheet = excel_writer.sheets[sheet_nm]\n",
    "    if ( head_fmt_ind == 'True'):\n",
    "         header_format = workbook.add_format({\n",
    "             'bold': True,\n",
    "             'text_wrap': True,\n",
    "             'valign': 'top',\n",
    "             'bg_color': 'grey', \n",
    "             'border': 1})        \n",
    "                 \n",
    "    _col = 'A'\n",
    "    for col in pdf_nm.columns.values:        \n",
    "         max_col_cnt =  max(pdf_nm[col].apply(repr).apply(len).max(), len(col))\n",
    "         col_rang =  _col + ':' + _col \n",
    "         worksheet.set_column(col_rang , max_col_cnt) + 2\n",
    "         _col = chr(ord(_col) + 1)    \n",
    "    return workbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  f_chk_df_is_empty(df):\n",
    "     \"\"\"\n",
    "      module name : f_chk_df_is_empty(df)\n",
    "      purpose : Check if python dataframe is empty\n",
    "      parameter\n",
    "        df    : Python data frame     \n",
    "      Note: \n",
    "          The function will return True if the length of DF is 0 \n",
    "    \"\"\"     \n",
    "    if len(df) == 0:\n",
    "       return True\n",
    "    else : return False\n",
    " \n",
    "def f_rm_extra_space(in_str):\n",
    "    \"\"\"\n",
    "      module name : f_rm_extra_space\n",
    "      purpose     : Replace multiple spaces inside the string with a single space and remove both leading and trailing spaces\n",
    "      parameter   :\n",
    "        in_str: String\n",
    "      note\n",
    "       -re.sub()  : Search and replace function in re (regular expression) module  \n",
    "      example     : f_rm_extra_space(\"  a b          d  \") # Return 'a b d'\n",
    "    \"\"\"\n",
    "    return re.sub(' +', ' ', in_str).strip()\n",
    " \n",
    "def f_union_all(*dfs):\n",
    "    \"\"\"\n",
    "      module name : f_union_all\n",
    "      purpose     : Merge PySpark dataframe row-wise\n",
    "      parameter   :\n",
    "        *dfs      : Any number of PySpark dataframe, and separated by ',' \n",
    "      note        : Adapted from https://datascience.stackexchange.com/questions/11356/merging-multiple-data-frames-row-wise-in-pyspark\n",
    "      example     : f_union_all(td1, td2, td3, td4) # merge PySpark dataframe td1, td2, td3, and td4    \n",
    "    \"\"\"    \n",
    "    from   functools   import reduce \n",
    "    from   pyspark.sql import DataFrame\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "def  f_cr_dic_from_csv(csv_file, val_nam, key_nam ):\n",
    "    \"\"\"\n",
    "      module name : f_cr_dic_from_csv\n",
    "      purpose     : Create a dictionary from a csv file \n",
    "      parameter   :\n",
    "        csv_file  : include file name and  for the CSV file\n",
    "        val_nam   : variable for value\n",
    "        key_nam   : variable for key \n",
    "     \"\"\"      \n",
    "     pdf_csv = pd.read_csv(csv_file)\n",
    "     l_val = pdf_csv[val_nam]\n",
    "     l_key = pdf_csv[key_nam].tolist()\n",
    "     return  dict (zip(l_key, l_val\n",
    "\n",
    "def f_str_com_word(str1, str2):   \n",
    "    \"\"\"\n",
    "      module name : f_str_com_word\n",
    "      purpose     : Return a string of common words between srt1 and str2\n",
    "      parameter   :\n",
    "        str1      : string with words separated by space(s)        \n",
    "        str2      : string with words separated by space(s)\n",
    "      note:\n",
    "          - words are separate by space(s)\n",
    "          - The sequence of words is not important,  e.g.\n",
    "            -  f_str_com_word('ab cd ef ef', 'ef ab ef')   # return 'ef ab'\n",
    "            -  Will dedupe the repeating word and then do the comparison, e.g.\n",
    "               - f_str_com_word('ab cd ef ef', 'ef ab ef')  # return 'ef ab'\n",
    "          - str1.strip().split() - convert a string to a list       \n",
    "\"\"\"\n",
    "    return ' '.join(list( set(str1.strip().split())  & set(str2.strip().split\n",
    "\n",
    "\n",
    "def f_str_dif_wrd(str1, str2): \n",
    "     \"\"\"\n",
    "      module name : f_str_dif_wrd\n",
    "      purpose     : Return a string of words in str1 and not in str2\n",
    "      parameter   :\n",
    "        str1      : string with words separated by space(s)        \n",
    "        str2      : string with words separated by space(s  \n",
    "      example     :\n",
    "                    - f_str_dif_wrd('ab cd ef ef', 'ef ab     ef')   # 'cd'\n",
    "                    - f_str_dif_wrd('ab cd ef ef', 'ef ab cd ef hj')  # ''\n",
    "                    - f_str_dif_wrd('ab cd ef ef ba', 'ef ab cd ef hj')  # 'ba'\n",
    "      notes       :\n",
    "          - words are separate by space(s)\n",
    "          - The sequence of words is not important\n",
    "          - Will dedupe the repeating word and then do the comparison       \n",
    "          - str1.strip().split() - convert a string to a list       \n",
    "     \"\"\"    \n",
    "        return ' '.join(list( set(str1.strip().split())  - set(str2.strip().split())))                 \n",
    "                                                           \n",
    "Misc\n",
    ",lpad(first_rspbl_cd, 3, \"0\") as first_rspbl_cd\n",
    ",lpad(scnd_rspbl_cd, 3, \"0\") as scnd_rspbl_cd\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
