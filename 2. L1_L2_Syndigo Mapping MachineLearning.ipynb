{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1_L2_Syndigo Mapping MachineLearning\n",
    "  - module name: 2. L1_L2_Syndigo Mapping MachineLearning.ipynb\n",
    "  - Purpose: Combine Level 1 & Level 2 from  Syndigo to build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_1():\n",
    "    pgm = inspect.currentframe().f_code.co_name  \n",
    "    start_time = time.time() \n",
    "    # Reading PIMMART data\n",
    "    pim_gtin_mapped = pd.read_csv(DBFR + \"PIM_Data_New_50_82Mn.csv\", dtype=object)\n",
    "    for i in ['SUBCOM_CD', 'DPT_CD', 'COM_CD','PMY_DPT_CD', 'REC_DPT_CD', 'ITM_ID', 'GTIN']:\n",
    "        pim_gtin_mapped[i] = pim_gtin_mapped[i].astype(np.float64)\n",
    "    \n",
    "    # Reading Syndigo 259K data\n",
    "    synd_ALL = pd.read_csv(DBFR + 'Syndigo_Final_ALL.csv', dtype='unicode') # 259k Syndigo Data\n",
    "    for i in ['SUBCOM_CD', 'DPT_CD', 'COM_CD', 'GTIN', 'ITM_ID', 'PMY_DPT_CD']:\n",
    "        synd_ALL[i] = synd_ALL[i].astype(np.float64)\n",
    "    \n",
    "    # Stripping spaces from all columns\n",
    "    df_obj = synd_ALL.select_dtypes(['object'])\n",
    "    synd_ALL[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    syndigo_mapped = synd_ALL\n",
    "    pimmart = pim_gtin_mapped\n",
    "    syndigo_mapped.drop_duplicates('GTIN', inplace = True)\n",
    "    syndigo_mapped['ITEM_SUBCOM_text'] = \\\n",
    "    (syndigo_mapped.VND_ECOM_DSC + ' ' + syndigo_mapped.SUBCOM_DSC).fillna('').str.lower()\n",
    "    #syndigo_mapped['Level 1'].value_counts()\n",
    "    #print(f'syndigo_mapped.shape {syndigo_mapped.shape}') \n",
    "    #print(f'syndigo_mapped.info() {syndigo_mapped.info()}') \n",
    "    #print(f'syndigo_mapped.head() {syndigo_mapped.head()}') \n",
    "    #print(f'syndigo_mapped.columns {syndigo_mapped.columns}') \n",
    "    \n",
    "    syndigo_mapped['l1_l2'] = syndigo_mapped['Level 1'] + ' + ' + syndigo_mapped['Level 2']\n",
    "    #print(f\"syndigo_mapped['l1_l2'].value_counts() {syndigo_mapped['l1_l2'].value_counts(dropna = False)}\")    \n",
    "    return syndigo_mapped, pim_gtin_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_2(samp_frac = 1):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name \n",
    "    global syndigo_mapped\n",
    "    syndigo_mapped = syndigo_mapped_bkup.copy()\n",
    "    print(f' samp_frac = {samp_frac}') \n",
    "    print (f'Before sample syndigo_mapped.shape {syndigo_mapped.shape }')\n",
    "    \n",
    "    vect = TfidfVectorizer(ngram_range = (1,2), max_features = 50000)\n",
    "    whole_frac = 1\n",
    "    if samp_frac  < whole_frac :   syndigo_mapped = syndigo_mapped.sample(frac = samp_frac,  random_state=42)\n",
    "   \n",
    "    print (f'After sample syndigo_mapped.shape {syndigo_mapped.shape }')\n",
    "  \n",
    "    print (f' syndigo_mapped .shape {syndigo_mapped .shape }')\n",
    "    X = vect.fit_transform(syndigo_mapped.ITEM_SUBCOM_text) #scipy.sparse._csr.csr_matrix\n",
    "    l1_l2_id_map = dict(zip(syndigo_mapped['l1_l2'].fillna('Other').unique(), range(syndigo_mapped['l1_l2'].fillna('Other').nunique())))\n",
    "    id_l1_l2_map = dict(zip(range(syndigo_mapped['l1_l2'].fillna('Other').nunique()), syndigo_mapped['l1_l2'].fillna('Other').unique()))\n",
    "    y  = syndigo_mapped['l1_l2'].fillna('Other').map(l1_l2_id_map)\n",
    "    \n",
    "    end_time = time.time()   \n",
    "    desc = f' Elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)\n",
    "    return  X, y, l1_l2_id_map, id_l1_l2_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X, y):\n",
    "    knn_grid = dict(n_neighbors=range(1, 21, 2),weights=['uniform', 'distance'],metric=['manhattan', 'minkowski'], p =[1, 2])\n",
    "    lr_grid = dict(solver=['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],penalty=['l2'],C=[100, 10, 1.0, 0.1, 0.01], multi_class=['ovr','ovo', 'multinomial'])\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression(), lr_grid))\n",
    "    models.append(('KNN', KNeighborsClassifier(), knn_grid))\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    fit_mdl ={}\n",
    "    for name, model, grid in models:    \n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "        grid_result = grid_search.fit(X, y)\n",
    "        fit_mdl[name] = grid_result\n",
    "        # summarize results\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        means = grid_result.cv_results_['mean_test_score']\n",
    "        stds = grid_result.cv_results_['std_test_score']\n",
    "        params = grid_result.cv_results_['params']\n",
    "        for mean, stdev, param in zip(means, stds, params):\n",
    "            print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_predict(test_size):\n",
    "     start_time = time.time()\n",
    "     pgm = inspect.currentframe().f_code.co_name   \n",
    "     global A_train, A_test, B_train, B_test, X_train, X_test, y_train, y_test\n",
    "     A_train, A_test, B_train, B_test = train_test_split(syndigo_mapped.GTIN.tolist(), syndigo_mapped['l1_l2'].tolist(), test_size=test_size,  random_state=42)\n",
    "     \n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test_size,  random_state=42)\n",
    "     lr_tf = LogisticRegression(C = 100, multi_class = 'multinomial', solver = 'saga', n_jobs=-1)\n",
    "     \n",
    "     print(f\"Training starts for test_size = {test_size} \")\n",
    "     lr_tf.fit(X_train, y_train)\n",
    "     preds = lr_tf.predict(X_test)\n",
    "     #print(f\"type(preds) {type(preds)} len(preds) {len(preds)} \\n preds[0:10] {preds[0:10]}\")\n",
    "     preds_lrtf = preds\n",
    "     probs = lr_tf.predict_proba(X_test)\n",
    "     #print(f\"type(probs) {type(probs)} len(probs) {len(probs)} \\n probs[0:10] {probs[0:10]}\")   \n",
    "     preds_train = lr_tf.predict(X_train)\n",
    "     #print(f\"type(preds_train) {type(preds_train)} len(preds_train) {len(preds_train)} \\n preds_train[0:10] {preds_train[0:10]} \")   \n",
    "     probs_train = lr_tf.predict_proba(X_train)\n",
    "     #print(f\"type(probs_train) {type(probs_train)} len(probs_train) {len(probs_train)} \\n probs_train[0:10] {probs_train[0:10]}\")   \n",
    "     #print(classification_report(y_test, preds_test,labels = lr_tf.classes_, target_names = [id2_level_map[i] for i in lr_tf.classes_]))\n",
    "     \n",
    "    \n",
    "    \n",
    "     #print(\"Accuracy = \",accuracy_score(y_test,preds))\n",
    "     #print(\"Display TFIDF metrics\")\n",
    "     #print(classification_report(y_test, preds,labels = lr_tf.classes_, target_names = [id_l1_l2_map[i] for i in lr_tf.classes_]))\n",
    "    \n",
    "     print(f\"Done Training \")\n",
    "     end_time = time.time()   \n",
    "     desc = f' Elapse_time for \"{pgm}\".' \n",
    "     elapse_time (  start_time, end_time, desc)\n",
    "     return preds, preds_lrtf, probs, preds_train, probs_train, lr_tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(sample_frec, test_size):\n",
    "     start_time = time.time()\n",
    "     pgm = inspect.currentframe().f_code.co_name     \n",
    "     print(f'Validation for sample_frec = {sample_frec} and  test_size = {test_size}' ) \n",
    "     print(\"Accuracy = \",accuracy_score(y_test,preds))\n",
    "     print (f' f1_score_weighted = {f1_score(y_test, preds, average=\"weighted\")}')\n",
    "     print (f\" f1_score_macro = {f1_score(y_test, preds, average='macro')}\")\n",
    "     print (f\" f1_score_micro = {f1_score(y_test, preds, average='micro')}\")\n",
    "     # Compute the Multiclass ROC AUC score\n",
    "     #multi_class = 'multinomial'\n",
    "     #score = roc_auc_score(y_test, preds, multi_class= multi_class)\n",
    "     #print(f\"ROC AUC score for Multiclass= {multi_class}: {score:.2f}\")\n",
    "\n",
    "     print(\"Display TFIDF metrics\")\n",
    "     test_metrics = pd.DataFrame(classification_report(y_test, preds,labels = lr_tf.classes_, target_names = [id_l1_l2_map[i] for i in lr_tf.classes_],  output_dict= True)).T\n",
    "     #test_metrics = pd.DataFrame(classification_report(results_test.Actuals, results_test.Predictions, output_dict= True)).T \n",
    "     #print(test_metrics)\n",
    "     #Stest_metrics.to_csv('L1_L2_Test_Metrics_yue.csv')\n",
    "     tab_name = f'L1_L2_Test_Metrics_sample_frec{sample_frec}_test_size{test_size}'\n",
    "     print(f'tab_name = {tab_name}')  \n",
    "    \n",
    "     end_time = time.time()   \n",
    "     desc = f' Elapse_time for \"{pgm}\".' \n",
    "     elapse_time (  start_time, end_time, desc)    \n",
    "     return test_metrics, tab_name \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_append():\n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name     \n",
    "    test_new_proobs = []\n",
    "    \n",
    "    #print(f'len(preds)  {len(preds)}')\n",
    "    for i in range(len(preds)):\n",
    "        test_new_proobs.append(probs[i][np.argsort(probs[i])][::-1][:1].tolist())\n",
    "    test_new_proobs = [element for sublist in list(test_new_proobs) for element in sublist]\n",
    "    #print(f'test_new_proobs {test_new_proobs}')\n",
    "    \n",
    "    train_new_proobs = []\n",
    "    \n",
    "    #print(f'len(preds_train)  {len(preds_train)}')\n",
    "    for i in range(len(preds_train)):\n",
    "         test_new_proobs.append(probs_train[i][np.argsort(probs_train[i])][::-1][:1].tolist())\n",
    "    train_new_proobs = [element for sublist in list(train_new_proobs) for element in sublist]\n",
    "    #print(f'train_new_proobs {train_new_proobs}')  \n",
    "    \n",
    "    # process testLevels & trainLevelss \n",
    "    testLevels = []\n",
    "    for j in y_test:\n",
    "        testLevels.append([i for i in level2_id_map if level2_id_map[i]==j][0])\n",
    "    #print(f'testLevels {testLevels [0:10]}')\n",
    "    testLevelss = []\n",
    "    for j in preds:\n",
    "        testLevelss.append([i for i in level2_id_map if level2_id_map[i]==j][0])\n",
    "    #print(f'testLevelss {testLevelss [0:10]}')\n",
    "    \n",
    "    trainLevelss = []\n",
    "    for j in preds_train:\n",
    "        trainLevelss.append([i for i in level2_id_map if level2_id_map[i]==j][0])\n",
    "     \n",
    "    end_time = time.time()   \n",
    "    desc = f' Elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)    \n",
    "    return  test_new_proobs, train_new_proobs, testLevels, testLevelss, trainLevelss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_save():\n",
    "\n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name \n",
    "    print(\"---------------------------------------------\\FINAL COUNTS:\\n---------------------------------------------\")\n",
    "    print(\"1)\", len(A_test + A_train))\n",
    "    print(\"2)\", len(['Test']*len(A_test) + ['Train']*len(A_train)))\n",
    "    print(\"3)\", len(B_test + B_train))\n",
    "    print(\"4)\", len(testLevels + B_train))\n",
    "    print(\"5)\", len(testLevelss + trainLevelss))\n",
    "    print(\"6)\", int(len(test_new_proobs + train_new_proobs)))\n",
    "    \n",
    "    data = {\n",
    "        'GTIN' : A_test + A_train,\n",
    "        'Source': ['Test']*len(A_test) + ['Train']*len(A_train),\n",
    "        'Actual l1_l2' : B_test + B_train,\n",
    "        'Actuals' : testLevels + B_train,\n",
    "        'Predictions' : testLevelss + trainLevelss,\n",
    "        'Scores' : test_new_proobs + train_new_proobs,\n",
    "    \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.merge(pim_gtin_mapped[['GTIN', 'ITM_ID', 'PMY_DPT_CD', 'PMY_DPT_DSC', 'REC_DPT_CD',\n",
    "        'REC_DPT_DSC', 'DPT_CD', 'DPT_DSC', 'COM_CD', 'COM_DSC', 'SUBCOM_CD',\n",
    "        'SUBCOM_DSC', 'VND_ECOM_DSC']], on='GTIN', how='left')\n",
    "    df = df[['ITM_ID', 'GTIN', 'PMY_DPT_CD', 'PMY_DPT_DSC', 'REC_DPT_CD', 'REC_DPT_DSC', 'DPT_CD',\n",
    "        'DPT_DSC', 'COM_CD', 'COM_DSC', 'SUBCOM_CD', 'SUBCOM_DSC',\n",
    "        'VND_ECOM_DSC', 'Source', 'Actual l1_l2', 'Actuals', 'Predictions', 'Scores']]\n",
    "    filename = 'Syndigo_Mapping_L1_L2_ML.csv'\n",
    "    df.to_csv(DBFO + filename, index=False)\n",
    "    print(f\"{filename}  --file created successfully\")\n",
    "\n",
    "    end_time = time.time()   \n",
    "    desc = f' Elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b64183-34df-46b6-b1c9-6bbe78776fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier,  RidgeClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import re, time, inspect\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format \n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# load  function of 'elapse_time'\n",
    "path_code = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Code\\\\'  \n",
    "f_com_code = path_code + \"com_code.py\"\n",
    "exec(compile(open(f_com_code , \"rb\").read(), f_com_code, 'exec' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1a5c73-5356-4b00-89a8-c87c775a321a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBFS = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFO = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFM = \"/dbfs/FileStore/tables/MALLIK/\"\n",
    "DBFR = \"/dbfs/FileStore/tables/OFFSHORE_RESULTS/\"\n",
    "path = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Data\\\\'   \n",
    "DBFS = path\n",
    "DBFO = path\n",
    "DBFM = path\n",
    "DBFR = path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "syndigo_mapped, pim_gtin_mapped = prepare_data_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "syndigo_mapped_bkup = syndigo_mapped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " samp_frac = 0.3\n",
      "Before sample syndigo_mapped.shape (259085, 25)\n",
      "After sample syndigo_mapped.shape (77726, 25)\n",
      " syndigo_mapped .shape (77726, 25)\n",
      "  Elapse_time for \"prepare_data_2\". It took 2.367963 seconds - 0hh:0mm:2ss.\n",
      " start time: Aug 25 2023 13:22:08  end time:  Aug 25 2023 13:22:10\n"
     ]
    }
   ],
   "source": [
    "X, y, l1_l2_id_map, id_l1_l2_map = prepare_data_2(samp_frac = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " samp_frac = 0.3\n",
      "Before sample syndigo_mapped.shape (259085, 25)\n",
      "After sample syndigo_mapped.shape (77726, 25)\n",
      " syndigo_mapped .shape (77726, 25)\n",
      "  Elapse_time for \"prepare_data_2\". It took 2.861276 seconds - 0hh:0mm:2ss.\n",
      " start time: Aug 23 2023 01:31:01  end time:  Aug 23 2023 01:31:04\n",
      "Training starts for test_size = 0.3 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 111.158615 seconds - 0hh:1mm:51ss.\n",
      " start time: Aug 23 2023 01:31:04  end time:  Aug 23 2023 01:32:55\n",
      "Validation for sample_frec = 0.3 and  test_size = 0.3\n",
      "Accuracy =  0.8790633845098207\n",
      " f1_score_weighted = 0.8742430204178655\n",
      " f1_score_macro = 0.46950650637609476\n",
      " f1_score_micro = 0.8790633845098208\n",
      "Display TFIDF metrics\n",
      "tab_name = L1_L2_Test_Metrics_sample_frec0.3_test_size0.3\n",
      "  Elapse_time for \"validation\". It took 0.095665 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 23 2023 01:32:55  end time:  Aug 23 2023 01:32:56\n"
     ]
    }
   ],
   "source": [
    "X, y, l1_l2_id_map, id_l1_l2_map = prepare_data_2(samp_frac = 0.3)\n",
    "preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.3)\n",
    "test_metric_1, tab_1 =  validation(sample_frec = 0.3, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts for test_size = 0.1 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 154.818257 seconds - 0hh:2mm:34ss.\n",
      " start time: Aug 23 2023 01:32:56  end time:  Aug 23 2023 01:35:30\n",
      "Validation for sample_frec = 0.3 and  test_size = 0.1\n",
      "Accuracy =  0.8815129293708993\n",
      " f1_score_weighted = 0.8772497941434809\n",
      " f1_score_macro = 0.541360092896292\n",
      " f1_score_micro = 0.8815129293708993\n",
      "Display TFIDF metrics\n",
      "tab_name = L1_L2_Test_Metrics_sample_frec0.3_test_size0.1\n",
      "  Elapse_time for \"validation\". It took 0.057141 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 23 2023 01:35:30  end time:  Aug 23 2023 01:35:30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.1)\n",
    "test_metric_2, tab_2 =  validation(sample_frec = 0.3, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndigo_mapped = syndigo_mapped_bkup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " samp_frac = 1\n",
      "Before sample syndigo_mapped.shape (259085, 25)\n",
      "After sample syndigo_mapped.shape (259085, 25)\n",
      " syndigo_mapped .shape (259085, 25)\n",
      "  Elapse_time for \"prepare_data_2\". It took 9.952945 seconds - 0hh:0mm:9ss.\n",
      " start time: Aug 23 2023 01:50:22  end time:  Aug 23 2023 01:50:32\n",
      "Training starts for test_size = 0.3 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 494.629706 seconds - 0hh:8mm:14ss.\n",
      " start time: Aug 23 2023 01:50:32  end time:  Aug 23 2023 01:58:46\n",
      "Validation for sample_frec = 1 and  test_size = 0.3\n",
      "Accuracy =  0.8902426472480252\n",
      " f1_score_weighted = 0.8880122013849431\n",
      " f1_score_macro = 0.45908703124835426\n",
      " f1_score_micro = 0.8902426472480252\n",
      "Display TFIDF metrics\n",
      "tab_name = L1_L2_Test_Metrics_sample_frec1_test_size0.3\n",
      "  Elapse_time for \"validation\". It took 0.371622 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 23 2023 01:58:46  end time:  Aug 23 2023 01:58:47\n"
     ]
    }
   ],
   "source": [
    "X, y, l1_l2_id_map, id_l1_l2_map = prepare_data_2(samp_frac = 1)\n",
    "preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.3)\n",
    "test_metric_3, tab_3 = validation(sample_frec = 1, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts for test_size = 0.1 \n",
      "Done Training \n",
      "  Elapse_time for \"proc_predict\". It took 623.698597 seconds - 0hh:10mm:23ss.\n",
      " start time: Aug 23 2023 02:10:27  end time:  Aug 23 2023 02:20:51\n",
      "Validation for sample_frec = 1 and  test_size = 0.1\n",
      "Accuracy =  0.8931645374194296\n",
      " f1_score_weighted = 0.8907344356162481\n",
      " f1_score_macro = 0.5624715577166559\n",
      " f1_score_micro = 0.8931645374194297\n",
      "Display TFIDF metrics\n",
      "tab_name = L1_L2_Test_Metrics_sample_frec1_test_size0.1\n",
      "  Elapse_time for \"validation\". It took 0.320256 seconds - 0hh:0mm:0ss.\n",
      " start time: Aug 23 2023 02:20:51  end time:  Aug 23 2023 02:20:52\n"
     ]
    }
   ],
   "source": [
    "preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.1)\n",
    "test_metric_4, tab_4 = validation(sample_frec = 1, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iny2819\\AppData\\Local\\Temp\\ipykernel_24040\\2099386414.py:16: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "#  Write classification report to excel\n",
    "#  save() is going to remove, use close() instead\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "excel_file = DBFO + 'L1_L2_Test_Metrics_cls_rpt.xlsx'\n",
    "\n",
    "writer = pd.ExcelWriter(excel_file) \n",
    "tab_1 = 'L1_L2_TM_smp_frec0.3_tst_siz0.3'\n",
    "tab_2 = 'L1_L2_TM_smp_frec0.3_tst_siz0.1'\n",
    "tab_3 = 'L1_L2_TM_smp_frec1_tst_siz0.3'\n",
    "tab_4 = 'L1_L2_TM_smp_frec1_tst_siz0.1'\n",
    "\n",
    "test_metric_1.to_excel(writer,tab_1)\n",
    "test_metric_2.to_excel(writer,tab_2)\n",
    "test_metric_3.to_excel(writer,tab_3)\n",
    "test_metric_4.to_excel(writer,tab_4)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metric_1, tab_1 =  validation(sample_frec = 0.3, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metric_2, tab_2 =  validation(sample_frec = 0.3, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic_test={\"Hospitality Supplies\":\"hospitality_spplies\", \"Tobacco Products\":\"bacco_products\"}\n",
    "#dic_test={\"Food / Beverages\":\"food_beverages\"}\n",
    "\n",
    "\n",
    "    X_subset, y_subset, subset_df, level2_id_map =  prepare_data_2(level__1)\n",
    "    preds, preds_lrtf, probs, preds_train, probs_train = proc_predict(level__1)\n",
    "    test_new_proobs, train_new_proobs, testLevels, testLevelss, trainLevelss = proc_append()\n",
    "    proc_save(level__1, filenamee) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndigo_mapped[['Level 1', 'Level 2']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndigo_mapped_bkup = syndigo_mapped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndigo_mapped['l1_l2'] = syndigo_mapped['Level 1'] + ' + ' + syndigo_mapped['Level 2']\n",
    "syndigo_mapped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1_l2 = pd.DataFrame(syndigo_mapped[['Level 1', 'Level 2']].value_counts(dropna = False).reset_index())\n",
    "df_l1_l2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndigo_mapped['l1_l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndigo_mapped['l1_l2'].value_counts(dropna =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marine + Heating, Ventilation, & Air Conditioning                                   9\n",
       "Hardware + Safety & Security                                                        9\n",
       "Electrical + Conduit, Raceways, Fittings & Accessories                              9\n",
       "Tools + Benchtop & Stationary Tools                                                 8\n",
       "Electronics + Optical Equipment                                                     8\n",
       "Home & Venue Decoration + Wallpaper & Accessories                                   8\n",
       "Sports & Outdoor Recreation Equipment + Basketball                                  8\n",
       "Sports & Outdoor Recreation Equipment + Disc Golf, Ultimate Frisbee & Boomerangs    8\n",
       "Office Supplies + Presentation Equipment                                            7\n",
       "Furniture + Dining Room                                                             7\n",
       "Electrical + Wall Plates                                                            7\n",
       "Building Supplies + Rain Guttering                                                  7\n",
       "Sports & Outdoor Recreation Equipment + Soccer                                      7\n",
       "Electronics + Emergency Alert Radios                                                7\n",
       "Tobacco Products + Herbal Snuff (Non Tobacco)                                       6\n",
       "Electronics + Global Positioning Systems & Navigation                               6\n",
       "Electrical + Receptacles & Plugs                                                    6\n",
       "Building Supplies + Drywall, Wallboard, Gypsum & Stucco                             6\n",
       "Cleaning & Janitorial + Sweepers                                                    6\n",
       "Flooring + Carpeting & Area Rugs                                                    6\n",
       "Sports & Outdoor Recreation Equipment + Team / Field Equipment                      6\n",
       "Tools + Test Equipment                                                              6\n",
       "Books & Videos + Film & Television                                                  6\n",
       "Building Supplies + Concrete, Asphalt, Rebar & Accessories                          6\n",
       "Hospitality Supplies + Food & Beverage Condiments                                   5\n",
       "Hardware + Shopping Bags                                                            5\n",
       "Building Supplies + Blocks & Bricks                                                 5\n",
       "Electrical + Insulation                                                             5\n",
       "Appliances + Ranges                                                                 5\n",
       "Hospitality Supplies + Paper / Plastic / Foam Cups Sleeves                          5\n",
       "Arts & Crafts + Stamps                                                              5\n",
       "Arts & Crafts + Glitter                                                             5\n",
       "Appliances + Microwaves                                                             5\n",
       "Sports & Outdoor Recreation Equipment + Rugby                                       4\n",
       "Sports & Outdoor Recreation Equipment + Boxing                                      4\n",
       "Home & Venue Decoration + Floral & Foliage - Artifical                              4\n",
       "Childcare + Bathroom Safety                                                         4\n",
       "Cleaning & Janitorial + Moisture / Mildew / Mold Removers                           4\n",
       "Sports & Outdoor Recreation Equipment + Volleyball                                  4\n",
       "Hospitality Supplies + Ice Buckets                                                  4\n",
       "Arts & Crafts + Craft Sticks                                                        4\n",
       "Hardware + Retail Merchandisers / Displays                                          4\n",
       "Paints & Coatings + Interior & Exterior Paint                                       4\n",
       "Plumbing & Water Service + Water Utility Products                                   4\n",
       "Lighting & Fans + Work Lights & Accessories                                         3\n",
       "Tobacco Products + Electronic Cigarettes                                            3\n",
       "Plumbing & Water Service + Tools                                                    3\n",
       "Cleaning & Janitorial + Absorbents                                                  3\n",
       "Hospitality Supplies + Dispensers                                                   3\n",
       "Flooring + Tiles & Accessories                                                      3\n",
       "Power Sports + Universal Parts & Accessories                                        3\n",
       "Paints & Coatings + Automotive Finishes & Body Repair                               3\n",
       "Paints & Coatings + Cleaners, Removers & Solvents                                   3\n",
       "Home & Venue Decoration + Mirrors & Accessories                                     3\n",
       "Marine + Docking & Mooring                                                          3\n",
       "Hospitality Supplies + Comment Cards                                                3\n",
       "Material Handling + Drum & Barrel Parts / Accessories                               3\n",
       "Hardware + Tarpaulins / Drop Cloths / Plastic Sheeting                              2\n",
       "Sports & Outdoor Recreation Equipment + Climbing & Mountaineering                   2\n",
       "Arts & Crafts + Drafting / Technical Drawing                                        2\n",
       "Agricultural Equipment + Tires                                                      2\n",
       "Automotive + Braking                                                                2\n",
       "Electrical + Indicator Lights                                                       2\n",
       "Power Sports + Replacement Parts                                                    2\n",
       "Hardware + Springs                                                                  2\n",
       "Sports & Outdoor Recreation Equipment + Combat Sports                               2\n",
       "Tobacco Products + Herbal Chew (Non Tobacco)                                        2\n",
       "Appliances + Cooktops                                                               2\n",
       "Hardware + Mobile Home                                                              2\n",
       "Cleaning & Janitorial + Adhesive Removers                                           2\n",
       "Paints & Coatings + Aerosol Spray Paint                                             2\n",
       "Office Supplies + Office Machinery                                                  2\n",
       "Electronics + Smartwatches                                                          2\n",
       "Tools + Tool Storage                                                                2\n",
       "Beer / Wine / Spirits + Sake                                                        2\n",
       "Tobacco Products + Cigarette Papers                                                 2\n",
       "Sports & Outdoor Recreation Equipment + Lacrosse                                    1\n",
       "Flooring + Laminate Flooring                                                        1\n",
       "Flooring + Tools                                                                    1\n",
       "Sports & Outdoor Recreation Equipment + Cue Sports                                  1\n",
       "Kitchen & Bathroom + Cabinets                                                       1\n",
       "Hospitality Supplies + Beverage Carriers                                            1\n",
       "Material Handling + Packaging Equipment                                             1\n",
       "Lighting & Fans + Gas Lighting                                                      1\n",
       "Marine + Safety Equipment                                                           1\n",
       "Electrical + Tools                                                                  1\n",
       "Heating / Ventilation / Air Conditioning + Ducts & Accessories                      1\n",
       "Building Supplies + Windows                                                         1\n",
       "Building Supplies + Insulation                                                      1\n",
       "Paints & Coatings + Waterproofers, Sealers & Preservatives                          1\n",
       "Hospitality Supplies + Beverage Container Covers                                    1\n",
       "Paints & Coatings + Interior Stains                                                 1\n",
       "Marine + Cabin & Galley                                                             1\n",
       "Material Handling + Forklift Attachments                                            1\n",
       "Flooring + Adhesives                                                                1\n",
       "Agricultural Equipment + Vehicle Parts                                              1\n",
       "Hardware + Signage                                                                  1\n",
       "Power Sports + Vehicles                                                             1\n",
       "Marine + Water & Waste Management                                                   1\n",
       "Power Transmission & Motion Control + Tools                                         1\n",
       "Toys / Games / Hobbies + Hobbies                                                    1\n",
       "Hospitality Supplies + Note Pads & Paper                                            1\n",
       "Musical Instruments + Universal Music Accessories                                   1\n",
       "Heating / Ventilation / Air Conditioning + Air Conditioning                         1\n",
       "Appliances + Wine Coolers / Preservation                                            1\n",
       "Heating / Ventilation / Air Conditioning + Ventilation                              1\n",
       "Paints & Coatings + Exterior Paints                                                 1\n",
       "Heating / Ventilation / Air Conditioning + Building HVAC System Components          1\n",
       "Building Supplies + Structural Connection Hardware                                  1\n",
       "Building Supplies + Doors                                                           1\n",
       "Furniture + Bedroom                                                                 1\n",
       "Flooring + Anti-Fatigue Mats / Platforms                                            1\n",
       "Marine + Fluids, Chemicals, & Compounds                                             1\n",
       "Building Supplies + Garage Door Openers & Accessories                               1\n",
       "Sports & Outdoor Recreation Equipment + Skateboarding                               1\n",
       "Material Handling + Stretch Wrap Equipment                                          1\n",
       "Arts & Crafts + Wood Burning / Engraving                                            1\n",
       "Appliances + Refrigerators                                                          1\n",
       "Building Supplies + Suspension / Framing Grid Systems                               1\n",
       "Electrical + Electricity Generation                                                 1\n",
       "Tools + Portable Electricity Generation                                             1\n",
       "Books & Videos + Music                                                              1\n",
       "Name: l1_l2, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = syndigo_mapped['l1_l2'].value_counts(dropna =  False)\n",
    "s1 = s[s < 10]\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {\"count\" : s1} \n",
    "\n",
    "df1 = pd.DataFrame(s, columns=[\"count\"])   #NW Return enmpty DF\n",
    "df1 = pd.DataFrame(data)\n",
    "df1['count'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    2\n",
       "5    3\n",
       "6    4\n",
       "7    2\n",
       "8    2\n",
       "9    5\n",
       "Name: l1_l2, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getnnz(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_metric_2, tab_2 =  validation(sample_frec = 0.3, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Syndigo Mapping MachineLearning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
