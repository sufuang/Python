{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imbed_L1_L2_Syndigo Mapping ML\n",
    "  - module name: imbed_L1_L2_Syndigo ML.ipynb\n",
    "  - Purpose: Combine Level 1 & Level 2 from  Syndigo to build model\n",
    "             - Apply TFIDF and W2vec imbedding methods \n",
    "  - Excluding non-item SUBCOMs from un-label data and including  ‘stratify= y’ when I did train_test_split.           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_1():\n",
    "    pgm = inspect.currentframe().f_code.co_name  \n",
    "    start_time = time.time() \n",
    "    # Reading PIMMART data\n",
    "    pim_gtin_mapped = pd.read_csv(DBFR + \"PIM_Data_New_50_82Mn.csv\", dtype=object) # (5082212, 24)\n",
    "\n",
    "    SUBCOMS_excluded = ['INVENTORY VALUES','MISC SALES TRANS','MISC SALES TRANS (NON TAX)','CENTRAL SUPPLIES', \\\n",
    "                        'MISC SALES TRANS (TX TAXABLE)','COUPON','MISCELLANEOUS INCOME','CRV/EXCISE TAX NT/NF', \\\n",
    "                        'CRV DEPOSIT NT/F','CRV DEPOSIT T/NF','MISCELLANEOUS REFUNDS','CUSTOMER EXPENSES','ROUND UP COUPONS']\n",
    "    pim_gtin_mapped = pim_gtin_mapped[~pim_gtin_mapped.SUBCOM_DSC.isin(SUBCOMS_excluded)]\n",
    "    print(f'Before drop pim_gtin_mapped.shape {pim_gtin_mapped.shape}')\n",
    "    print(f'After drop pim_gtin_mapped.shape {pim_gtin_mapped.shape}')\n",
    "    for i in ['SUBCOM_CD', 'DPT_CD', 'COM_CD','PMY_DPT_CD', 'REC_DPT_CD', 'ITM_ID', 'GTIN']:\n",
    "        pim_gtin_mapped[i] = pim_gtin_mapped[i].astype(np.float64)\n",
    "    \n",
    "    # Reading Syndigo 259K data\n",
    "    synd_ALL = pd.read_csv(DBFR + 'Syndigo_Final_ALL.csv', dtype='unicode') # 259k Syndigo Data\n",
    "    for i in ['SUBCOM_CD', 'DPT_CD', 'COM_CD', 'GTIN', 'ITM_ID', 'PMY_DPT_CD']:\n",
    "        synd_ALL[i] = synd_ALL[i].astype(np.float64)\n",
    "    \n",
    "    # Stripping spaces from all columns\n",
    "    df_obj = synd_ALL.select_dtypes(['object'])\n",
    "    synd_ALL[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    syndigo_mapped = synd_ALL\n",
    "    pimmart = pim_gtin_mapped\n",
    "    syndigo_mapped.drop_duplicates('GTIN', inplace = True)\n",
    "\n",
    "    \n",
    "    syndigo_mapped['ITEM_SUBCOM_text'] = \\\n",
    "    (syndigo_mapped.VND_ECOM_DSC + ' ' + syndigo_mapped.SUBCOM_DSC).fillna('').str.lower()\n",
    "    #syndigo_mapped['Level 1'].value_counts()\n",
    "    #print(f'syndigo_mapped.shape {syndigo_mapped.shape}') \n",
    "    #print(f'syndigo_mapped.info() {syndigo_mapped.info()}') \n",
    "    #print(f'syndigo_mapped.head() {syndigo_mapped.head()}') \n",
    "    #print(f'syndigo_mapped.columns {syndigo_mapped.columns}') \n",
    "    \n",
    "    syndigo_mapped['l1_l2'] = syndigo_mapped['Level 1'] + ' + ' + syndigo_mapped['Level 2']\n",
    "    #print(f\"syndigo_mapped['l1_l2'].value_counts() {syndigo_mapped['l1_l2'].value_counts(dropna = False)}\")    \n",
    "    return syndigo_mapped, pim_gtin_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_2(samp_frac = 1, embed_typ = 'tfidf'):\n",
    "  \n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name \n",
    "    global syndigo_mapped, model\n",
    "    print(f' samp_frac = {samp_frac}') \n",
    "    print (f'Before sample syndigo_mapped.shape {syndigo_mapped.shape }')\n",
    "    syndigo_mapped = syndigo_mapped_bkup.copy()\n",
    "    \n",
    "    whole_frac = 1\n",
    "    if samp_frac  < whole_frac :   syndigo_mapped = syndigo_mapped.sample(frac = samp_frac,  random_state=42)   \n",
    "    print (f'After sample syndigo_mapped.shape {syndigo_mapped.shape }')    \n",
    "    \n",
    "    \n",
    "    series = syndigo_mapped['l1_l2'].value_counts(ascending=True)\n",
    "    if series.tolist():\n",
    "       syndigo_mapped = syndigo_mapped.drop(syndigo_mapped[syndigo_mapped['l1_l2'].isin(series[series==1].index.tolist())].index)\n",
    "       print (f'after drop syndigo_mapped.shape {syndigo_mapped.shape }')                     \n",
    "    if embed_typ == 'tfidf': \n",
    "        vect = TfidfVectorizer(ngram_range = (1,2), max_features = 50000) \n",
    "        X = vect.fit_transform(syndigo_mapped.ITEM_SUBCOM_text) #scipy.sparse._csr.csr_matrix\n",
    "    else:\n",
    "        model = KeyedVectors.load_word2vec_format(DBFO + 'w2vmodel_053123_PIM_ALL.bin', binary=True)\n",
    "        X = np.array(list(syndigo_mapped.ITEM_SUBCOM_text.apply(lambda x: get_item_vector(x.split(' ')))))\n",
    "\n",
    "    l1_l2_id_map = dict(zip(syndigo_mapped['l1_l2'].fillna('Other').unique(), range(syndigo_mapped['l1_l2'].fillna('Other').nunique())))\n",
    "    id_l1_l2_map = dict(zip(range(syndigo_mapped['l1_l2'].fillna('Other').nunique()), syndigo_mapped['l1_l2'].fillna('Other').unique()))\n",
    "    y  = syndigo_mapped['l1_l2'].fillna('Other').map(l1_l2_id_map)\n",
    "    y_list= list(y)\n",
    "    y = pd.Series(list(filter (lambda z: y_list.count(z) > 1, y_list)))\n",
    "    \n",
    "    end_time = time.time()   \n",
    "    desc = f' Elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)\n",
    "    return  X, y, l1_l2_id_map, id_l1_l2_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build item vectors\n",
    "def get_item_vector(item_vocab):\n",
    "    vect = np.zeros_like(model.get_vector('chips'))\n",
    "    for word in item_vocab:\n",
    "        if word in model:\n",
    "            vect += model.get_vector(word)\n",
    "    return vect#/max(1,len(item_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_predict(test_size,c_parm, embed_typ):\n",
    "     start_time = time.time()\n",
    "     pgm = inspect.currentframe().f_code.co_name   \n",
    "     global A_train, A_test, B_train, B_test, X_train, X_test, y_train, y_test\n",
    "     A_train, A_test, B_train, B_test = train_test_split(syndigo_mapped.GTIN.tolist(), syndigo_mapped['l1_l2'].tolist(), test_size=test_size,  random_state=42, stratify= y)\n",
    "     \n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test_size,  random_state=42, stratify= y)\n",
    "     lr_tf = LogisticRegression(C = c_parm, multi_class = 'multinomial', solver = 'saga', n_jobs=-1)\n",
    "     \n",
    "     print(f\"Training starts for test_size = {test_size}, c_parm = {c_parm} \")\n",
    "     lr_tf.fit(X_train, y_train)\n",
    "     preds = lr_tf.predict(X_test)\n",
    "     #print(f\"type(preds) {type(preds)} len(preds) {len(preds)} \\n preds[0:10] {preds[0:10]}\")\n",
    "     preds_lrtf = preds\n",
    "     probs = lr_tf.predict_proba(X_test)\n",
    "     #print(f\"type(probs) {type(probs)} len(probs) {len(probs)} \\n probs[0:10] {probs[0:10]}\")   \n",
    "     preds_train = lr_tf.predict(X_train)\n",
    "     #print(f\"type(preds_train) {type(preds_train)} len(preds_train) {len(preds_train)} \\n preds_train[0:10] {preds_train[0:10]} \")   \n",
    "     probs_train = lr_tf.predict_proba(X_train)\n",
    "     #print(f\"type(probs_train) {type(probs_train)} len(probs_train) {len(probs_train)} \\n probs_train[0:10] {probs_train[0:10]}\")   \n",
    "     #print(classification_report(y_test, preds_test,labels = lr_tf.classes_, target_names = [id2_level_map[i] for i in lr_tf.classes_]))\n",
    "     \n",
    "    \n",
    "    \n",
    "     #print(\"Accuracy = \",accuracy_score(y_test,preds))\n",
    "     #print(\"Display TFIDF metrics\")\n",
    "     #print(classification_report(y_test, preds,labels = lr_tf.classes_, target_names = [id_l1_l2_map[i] for i in lr_tf.classes_]))\n",
    "     filename = path + f'L1_l2_LR_{embed_typ}_tst_siz{test_size}_C{c_parm}_{dte}.pkl'\n",
    "   \n",
    "     pickle.dump(lr_tf, open(filename, 'wb'))\n",
    "\n",
    "     print(f\"Done Training \")\n",
    "     end_time = time.time()   \n",
    "     desc = f' Elapse_time for \"{pgm}\".' \n",
    "     elapse_time (  start_time, end_time, desc)\n",
    "     return preds, preds_lrtf, probs, preds_train, probs_train, lr_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(test_size, c_parm, embed_typ):\n",
    "     start_time = time.time() \n",
    "     pgm = inspect.currentframe().f_code.co_name\n",
    "     global f1_score_weighted, f1_score_macro,f1_score_micro \n",
    "     l_metrics = []\n",
    "     print(f'Validation for c_parm = {c_parm}, test_size = {test_size}, and embed_type {embed_typ} ' ) \n",
    "     accuracy = accuracy_score(y_test,preds)\n",
    "     print(f\"Accuracy Score = {accuracy}\")\n",
    "     \n",
    "     f1_score_weighted = f1_score(y_test, preds, average=\"weighted\")\n",
    "     f1_score_macro    = f1_score(y_test, preds, average='macro')\n",
    "     f1_score_micro    = f1_score(y_test, preds, average='micro')  \n",
    "     print (f\" f1_score_weighted = {f1_score_weighted}\")\n",
    "     print (f\" f1_score_macro    = {f1_score_macro   }\")\n",
    "     print (f\" f1_score_micro    = {f1_score_micro   }\")\n",
    "     # Compute the Multiclass ROC AUC score\n",
    "     #multi_class = 'multinomial'\n",
    "     #score = roc_auc_score(y_test, preds, multi_class= multi_class)\n",
    "     #print(f\"ROC AUC score for Multiclass= {multi_class}: {score:.2f}\")\n",
    "\n",
    "     print(\"Display metrics\")\n",
    "     test_metrics = pd.DataFrame(classification_report(y_test, preds,labels = lr_tf.classes_, target_names = [id_l1_l2_map[i] for i in lr_tf.classes_],  output_dict= True)).T\n",
    "     #test_metrics = pd.DataFrame(classification_report(results_test.Actuals, results_test.Predictions, output_dict= True)).T \n",
    "     #print(test_metrics)\n",
    "     #Stest_metrics.to_csv('L1_L2_Test_Metrics_yue.csv')\n",
    "     #accuracy = 1\n",
    "     tab_name = f'{embed_typ}_L1_L2_TM_tst_siz{test_size}_C{c_parm}'   \n",
    "     print(f'tab_name = {tab_name}')  \n",
    "     l_metrics = [embed_typ, test_size, c_parm, accuracy, f1_score_weighted, f1_score_macro,f1_score_micro ]\n",
    "     end_time = time.time()   \n",
    "     desc = f' Elapse_time for \"{pgm}\".' \n",
    "     elapse_time (  start_time, end_time, desc)    \n",
    "     return test_metrics, tab_name, l_metrics \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_append():\n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name     \n",
    "    test_new_proobs = []\n",
    "    \n",
    "    #print(f'len(preds)  {len(preds)}')\n",
    "    for i in range(len(preds)):\n",
    "        test_new_proobs.append(probs[i][np.argsort(probs[i])][::-1][:1].tolist())\n",
    "    test_new_proobs = [element for sublist in list(test_new_proobs) for element in sublist]\n",
    "    #print(f'test_new_proobs {test_new_proobs}')\n",
    "    \n",
    "    train_new_proobs = []\n",
    "    \n",
    "    #print(f'len(preds_train)  {len(preds_train)}')\n",
    "    for i in range(len(preds_train)):\n",
    "         test_new_proobs.append(probs_train[i][np.argsort(probs_train[i])][::-1][:1].tolist())\n",
    "    train_new_proobs = [element for sublist in list(train_new_proobs) for element in sublist]\n",
    "    #print(f'train_new_proobs {train_new_proobs}')  \n",
    "    \n",
    "    # process testLevels & trainLevelss \n",
    "    testLevels = []\n",
    "    for j in y_test:\n",
    "        testLevels.append([i for i in l1_l2_id_map if l1_l2_id_map[i]==j][0])\n",
    "    #print(f'testLevels {testLevels [0:10]}')\n",
    "    testLevelss = []\n",
    "    for j in preds:\n",
    "        testLevelss.append([i for i in l1_l2_id_map if l1_l2_id_map[i]==j][0])\n",
    "    #print(f'testLevelss {testLevelss [0:10]}')\n",
    "    \n",
    "    trainLevelss = []\n",
    "    for j in preds_train:\n",
    "        trainLevelss.append([i for i in l1_l2_id_map if l1_l2_id_map[i]==j][0])\n",
    "     \n",
    "    end_time = time.time()   \n",
    "    desc = f' Elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)    \n",
    "    return  test_new_proobs, train_new_proobs, testLevels, testLevelss, trainLevelss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_cr_df(test_size, c_parm, embed_typ):\n",
    "\n",
    "    start_time = time.time()\n",
    "    pgm = inspect.currentframe().f_code.co_name \n",
    "    print(\"---------------------------------------------\\FINAL COUNTS:\\n---------------------------------------------\")\n",
    "    print(\"1)\", len(A_test + A_train))\n",
    "    print(\"2)\", len(['Test']*len(A_test) + ['Train']*len(A_train)))\n",
    "    print(\"3)\", len(B_test + B_train))\n",
    "    print(\"4)\", len(testLevels + B_train))\n",
    "    print(\"5)\", len(testLevelss + trainLevelss))\n",
    "    print(\"6)\", int(len(test_new_proobs + train_new_proobs)))\n",
    "    \n",
    "    data = {\n",
    "        'GTIN' : A_test + A_train,\n",
    "        'Source': ['Test']*len(A_test) + ['Train']*len(A_train),\n",
    "        'Actual l1_l2' : B_test + B_train,\n",
    "        'Actuals' : testLevels + B_train,\n",
    "        'Predictions' : testLevelss + trainLevelss,\n",
    "        'Scores' : test_new_proobs + train_new_proobs,\n",
    "    \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.merge(pim_gtin_mapped[['GTIN', 'ITM_ID', 'PMY_DPT_CD', 'PMY_DPT_DSC', 'REC_DPT_CD',\n",
    "        'REC_DPT_DSC', 'DPT_CD', 'DPT_DSC', 'COM_CD', 'COM_DSC', 'SUBCOM_CD',\n",
    "        'SUBCOM_DSC', 'VND_ECOM_DSC']], on='GTIN', how='left')\n",
    "    df = df[['ITM_ID', 'GTIN', 'PMY_DPT_CD', 'PMY_DPT_DSC', 'REC_DPT_CD', 'REC_DPT_DSC', 'DPT_CD',\n",
    "        'DPT_DSC', 'COM_CD', 'COM_DSC', 'SUBCOM_CD', 'SUBCOM_DSC',\n",
    "        'VND_ECOM_DSC', 'Source', 'Actual l1_l2', 'Actuals', 'Predictions', 'Scores']]\n",
    "    #filename = 'Syndigo_Mapping_L1_L2_ML.csv'\n",
    "    tab_name = f'{embed_typ}_L1_L2_prd_tst_siz{test_size}_C{c_parm}'  \n",
    "    #df.to_csv(DBFO + filename, index=False)\n",
    "    #print(f\"{filename}  --file created successfully\")\n",
    "\n",
    "    end_time = time.time()   \n",
    "    desc = f'Elapse_time for \"{pgm}\".' \n",
    "    elapse_time (  start_time, end_time, desc)\n",
    "    return df, tab_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_excel_pred( excel_file, dict):\n",
    "#  Write classification report to excel\n",
    "#  save() is going to remove, use close() instead\n",
    "   from pandas import ExcelWriter\n",
    "   from pandas import ExcelFile\n",
    "   writer = pd.ExcelWriter(excel_file)\n",
    "   for key,  value in dict.items():\n",
    "       df   =  value[0] \n",
    "       tab  =  value[1] \n",
    "       df.to_excel(writer,tab)\n",
    "   writer.close()\n",
    "   return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_excel_metric( excel_file, dict, l_metric):\n",
    "#  Write classification report to excel\n",
    "#  save() is going to remove, use close() instead\n",
    "   from pandas import ExcelWriter\n",
    "   from pandas import ExcelFile\n",
    "   writer = pd.ExcelWriter(excel_file)\n",
    "   for key,  value in dict.items():\n",
    "       df   =  value[0] \n",
    "       tab  =  value[1] \n",
    "       df.to_excel(writer,tab)\n",
    "   df_pred = pd.DataFrame(l_metric, columns = ['embed_typ', 'test_size', 'c_parm', 'accuracy', 'f1_score_weighted', 'f1_score_macro',\n",
    "                                                'f1_score_micro', 'elapse_time'] )   \n",
    "   df_pred.to_excel(writer,'Consolidate Metrics')                                         \n",
    "   writer.close()\n",
    "   return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b64183-34df-46b6-b1c9-6bbe78776fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier,  RidgeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import re, time, inspect, pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format \n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# load  function of 'elapse_time'\n",
    "path_code = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Code\\\\'  \n",
    "f_com_code = path_code + \"com_code.py\"\n",
    "exec(compile(open(f_com_code , \"rb\").read(), f_com_code, 'exec' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBFS = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFO = \"/dbfs/FileStore/tables/OFFSHORE/\"\n",
    "DBFM = \"/dbfs/FileStore/tables/MALLIK/\"\n",
    "DBFR = \"/dbfs/FileStore/tables/OFFSHORE_RESULTS/\"\n",
    "path = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Data\\\\'   \n",
    "DBFS = path\n",
    "DBFO = path\n",
    "DBFM = path\n",
    "DBFR = path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop pim_gtin_mapped.shape (5061471, 24)\n",
      "After drop pim_gtin_mapped.shape (5061471, 24)\n"
     ]
    }
   ],
   "source": [
    "syndigo_mapped, pim_gtin_mapped = prepare_data_1()\n",
    "syndigo_mapped_bkup = syndigo_mapped.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_typ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, y, l1_l2_id_map, id_l1_l2_map \u001b[38;5;241m=\u001b[39m prepare_data_2(samp_frac \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m, embed_typ \u001b[38;5;241m=\u001b[39m \u001b[43membed_typ\u001b[49m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embed_typ' is not defined"
     ]
    }
   ],
   "source": [
    "X, y, l1_l2_id_map, id_l1_l2_map = prepare_data_2(samp_frac = 0.3, embed_typ = embed_typ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_c_parm = [0.1, 1, 10, 100] \n",
    "l_c_parm = [100] \n",
    "l_embed_typ = ['tfidf', 'W2vec']\n",
    "#l_embed_typ = ['tfidf']\n",
    "\n",
    "dic_metric={}\n",
    "dic_predict = {}\n",
    "l_metric=[]\n",
    "test_size = 0.3\n",
    "i = 0\n",
    "dte = date.today().strftime('%m%d%y')\n",
    "\n",
    "for embed_typ in l_embed_typ:\n",
    "    X, y, l1_l2_id_map, id_l1_l2_map = prepare_data_2(samp_frac = 1, embed_typ = embed_typ )\n",
    "    for c_parm in l_c_parm:\n",
    "        start_time = time.time()        \n",
    "        \n",
    "        preds, preds_lrtf, probs, preds_train, probs_train, lr_tf = proc_predict(test_size = 0.3, c_parm =c_parm,  embed_typ= embed_typ )\n",
    " \n",
    "        test_metrics, vtab, l_metrics  =  validation(test_size = 1, c_parm =c_parm, embed_typ=embed_typ) \n",
    "        key = 'test_metric' + str(i)\n",
    "        dic_metric [key] = (test_metrics, vtab)\n",
    "        \n",
    "        test_new_proobs, train_new_proobs, testLevels, testLevelss, trainLevelss = proc_append()\n",
    "        df_save, ftab  = proc_cr_df(test_size=0.3, c_parm=c_parm, embed_typ= embed_typ )\n",
    "        key = 'predict'+ str(i)\n",
    "        dic_predict[key] = (df_save, ftab)\n",
    "        i = i + 1   \n",
    "        end_time = time.time() \n",
    "        desc = f' Elapse_time to build model for for test_size = 0.3, c_parm = {c_parm}, embed_typ= {embed_typ}' \n",
    "        hh, mm, ss= elapse_time (  start_time, end_time, desc)\n",
    "                                 \n",
    "        l_metrics = l_metrics +  [f\"{hh:2d}hh:{mm:2d}mm:{ss:2d}ss\"]   \n",
    "        l_metric.append(l_metrics)\n",
    "                                  \n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_cls_rpt_size{test_size}_{dte}.xlsx'\n",
    "save_excel_metric( excel_file, dic_metric, l_metric)\n",
    "\n",
    "\n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_tst_size{test_size}_{dte}.xlsx'\n",
    "save_excel_pred( excel_file, dic_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = DBFO + f'L1_L2_syndigo_pred_tst_size{test_size}_082527.xlsx'\n",
    "save_excel( excel_file, dic_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_predict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3 \n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_cls_rpt_size{test_size}_082527.xlsx'\n",
    "save_excel_metric( excel_file, dic_metric, l_metric)\n",
    "\n",
    "\n",
    "excel_file = DBFO + f'L1_L2_syndigo_pred_tst_size{test_size}_082527.xlsx'\n",
    "save_excel_pred( excel_file, dic_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class = 'multinomial'\n",
    "score = roc_auc_score(y_test, preds, multi_class= multi_class)\n",
    "print(f\"ROC AUC score for Multiclass= {multi_class}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim_gtin_mapped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim_gtin_mapped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim_gtin_mapped['ITM_ID_y'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4823143/len(pim_gtin_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(l1_l2_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = pd.DataFrame.from_dict(id_l1_l2_map, orient ='index', columns=['l1_l2']) \n",
    "df_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = df_dict.rename_axis('value').reset_index()\n",
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dict = pd.DataFrame.from_dict(id_l1_l2_map, orient ='index', columns=['l1_l2']).reset_index() \n",
    "df_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\users\\\\iny2819\\\\kroger\\\\Data\\\\'\n",
    "df_dict.to_csv(path + \"id_l1_l2_map.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_1 = pd.DataFrame.from_dict(id_l1_l2_map)  #  If using all scalar values, you must pass an index\n",
    "df_dict_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_l2_id_map.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list= list(y)\n",
    "z = pd.Series(list(filter (lambda z: y_list.count(z) > 1, y_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = syndigo_mapped[syndigo_mapped['l1_l2'].value_counts(ascending=True)\n",
    "if series.tolist():\n",
    "    subset_df = syndigo_mappeddrop(subset_df[subset_df['Level 2'].isin(series[series==1].index.tolist())].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndigo_mapped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndigo_mapped['COM_DSC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndigo_mapped[syndigo_mapped['COM_DSC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = syndigo_mapped[syndigo_mapped['COM_DSC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6      74863\n",
    "1      22727\n",
    "12     21240\n",
    "5      19935\n",
    "20     13582\n",
    "0       9018\n",
    "2       8928\n",
    "25      7114\n",
    "15      6296\n",
    "3       6123\n",
    "7       5729\n",
    "21      5333\n",
    "19      5048\n",
    "9       4940\n",
    "14      4483\n",
    "13      3976\n",
    "30      3557\n",
    "41      2150\n",
    "17      2082\n",
    "10      1772\n",
    "44      1649\n",
    "32      1623\n",
    "24      1232\n",
    "11      1224\n",
    "72      1157\n",
    "23      1139\n",
    "27       964\n",
    "34       950\n",
    "29       937\n",
    "99       925\n",
    "4        875\n",
    "22       863\n",
    "35       843\n",
    "38       651\n",
    "53       589\n",
    "42       574\n",
    "87       559\n",
    "50       559\n",
    "16       540\n",
    "18       505\n",
    "83       416\n",
    "48       407\n",
    "63       394\n",
    "69       384\n",
    "52       372\n",
    "58       370\n",
    "84       347\n",
    "28       328\n",
    "33       312\n",
    "65       309\n",
    "90       294\n",
    "74       270\n",
    "70       259\n",
    "75       258\n",
    "45       258\n",
    "40       245\n",
    "43       237\n",
    "47       215\n",
    "31       202\n",
    "78       191\n",
    "8        171\n",
    "62       167\n",
    "36       165\n",
    "61       164\n",
    "79       164\n",
    "100      155\n",
    "85       138\n",
    "55       135\n",
    "67       133\n",
    "37       132\n",
    "49       113\n",
    "73       111\n",
    "114      110\n",
    "76       109\n",
    "46       108\n",
    "121      106\n",
    "88       103\n",
    "57        99\n",
    "80        98\n",
    "105       97\n",
    "96        95\n",
    "60        87\n",
    "77        84\n",
    "113       82\n",
    "81        81\n",
    "39        80\n",
    "134       76\n",
    "26        73\n",
    "120       73\n",
    "106       72\n",
    "66        69\n",
    "148       67\n",
    "86        65\n",
    "169       63\n",
    "158       61\n",
    "139       58\n",
    "123       55\n",
    "122       49\n",
    "108       48\n",
    "64        48\n",
    "93        48\n",
    "117       46\n",
    "149       45\n",
    "110       45\n",
    "143       45\n",
    "151       42\n",
    "92        42\n",
    "115       41\n",
    "125       40\n",
    "56        38\n",
    "51        38\n",
    "129       36\n",
    "109       35\n",
    "130       33\n",
    "59        31\n",
    "103       29\n",
    "112       29\n",
    "177       29\n",
    "176       29\n",
    "172       28\n",
    "128       27\n",
    "89        27\n",
    "116       26\n",
    "173       26\n",
    "54        26\n",
    "107       25\n",
    "136       24\n",
    "111       23\n",
    "168       23\n",
    "102       23\n",
    "181       23\n",
    "126       22\n",
    "153       22\n",
    "159       22\n",
    "178       22\n",
    "199       21\n",
    "124       20\n",
    "167       19\n",
    "187       17\n",
    "118       17\n",
    "94        16\n",
    "137       15\n",
    "179       14\n",
    "171       14\n",
    "221       14\n",
    "150       13\n",
    "97        13\n",
    "200       13\n",
    "101       12\n",
    "156       12\n",
    "180       12\n",
    "204       12\n",
    "135       12\n",
    "104       11\n",
    "160       11\n",
    "207       10\n",
    "227       10\n",
    "170        9\n",
    "163        9\n",
    "259        9\n",
    "183        8\n",
    "95         8\n",
    "98         8\n",
    "161        8\n",
    "132        8\n",
    "71         7\n",
    "185        7\n",
    "234        7\n",
    "249        7\n",
    "202        7\n",
    "253        7\n",
    "154        6\n",
    "236        6\n",
    "232        6\n",
    "188        6\n",
    "175        6\n",
    "142        6\n",
    "141        6\n",
    "165        6\n",
    "82         6\n",
    "205        6\n",
    "194        5\n",
    "131        5\n",
    "196        5\n",
    "144        5\n",
    "174        5\n",
    "233        5\n",
    "155        5\n",
    "157        5\n",
    "166        5\n",
    "229        4\n",
    "230        4\n",
    "201        4\n",
    "218        4\n",
    "214        4\n",
    "208        4\n",
    "231        4\n",
    "206        4\n",
    "182        4\n",
    "195        4\n",
    "192        4\n",
    "162        3\n",
    "210        3\n",
    "262        3\n",
    "186        3\n",
    "133        3\n",
    "217        3\n",
    "216        3\n",
    "152        3\n",
    "225        3\n",
    "266        3\n",
    "245        3\n",
    "197        3\n",
    "270        3\n",
    "256        2\n",
    "239        2\n",
    "241        2\n",
    "235        2\n",
    "147        2\n",
    "242        2\n",
    "255        2\n",
    "228        2\n",
    "164        2\n",
    "91         2\n",
    "252        2\n",
    "271        2\n",
    "212        2\n",
    "189        2\n",
    "269        2\n",
    "268        2\n",
    "193        2\n",
    "119        2\n",
    "191        2\n",
    "261        1\n",
    "251        1\n",
    "254        1\n",
    "250        1\n",
    "267        1\n",
    "257        1\n",
    "273        1\n",
    "258        1\n",
    "265        1\n",
    "272        1\n",
    "264        1\n",
    "277        1\n",
    "276        1\n",
    "275        1\n",
    "274        1\n",
    "263        1\n",
    "260        1\n",
    "220        1\n",
    "248        1\n",
    "247        1\n",
    "146        1\n",
    "145        1\n",
    "140        1\n",
    "138        1\n",
    "184        1\n",
    "190        1\n",
    "198        1\n",
    "127        1\n",
    "68         1\n",
    "203        1\n",
    "209        1\n",
    "211        1\n",
    "213        1\n",
    "215        1\n",
    "219        1\n",
    "222        1\n",
    "223        1\n",
    "224        1\n",
    "226        1\n",
    "237        1\n",
    "238        1\n",
    "240        1\n",
    "243        1\n",
    "244        1\n",
    "246        1\n",
    "278        1"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Syndigo Mapping MachineLearning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
